---
title: "Replication Code"
subtitle: "for manuscript<br>Trust but Verify: Validating New Measures for Mapping Social Infrastructure in Cities"
author: "Code by Timothy Fraser"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE)
library(rmdformats)
library(tidyverse)
```

# 0. Setup

## 0.1 Packages

To get started, please install these packages!

```{r install_packages}
# Gather a vector of packages currently installed
mypackages <- installed.packages()[,1]

# Get a list of all packages needed
needed <- c(
  # Data wrangling
  "tidyverse",
  # Modeling
  "gtools", "car", "lmtest",
  # Visualization
  "viridis","ggpubr", "ggtext", 
  # Mapping
  "sf", "ggspatial", "gstat",
  # Census Data
  "tigris", "tidycensus", "censusapi",
  # Images and Icons
  "magick", "fontawesome", "rsvg", "ggimage",
  # Queries
  "googleway",
  # Tables
  "knitr", "kableExtra",
  # Fancy visuals
  "ggh4x", "devtools", "moderndive"
)

# Show me which needed packages have 'not yet' been installed
notyet <- needed[!needed %in% mypackages]


# If there are any packages that have not yet been installed,
if(length(notyet) > 0){
  
  if("ggh4x" %in% notyet){
    # If the ggh4x package has not yet been installed,
    # we're going to install it directly from github
    # the strip_split() function is very-very-new and only on the development version.
    devtools::install_github("teunbrand/ggh4x")
    # Once these are installed,
    # remove them from 'notyet'
    notyet <- notyet[!notyet %in% "ggh4x"]
  }
  
  # Please install them!
  install.packages(notyet)
  print("New packages installed. You're all set!")
}else{
  print("No new packages needed. You're all set!")
}


# Clear data
rm(list = ls())
```

## 0.2 Load Packages

Next, please load main packages for data wrangling. Other packages will be loaded as needed below.

```{r load_packages}
library(tidyverse)
library(sf)
library(viridis)
```

## 0.3 Make Metadata

First, we can gather a few types of data quickly, using data to be made *later in this code!*



Then, this analysis will use several frequently used vectors (eg. GIS projections). We're going to record their details in a list, so we can quickly summon them at any time!

```{r metadata}
list(
  # WGS projection details
  "wgs" = list(
      "name" = "EPSG:4326 (WGS 84) projection",
      "proj" = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs",
      "url" = "https://spatialreference.org/ref/epsg/wgs-84/"),
  # Albers Equal Area Conic projection details  
  "aea" = list(
    "name" = "North American Albers Equal Area Conic Projection",
    "proj" = "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs",
    "url" = "https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/" ),
  # Cell IDs for contiguous study region in inner Boston
  "inner_cells" = read_csv("helper/cell_zones.csv") %>% 
    filter(zone == "validation") %>% with(cell_id),
  # Cell IDs for sites within greater Boston area
  "boston_cells" = read_csv("helper/cell_zones.csv") %>% 
    filter(zone != "outside_boston") %>% with(cell_id),
  # Tract GEOIDs for tracts within Boston (excluding Chelsea, etc.)
  "boston_tracts" = read_lines("helper/boston_tracts.txt")
  
) %>% 
  # We'll use dput to 'put' this nested list into a text file,
  # and then we can retrieve it using dget()
  dput(file = "meta.txt")

# Clear data
rm(list = ls())
```


```{r}
# For example
dget("meta.txt")
```

This allows us to quickly access projections, like so:

```{r}
dget("meta.txt")$wgs$proj
```

<br>
<br>

## 0.4 Set API Keys


Next, we're going to make a file called `keys.R`, where we are going to put any keys we use in our environment. However, since every user has a different API key for various products, you're going to have to make your own for the code below to work!

Steps:

1. Make a file called `keys.R` in the main project directory.

2. Save the following code in it.

```{r, eval = FALSE}
Sys.setenv(CENSUS_API_KEY = "MY_API_KEY_HERE!")
```

3. Run the following code, which tells `R` to run that script, without anyone having to see specifically what your API key is :)

```{r}
# Load api keys
source("keys.R")
```


# 1. Geographic Data

Census block data is currently only available through tidycensus from the 2010 decennial census. Further, this data is not complete, since the Census bureau is still double checking numbers for many variables. So, we primarily work with data from the tract or block group levels instead.

<br>
<br>

## 1.1 Gather Census Variables

First, let's make a data.frame listing all the variable IDs we want, each with a corresponding variable name we can use later in the code.

```{r}
# First, let's get a record of all the variables we want!
tribble(
  ~id, ~variable,
    "B01003_001",  "pop", # Total Population
    # Age
    "B01001_020",  "pop_age_65_66_male",
    "B01001_021",  "pop_age_67_69_male",
    "B01001_022",  "pop_age_70_74_male",
    "B01001_023",  "pop_age_75_79_male",
    "B01001_024",  "pop_age_80_84_male",
    "B01001_025",  "pop_age_85_over_male",
    
    "B01001_044",  "pop_age_65_66_female",
    "B01001_045",  "pop_age_67_69_female",
    "B01001_046",  "pop_age_70_74_female",
    "B01001_047",  "pop_age_75_79_female",
    "B01001_048",  "pop_age_80_84_female",
    "B01001_049",  "pop_age_85_over_female",
    
    "B01001_026",  "pop_women", # Gender
    "B02001_002",  "pop_white",
    "B02001_003",  "pop_black", # Estimate!!Total!!Black or African American alone
    "B02001_004",  "pop_natam", #Estimate!!Total!!American Indian andAlaska Native alone
    "B02001_005",  "pop_asian", #Estimate!!Total!!Asian alone
    "B02001_006",  "pop_pacific", #Estimate!!Total!!Native Hawaiian and Other Pacific Islander alone
    "B03001_003",  "pop_hisplat", # Hispanic or Latino
    
    "B06009_001",  "pop_edu_total",
    "B06009_002",  "pop_edu_no_hs",
    "B06009_003",  "pop_edu_hs",
    "B06009_004",  "pop_edu_some_college",
    "B06009_005",  "pop_edu_college",
    "B06009_006",  "pop_edu_grad",
    #"B06009_004",  "pop_some_college",
    "B19083_001",  "income_inequality", #Income inequality: Estimate!!Gini Index)
    
    "B23025_003",  "pop_labor_force", # Estimate!!Total!!In labor force!!Civilian labor force
    "B23025_005",  "pop_unemployed", #Estimate!!Total!!In labor f orce!!Civilian labor force!!Unemployed
    
    "B19013_001",  "median_income", #Estimate!!Median Household income (dollars)!
    "B25105_001",  "median_monthly_housing_cost",
    "B08128_006",  "employees_muni", #Estimate!!Total!!Local government workers
    "B08128_007",  "employees_state", #Estimate!!Total!!State government workers
    "B08128_008",  "employees_fed" #Estimate!!Total!!Federal government workers
) %>%
  mutate(geography = "tract_or_bg") %>%
  write_csv("data/covariates/variables.csv")
```

<br>
<br>

## 1.2 Download Census Data

```{r get_census_data, eval = FALSE}
# Load relevant packages
library(tidycensus)
library(censusapi)

# Next, let's write a quick function to format this data.
# This will work for tracts or block groups.
format = function(data){
  # Import data.frame of census variables  
  vars <- read_csv("data/covariates/variables.csv")

  data %>%
      # Grab these variables
  select(geoid = GEOID, variable_id = variable, estimate) %>%
  # Filter into just Suffolk County
  filter(str_sub(geoid, 1, 5) == "25025") %>% 
  # Join in the appropriate name for each id
  left_join(by = c("variable_id" = "id"), 
            y = filter(vars, geography == "tract_or_bg")) %>%
  # Pivot into a wide matrix, where each variable is a column
  pivot_wider(id_cols = geoid,
              names_from = variable,
              values_from = estimate) %>%
  # Tally up population in this age category
  mutate(pop_age_65_plus = pop_age_65_66_male + pop_age_67_69_male +
           pop_age_70_74_male + pop_age_75_79_male +
           pop_age_80_84_male + pop_age_85_over_male +
           pop_age_65_66_female + pop_age_67_69_female +
           pop_age_70_74_female + pop_age_75_79_female +
           pop_age_80_84_female + pop_age_85_over_female) %>%
  # Tally up total folks with some college education or more
  mutate(pop_some_college = pop_edu_some_college + pop_edu_college + pop_edu_grad) %>%
  # % percentage of population
  mutate_at(
    vars(
      pop_age_65_plus, pop_women,
      pop_white, pop_black, pop_natam,
      pop_asian, pop_pacific, pop_hisplat,
      pop_some_college,
      employees_muni, employees_state, employees_fed),
    # Normalize as a percent
    funs(. / pop)) %>%
  # Calculate percentage of residents unemployed
  mutate(pop_unemployed = pop_unemployed / pop_labor_force) %>%
  # Get age groups
  select(-c(pop_age_65_66_male, pop_age_67_69_male,
            pop_age_70_74_male, pop_age_75_79_male,
            pop_age_80_84_male, pop_age_85_over_male,
            pop_age_65_66_female, pop_age_67_69_female,
            pop_age_70_74_female, pop_age_75_79_female,
            pop_age_80_84_female, pop_age_85_over_female)) %>%
  return()
}


# Get census variables of interest
vars <- read_csv("data/covariates/variables.csv") %>%
  # Narrow to tract or block group level
  filter(geography == "tract_or_bg")


# Census Tracts

# Grab ids we want
vars$id %>%
  # And then query the US census API for those variables!
  get_acs(
    variables = .,
  year = 2019, survey = "acs5",
  state = "MA", county = "025", geography = "tract") %>%
  # Format the results
  format() %>%
  # and save
  write_csv("data/covariates/tracts_data.csv")

# Census Block Groups

# Grab the ids we want
vars$id %>%
  # And then query the US census API for those variables!
  get_acs(
    variables = ., survey = "acs5",
    year = 2019, state = "MA", county = "025",
    geography = "block group") %>%
  # Format the results with appropriate names and transformations  
  format() %>%
  # and save
  write_csv("data/covariates/bg_data.csv")
```

## 1.3 Download Social Capital Indices

```{r get_sci_data, eval = FALSE}
# Let's download the newest versions of the SCI
library(dataverse)
library(tidyverse)

dataverse::get_dataframe_by_name(
  dataset = "10.7910/DVN/OSVCRC", 
  filename = "index_tracts_V3_04_10_2022.tab",
  server = "dataverse.harvard.edu") %>%
  # Filter to just Boston
  filter(str_sub(geoid, 1,5) == "25025") %>%
  select(year, geoid, social_capital, bonding, bridging, linking) %>%
  # save to file!
  write_csv("data/covariates/sci_census_tracts_2022_04_10.csv")
```


## 1.4 Polygons

In this part of the script, we're going to compile all necessary pieces of data for later analysis.

```{r get_polygons, eval = FALSE}
# Get our projections file
meta <- dget("meta.txt")

## County Polygons

# Download the polygons for Suffolk County, MA
tigris::counties(cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Filter to Suffolk Count in Massachusetts
  filter(statefp == "25", name %in% c("Suffolk")) %>%
  # Zoom into just these variables
  select(name, geoid, area_land = aland, geometry) %>%
  # Write to file
  st_write("shapes/county.geojson", delete_dsn = TRUE)

## Census Tract Polygons

# Download the polygons for Suffolk County census tracts
tigris::tracts(state = "MA", county = "025", cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Keep only a subset of variables
  select(geoid, area_land = aland, geometry) %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("data/covariates/tracts_data.csv",
                         col_types = list(geoid = col_character()))) %>%
  # Join in social capital indices from 2020
  left_join(by = "geoid",
            y = read_csv("data/covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid)) %>%
              filter(year == 2020) %>% select(-year)) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Write to file
  st_write("shapes/tracts.geojson", delete_dsn = TRUE)



# Load in census tract level demographics from 2019
read_sf("shapes/tracts.geojson") %>%
  # Transform to the Equal Area Conic projection
  st_transform(crs = dget("meta.txt")$aea$proj) %>%
  # Keep just land tracts
  filter(area_land > 0) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk County
  filter(!str_sub(geoid, 6,7) %in% c(15:18)) %>%
  # Next, we're going to get the exact list of census tract GEOIDs in Boston
  with(geoid) %>%
  write_lines("helper/boston_tracts.txt")


# This is what creates our metadata value: boston_tracts


# Census block data is currently only available through tidycensus from the 2010 decennial census. Further, this data is not complete, since the Census bureau is still double checking numbers for many variables. So, I gathered data from the block group level instead.


## Census Block Group Polygons

# Download block group boundaries!
tigris::block_groups(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(geoid = GEOID, area_land = ALAND, geometry) %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("data/covariates/bg_data.csv",
                         col_types = list(geoid = col_character()))) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000))  %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Write to file
  st_write("shapes/block_groups.geojson", delete_dsn = TRUE)

# Census Block Polygonns

# Download block boundaries.
tigris::blocks(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(geoid = GEOID20, area_land = ALAND20, geometry) %>%
  # Write to file
  st_write("shapes/blocks.geojson", delete_dsn = TRUE)


# Boston Polygon Outline

# Create one big polygon outline of tracts within Boston
read_sf("shapes/tracts.geojson") %>% 
  st_transform(crs= meta$aea$proj) %>%
  summarize(geoid = "Boston", geometry = st_union(geometry)) %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/boston.geojson", delete_dsn = TRUE)


read_sf("shapes/tracts.geojson") %>%
  st_transform(crs= dget("meta.txt")$aea$proj) %>%
  filter(geoid %in% read_lines("helper/boston_tracts.txt")) %>%
  summarize(geometry = st_union(geometry)) %>%
  st_transform(crs = dget("meta.txt")$wgs$proj)  %>%
  mutate(type = "Boston") %>%
  st_write("shapes/boston.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```

<br>
<br>




## 1.5 Fishnet Grids and Points

Next, we're going to make a series of fishnet grids for our analyses.

```{r make_fishnets, eval = FALSE}
# Get projections
meta <- dget("meta.txt")

# Import tracts
tracts <- read_sf("shapes/tracts.geojson") %>%
  # Transform to equal area conic projection
  st_transform(crs = meta$aea$proj)  

# Let's write a 'fishnet()' function to make grids of a given 'size'
# for each set of tract polygons supplied
fishnet = function(data, size){
 data %>%
  # make a cell size of 'XXXX' km squared 
  # (1 city block is ~100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = size, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(data) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(cell_id = 1:n()) %>%
  return()
}


# 1 city block is 1000 meters long;
# 2 blocks are 2000 meters long
# 5 blocks are 5000 meters long
# 10 blocks are 10000 meters long
tibble(size = c(1000, 2000, 5000, 10000)) %>%
  # For each size of grid,
  split(.$size) %>%
  # Generate a grid, saving the size in a 'size' column
  map_dfr(~fishnet(tracts, size = .$size), .id = "size") %>%
  # Transform back to normal coorindate projection
  st_transform(crs = meta$wgs$proj) %>%
  # Save fishnet grids to file
  st_write("shapes/fishnet.geojson", delete_dsn = TRUE)


# Get centroids from these points
read_sf("shapes/fishnet.geojson") %>%
  # Transform to Equal Area Conic projection!
  st_transform(crs = meta$aea$proj) %>%
  # Now let's extract the centroids of these cells too
  group_by(size, cell_id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
  # Transform back to normal coorindate projection
  st_transform(crs = meta$wgs$proj) %>%
  # Extract centroid coordinates
  mutate(lng = st_coordinates(geometry)[,1],
         lat = st_coordinates(geometry)[,2])  %>%
  # Save to file
  st_write("shapes/fishnet_points.geojson", delete_dsn = TRUE)
```

<br>
<br>

## 1.6 Classify Grid Cells

```{r classify_grid}
list(
  # Validation study area cells
  validation = c(60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77,
                 78, 79, 82, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 98,
                 99, 106, 107, 108, 109, 110, 111, 123, 124, 125, 126, 127, 128, 
                 142, 143, 144, 145, 146, 147, 148, 161, 162, 163, 164, 
                 175, 183, 174,
                 # Added
                 59, 49, 80, 176, 165, 166, 184),

  # Cells around validation area that are not very comparable to validation area,
  # due to being split by water, etc., thus dubbed "excluded"
  # Excluded from Validation area
  excluded_validation = c(50, 62, 72, 81, 89,  100, 112, 129, 
                          149), #80, 165, 166,  176,
  
  # Cells located within Boston city limits, 
  # but not in validation and not incomparable for some other reason
  boston = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 
             18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 
             33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48,
             51, 52, 53, 54, 55, 56, 57, 58, 90, 91, 92, 103, 
             104, 105, 119, 120, 121, 122, 137, 138, 139, 140, 141, 156, 
             157, 158, 159, 172, 185, 192, 193, 194, 195, 
             196, 203, 204, 205, 206, 207, 208,  218, 219, 225, 226,
             
              182, 160),

  # Cells located within Boston city limits,
  # but not comparable (due to being airport, islands, etc.)
  # Excluded from Boston
  excluded_boston = c(12, 19, 38, 39, 63, 101, 102, 113, 114, 115, 116, 117, 118, 130, 
                      131, 132, 133, 134, 135, 136, 150, 151, 152, 153, 154, 155, 167, 168,
                      169, 170, 177, 178, 179, 180, 186, 187, 188, 189, 191, 197, 198, 199, 
                      200, 202, 209, 210, 214, 220, 227, 232, 233, 234,
                      181, 171,  173,
                      93),
  
  
  # Cells located outside of Boston
  outside_of_boston = c(190, 201, 211, 212, 213, 215, 216, 217, 221, 
                        222, 223, 224, 228, 229, 230, 231, 235, 236, 
                        237, 238, 239, 240, 241, 242, 243, 244, 245, 
                        246, 247, 248, 249, 250, 251, 252, 253, 254, 
                        255, 256, 257, 258, 259, 260, 261)
) %>%
  map_dfr(~tibble(cell_id = .), .id = "zone") %>%
  write_csv("helper/cell_zones.csv")
```

<br>
<br>

## 1.7 Spatially Average to Grid Cell Level

### Block Group Membership Rates (Simply Analytics)

One of our supplementary Figures, Figure C3, relies on some block group level data derived from Simply Analytics. We are not allowed to share the block group level data, to our knowledge, since it comes from behind a paywall. 

- So, we have saved the block group level data in a password-protected zipfile on our Github repository. We provide code below to show how one would, if they had the password, unlock it, analyze it, and produce derivative measures.

- We have shared the derivative measures (since they are substantively different from the original measures, as we spatially averaged them to a different geographic level); these derivative measures are necessary for replicating Figure C3. They have been saved as columns in `shapes/grid_membership_rates.csv`.

So, if we had the password, how would we analyze these measures to calculate the derivative measures used in the models in **Figure C3**?

To access it, you will need the password, stored in your `R` environment using `Sys.setenv(SIMPLYANALYTICS = "hypothetical_password_goes_here")`.

```{r unzip_simply, eval = FALSE}
# Let's construct our code to the Linux terminal here...
run <- paste(
  # name of function
  "unzip",
  # overwrite any files already there
  "-o",
  # unlock with password
  "-P",
  # Call password from environment
  Sys.getenv("SIMPLYANALYTICS"),
  # specify location of zipfile
  "data/covariates/simply_analytics.zip")
# And run it through the command line
system(command = run)

# Clear data
rm(list = ls())
```

The code above will have unzipped the files into a new `data/covariates/simply_analytics` folder.

Now, let's access the block-group level shapefiles for it and spatially average them to the grid-cell level.

```{r simply_analytics, eval = FALSE}
library(sf)
# Simply Analytics made probabilistic estimates of membership 
# block-group-to-block-group based on demographics
bind_rows(
  read_sf("data/covariates/simply_analytics/bonding.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id, 
           fraternal = value0, religious = value1, 
           veterans = value2, country_clubs = value3) %>%
    as_tibble(),
  
  read_sf("data/covariates/simply_analytics/bridging.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           civic = value0, business = value1, collectors = value2, 
           union = value3, charitable = value4) %>%
    as_tibble(),
  
  read_sf("data/covariates/simply_analytics/linking.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           church_board = value2, school_board = value1, local_board = value0) %>%
    as_tibble()) %>%
  select(-geometry) %>%
  pivot_longer(cols = -c(geoid), names_to = "variable", 
               values_to = "x", values_drop_na = TRUE) %>%
  pivot_wider(id_cols = c(geoid), names_from = variable, values_from = x) %>%
  # Join back in geographies from one of them
  left_join(by = "geoid",
            y = read_sf("data/covariates/simply_analytics/linking.shp") %>%
              magrittr::set_colnames(value = names(.) %>% tolower()) %>%
              select(geoid = spatial_id, geometry)) %>%
  # Combine into one geojson file
  st_write("data/covariates/simply_analytics/bg_membership_rates.geojson", delete_dsn = TRUE)


# Import grid cells
read_sf("shapes/fishnet.geojson") %>%
  select(cell_id, size, geometry) %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = dget("meta.txt")$aea$proj) %>%
  # Join in overlapping block groups
  st_join(read_sf("data/covariates/simply_analytics/bg_membership_rates.geojson") %>%
            st_as_sf() %>%
            st_transform(crs = dget("meta.txt")$aea$proj)) %>%
  as_tibble() %>%
  # Average to grid per cell
  group_by(cell_id) %>%
  summarize_at(vars(fraternal:local_board),
               list(~mean(., na.rm = TRUE))) %>%
  ungroup() %>%
  write_csv("data/covariates/grid_membership_rates.csv")
# All done!
```

Finally, we save these files in a password-protected zipfile, and delete the directory.

```{r delete_simply, eval = FALSE}
# Load our password
source("keys.R")
# Get all files in the directory
dir("data/covariates/simply_analytics", full.names = TRUE) %>%
  # Zip it
  zip(files = ., zipfile = "data/covariates/simply_analytics.zip",
      # Assign this password from our environment
      flags = paste("-P", Sys.getenv("SIMPLYANALYTICS")))

# Delete the simply analytics folder
unlink("data/covariates/simply_analytics", recursive = TRUE)
```

<br>
<br>

### Precinct Voting Rates

First, let's download our precinct data from the Harvard Dataverse!

```{r get_precinct_data, eval = FALSE}
library(tidyverse)
library(dataverse)

# Let's download from Tim Fraser's Boston voting dataset on Harvard Dataverse...
# DOI: 10.7910/DVN/MMSBGJ

# Get projections
meta <- dget("meta.txt")

# Download voting data for 2020 presidential election by precinct in Boston.
votes <- dataverse::get_dataframe_by_name(
  dataset = "doi:10.7910/DVN/MMSBGJ", 
  server = "dataverse.harvard.edu",
  filename = "boston_votes_data.tab")  %>%
  # Recode data values
  mutate(type = type %>% tolower() %>% recode_factor(
    "biden and harris" = "democrat_percent",
    "trump and pence" = "republican_percent",
    "ballots cast" = "turnout_percent")) %>%
  select(ward_precinct, type, percent) %>%
  pivot_wider(id_cols = c(ward_precinct), names_from = type, values_from = percent)

# Download Boston precinct polygons, as of 2017
# Originally sourced from:
# https://data.boston.gov/dataset/precincts/resource/95d37dea-9dec-4fda-a8a6-c7d9df7adc35
dataverse::get_dataframe_by_name(
  dataset = "doi:10.7910/DVN/MMSBGJ", 
  server = "dataverse.harvard.edu",
  filename = "boston_precinct_polygons.kml",
  .f = sf::read_sf) %>%
  select(ward_precinct = WARD_PRECINCT, geometry) %>%
  # Join in voting data
  left_join(by = "ward_precinct", y = votes) %>%
  # Convert and save
  st_as_sf(crs = meta$wgs$proj) %>%
  st_make_valid() %>%
  st_write("shapes/precincts.geojson", delete_dsn = TRUE)


# Aggregate precincts into ward polygons
read_sf("shapes/precincts.geojson") %>%
  select(ward_precinct, geometry) %>%
  mutate(ward = str_sub(ward_precinct, 1,2)) %>%
  group_by(ward) %>%
  summarize(geometry = st_union(geometry)) %>%
  ungroup() %>%
  st_write("shapes/wards.geojson", delete_dsn = TRUE)  


# Finally, let's spatially average voting measures 
read_sf("shapes/fishnet.geojson") %>%
  # to the 1 km^2 grid cell size
  filter(size == 1000) %>%
  select(cell_id, geometry) %>%
  st_transform(crs = meta$aea$proj) %>%
  # Joining precinct data into the grid
  st_join(read_sf("shapes/precincts.geojson") %>%
            st_transform(crs = meta$aea$proj)) %>%
  as_tibble() %>% 
  # And averaging per grid cell
  group_by(cell_id) %>%
  summarize(
    across(cols = c(democrat_percent, republican_percent, turnout_percent),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  select(cell_id, contains("percent")) %>%
  # Then saving the result to file
  write_csv("data/covariates/grid_voting_data.csv")

# While, we're at it, 
# let's gather two more types of data.


# First, using the following set of zipcode polygons,
# Let's get average measures by zipcode.
read_sf("shapes/zipcodes.geojson") %>% 
  st_transform(crs = meta$aea$proj) %>%
  # Joining precinct data into the grids
  st_join(read_sf("shapes/precincts.geojson") %>%
            st_transform(crs = meta$aea$proj)) %>%
  # And averaging per zipcode
  group_by(geoid) %>%
  summarize(
    across(cols = c(democrat_percent, republican_percent, turnout_percent),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  # Then saving the result to file
  write_csv("data/covariates/zipcode_voting_data.csv")

# Second, let's download Polling Places in Boston as Points
#https://data.boston.gov/dataset/polling-locations/resource/f5b67c80-98a9-4e51-bfa0-ffdb8961d547
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/f7c6dc9eb6b14463a3dd87451beba13f_5.kml?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(-c(Name:icon)) %>%
  st_write("shapes/polls.geojson", delete_dsn = TRUE)

# Clear Data
rm(list = ls())
```

### Tract Measures

Finally, we need to spatially average tract data to the census tract level.

```{r get_tract_data, message = FALSE, warning = FALSE,  eval = FALSE}
# Load packages
library(tidyverse)
library(sf)

# Import metadata (projections)
meta <- dget("meta.txt")

# Import census tracts and data
tracts <- read_sf("shapes/tracts.geojson") %>%
  # Multiply into grid from 2011 to 2020
  expand_grid(year = 2011:2020) %>%
  # cut 2020 social capital,
  select(-c(social_capital:linking)) %>%
  # and join in for every year instead!
  left_join(by = c("geoid", "year"),
            y = read_csv("data/covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid))) %>%
  # format as sf
  st_as_sf() %>%
    # Transform to equal area conic projection
  st_transform(crs = meta$aea$proj) 


# Import grid cells
read_sf("shapes/fishnet.geojson") %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = meta$aea$proj) %>%
  # Spatially average tract data to grid level
  # Join in the census tract data
  st_join(tracts) %>% 
  # Convert to dataframe
  as_tibble() %>%
  # For each grid cell,
  group_by(year, cell_id) %>%
  summarize(
    # Take average of these variables
    across(.cols = c(pop_density,
                     pop_women, pop_white, pop_black, pop_asian,
                     pop_hisplat, pop_natam, pop_pacific,
                     pop_some_college,
                     employees_muni, employees_state, employees_fed,
                     median_income, income_inequality, pop_unemployed,
                     median_monthly_housing_cost, pop_age_65_plus,
                     social_capital, bonding, bridging, linking),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  # Save to file!
  write_csv("data/covariates/grid_tract_measures.csv")
```

## 1.8 Combine Measures

```{r grid_measures, eval = FALSE}
# Clear data
rm(list = ls())
# Load packages
library(tidyverse)
library(sf)
library(gstat)

# Import metadata (projections)
meta <- dget("meta.txt")

# Import grid cells
grid <- read_sf("shapes/fishnet.geojson") %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Join in the milestone and neighborhood labels for each grid cell
  left_join(by = "cell_id", 
            y = read_csv("helper/milestones.csv")) %>%
  # Join in our cell-zone classifier
  left_join(by = "cell_id",
            y = read_csv("helper/cell_zones.csv")) %>%
  # Join in our tract measures (+ social capital indices for 10 years 2011 to 2020)
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_tract_measures.csv")) %>%
  # Join in our block-group org membership rates
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_membership_rates.csv")) %>%
  # Join in our precinct voting rates
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_voting_data.csv")) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = meta$aea$proj) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  # In a few cases, tracts/block groups/blocks 'ostensibly' had no residents, 
  # but they're in Boston! So it's better to treat these as NA 
  # so no population density.
  mutate(pop_density = case_when(
    is.infinite(pop_density) ~ NA_real_,
    pop_density == 0 ~ NA_real_,
    TRUE ~ pop_density))

# Get just one year-long slice (pop-density is same in all of them)
shapes <- grid %>%
  filter(year == 2020) %>%
  select(cell_id, pop_density, geometry)

# Some areas of Boston *are* populated, by the census doesn't pick them up. 
# We'll use spatial interpolation to estimate their 
# population density based on surrounding grid cells.

# Spatially interpolate population density!
gstat::gstat(
    id = "cell_id", 
    formula = pop_density ~ 1, 
    # Use a inverse-distance-squared weighting
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(shapes) %>%
  # Bring back cell id
  mutate(cell_id = shapes$cell_id) %>%
  as.data.frame() %>%
  select(cell_id, pop_density_int = cell_id.pred) %>%
  # Join those results into the grid from 2011 to 2020!
  right_join(by = "cell_id", y = grid) %>%
  # Convert back to WGS for easy sharing
  st_as_sf(crs = meta$aea$proj) %>%
  st_transform(crs = meta$wgs$proj) %>%
  # Save  to file
  st_write("shapes/grid_covariates_annual.geojson", delete_dsn = TRUE)

# Now make a version with just the year 2020!
read_sf("shapes/grid_covariates_annual.geojson") %>%
  filter(year == 2020) %>% select(-year) %>%
  # Save to file!
  st_write("shapes/grid_covariates.geojson", delete_dsn = TRUE) 


# Clear data
rm(list = ls())
```


<br>
<br>

## 1.9 Frequent Mapping Data

Let's also create a simple script we can run at any time to load in the following common data.

```{r mapdata_script}
'
library(tidyverse)
library(sf)

# Import metadata
meta <- dget("meta.txt")

# Import tracts
tracts <- read_sf("shapes/tracts.geojson") %>%
  st_transform(crs= meta$aea$proj) 

# Import Boston boundaries
boston <- read_sf("shapes/boston.geojson") %>%
  st_transform(crs = meta$aea$proj)

# Load in Suffolk county background
county <- read_sf("shapes/county.geojson") %>%
  st_transform(crs= meta$aea$proj)

# Import fishnet grid
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  st_transform(crs= meta$aea$proj)
' %>%
  write_lines(file = "helper/mapdata.r")
```

Now, if we ever want to get those files, all at once, we can just use the `source()` function to run the `helper/mapdata.r` function.

```{r demo_mapdata}
# For example, let's run it!
source("helper/mapdata.r")
# And clear the data
rm(list = ls())
```


## 1.10 GIS Data

### Trains

```{r get_trains, eval = FALSE}
# Clear data
rm(list = ls())

# Download it from source (MASS GIS)
download.file(url = "https://s3.us-east-1.amazonaws.com/download.massgis.digital.mass.gov/shapefiles/state/mbta_rapid_transit.zip", destfile = "data/transit/transit.zip")

# Unzip the files we need
unzip(zipfile = "data/transit/transit.zip", 
      exdir = "data/transit")

meta <- dget("meta.txt")

# Get train lines!
read_sf("data/transit/MBTA_ARC.shp") %>%
  st_transform(crs = meta$aea$proj) %>%
  group_by(line = LINE) %>%
  summarize(geometry = st_union(geometry)) %>%
  st_transform(crs = meta$wgs$proj) %>%
 st_write("shapes/train_lines.geojson", delete_dsn = TRUE)
# Get train stops!
read_sf("data/transit/MBTA_NODE.shp") %>%
  select(stop = STATION, line = LINE) %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/train_stops.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```
### Hydrology

```{r get_hydrology, eval = FALSE}
library(tidyverse)
library(sf)
# Projection
meta <- dget("meta.txt")

# Get grid cells
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  select(cell_id, geometry) %>%
  st_make_valid() 

# Bodies of water
w <- tigris::area_water(state = "MA", county = "025", year = 2019) %>%
  st_as_sf(crs = 4326) %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(id = HYDROID, geometry) %>%
  # Narrow into just streets in/overlapping the grid cell
  st_join(blocks, left = FALSE)  %>%
  # Now crop the street lines to exactly the grid cells
  st_intersection(blocks %>% select(geometry)) %>%
  group_by(cell_id) %>%
  summarize(geometry = st_union(geometry))  %>%
  ungroup() %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/water.geojson", delete_dsn = TRUE)

# Rviers
tigris::linear_water(state = "MA", county = "025", year = 2019) %>%
  st_as_sf(crs = 4326) %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(id = LINEARID, geometry) %>%
  # Narrow into just streets in/overlapping the grid cell
  st_join(blocks, left = FALSE) %>%
  # Now crop the street lines to exactly the grid cells
  st_intersection(blocks %>% select(geometry)) %>%
  group_by(cell_id) %>%
  summarize(geometry = st_union(geometry))  %>%
  ungroup() %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/rivers.geojson", delete_dsn = TRUE)

rm(list = ls())
```

### Elevation

```{r get_elevation, eval = FALSE}
dir.create("data/contour")

# Download Elevation Contour file from MASSGIS
download.file(url = "https://s3.us-east-1.amazonaws.com/download.massgis.digital.mass.gov/shapefiles/state/contours250k.zip", destfile = "data/contour.zip")

# Unzip file
unzip("data/contour.zip", exdir = "data/contour")

# Get projeciton
meta <- dget("meta.txt")

# Read in contour
read_sf("data/contour/CONTOURS250K_ARC.shp") %>%
  st_transform(crs = meta$aea$proj) %>%
  # Join in blocks
  st_join(
    read_sf("shapes/grid_sampled.geojson") %>% 
      filter(coders == "masters") %>%
      select(cell_id, geometry) %>%
        st_transform(crs = meta$aea$proj), left = FALSE) %>%
  select(feet = CONTOUR_FT, cell_id, geometry) %>%
  # Convert back to wgs for easy sharing
  st_transform(crs = meta$wgs$proj) %>%
  # save!
  st_write("shapes/elevation.geojson", delete_dsn = TRUE)
```

### Streets

```{r get_streets, eval = FALSE}
## Streets
download.file(url = "https://bostonopendata-boston.opendata.arcgis.com/datasets/cfd1740c2e4b49389f47a9ce2dd236cc_8.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D",
              destfile = "shapes/streets.geojson")

# Get grid cells
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  select(cell_id, geometry)


read_sf("shapes/streets.geojson") %>%
  # Simplify contents
  select(street = ST_NAME, geometry) %>%
   # Narrow into just streets in/overlapping the grid cell
  st_join(blocks, left = FALSE) %>%
  # Now crop the street lines to exactly the grid cells
  st_intersection(blocks %>% select(geometry)) %>%
  # Combine all lines from the same street within a cell into one row
  group_by(street, cell_id) %>%
  summarize(geometry = st_union(geometry)) %>%
  ungroup() %>%
  # Save!
  st_write("shapes/streets.geojson", delete_dsn = TRUE)

rm(list = ls())
```

### Buildings

```{r get_buildings, eval = FALSE}
# Buildings
download.file(url = "https://bostonopendata-boston.opendata.arcgis.com/datasets/8bf3e3b0bde0432c82f76ee6a0608e7d_0.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D",
              destfile = "shapes/buildings.geojson")

# Get grid cells
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  select(cell_id, geometry)

read_sf("shapes/buildings.geojson") %>%
  select(building = BLDG_ID, geometry) %>%
     # Narrow into just buildings in/overlapping the grid cell
  st_join(blocks, left = FALSE) %>%
  st_write("shapes/buildings.geojson", delete_dsn = TRUE)

rm(list = ls())
```

### Universities

```{r get_uni, eval = FALSE}
# Colleges
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/cbf14bb032ef4bd38e20429f71acb61a_2.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(id = SchoolId, name = Name, area = City, year_built = YearBuilt, 
         students = NumStudent, cost = Cost, url = URL, geometry) %>%
  # Narrow into major universities only
  filter(students > 2000) %>%
  select(id, name, area, geometry) %>%
  st_write("shapes/universities.geojson", delete_dsn = TRUE)
```

### Interpolate Race

```{r get_race_tiles, eval = FALSE}
library(gstat)
library(sf)
library(rgdal)
library(tidyverse)

meta <- dget("meta.txt")

shapes <- read_sf("shapes/block_groups.geojson") %>% 
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  st_make_valid() %>%
  st_transform(crs = meta$aea$proj) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  select(geoid, pop_nonwhite, geometry) 
  
# Import blocks
grid <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  select(cell_id, geometry)  %>%
  st_make_valid() %>%
  st_transform(crs = meta$aea$proj)

blocks <- grid %>%
  # Let's try 100 meters (1 city block is 100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = 100, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  st_join(grid, left = FALSE) %>%
  # Add a unit id to each mini-block
  mutate(id = 1:n())

m <- gstat::gstat(
    id = "cell_id", 
    formula = pop_nonwhite ~ 1, 
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(blocks) %>%
  select(pop_nonwhite_int = cell_id.pred, geometry) %>%
  mutate(id = blocks$id,
         cell_id = blocks$cell_id) %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/race_interpolated.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```




# 2. API Test Run

First, we conduct a test-run using the Google Maps Places API, accessed via the ```googleway``` package.

- This demonstration will use D. Cooley's helpful *googleway* vignette from November 2020, available [here](https://cran.r-project.org/web/packages/googleway/vignettes/googleway-vignette.html#google-places-api).

- You can check out pricing [here](https://cloud.google.com/maps-platform/pricing). The whole point of this demonstration is to figure out how much it will cost to run this.


## 2.1 API Key

First, we have to enable the Google Maps Places API. Second, we need to get an API Key. I made it restricted to work only from RStudio Cloud and only for the Places API. Let's set it below.

```{r google_set_key, eval = FALSE}
# Load package
library(googleway)

# Let's load in your API key here
mykey <- Sys.getenv("GOOGLEWAY_KEY")

# Now set the API KEY
set_key(key = mykey, api = "places")

# View which API Keys we currently have registered here
# google_keys()
# If you need to use it, clear keys will clear those keys too.
#clear_keys() ## clear any previously set keys
```

## 2.2 Test Query (Places of Worship)

Next, we need to test out the API!

```{r google_test_query, eval = FALSE}
# Make a 'query' folder to hold results
dir.create("query")

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 1000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat)

# Let's do this for all of Boston
testquery <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

# Get places of worship
mypoints %>%
  split(.$cell_id) %>%
  map_dfr(~google_places(
    location = c(.$lat[1], .$lng[1]),
    keyword = "place of worship",
    radius = 770, # circumference of a circle that extends that far
    key = mykey)$result, .id = "grid") %>%
  # Save to file
  saveRDS("query/test/test_query.rds")
```

## 2.3 Process Test Query

```{r format_query, eval = FALSE}
# Let's write a quick function to process query output into point data
format_points = function(data){
  # Load metadata, for projections
  meta <- dget("meta.txt")
  
  data %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
  st_transform(crs = meta$wgs$proj)  %>%
  # Grab just the distinct results
  select(-grid) %>%
  group_by(place_id) %>%
  summarize_at(vars(name, business_status, 
                 #plus_code.compound_code, 
                 rating, types, user_ratings_total, vicinity, 
                 permanently_closed, geometry),
            funs(unique(.))) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
  map(~paste(., collapse = "; ")) %>%
  unlist()) %>%
  return()
}

# Now generate points
read_rds("query/test/test_query.rds") %>%
  format_points() %>%
  saveRDS("query/test/test_points.rds")
```

## 2.4 Test Query Diagnostics

How many unique addresses are there? Does every vicinity have an ID?

```{r query_unique}
myquery <- read_rds("query/test/test_query.rds")

# How often do we duplicate?
myquery %>%
  group_by(vicinity) %>%
  count() %>%
  ggplot(mapping = aes(x = reorder(vicinity, -n), y = n)) +
  geom_col(color = "white", size = 0.1, fill = "steelblue") +
  theme_classic(base_size = 14) +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank()) +
  labs(x = "378 unique place addresses found (n = 1209 search results)", 
       y = "# of Duplicate Results")
```


So, the Places API will give us 20 results per query. It is possible to obtain up to 60 total sites from each initial query (although each time you ask for the next 20, eg. 21-40, 41-60, that they will charge that as a query as well, and it's computationally a little difficult).

However, you get a lot of information from each request. There are two types of requests, roughly - "basic search" and "details". Basic search still delivers a bunch of information, including the *place ID,* *address*, *status of the establishment (open/closed/etc.), type of place* (eg. park, point of interest, restaurant, hotel), the *opening hours*, *etc., a rating,* and *longitude and latitude,* store in the geometry.


Really, the best way to do this is to query on a grid. We *will* get overlap, because we will have to use circles that overlap slightly with a square fishnet grid, but we can use the place_IDs as a way to eliminate duplicates. This is much worth the cost, because otherwise, we would almost certainly leave places out.

<br>
<br>

## 2.5. API Real Searches

Having run this initial test, we will now gather social infrastructure sites using our 2 km (most efficient) grid, and tally the results up using our 1 km grid.

This project uses the Google Maps Places API to collect geocoded point and polygon data of social infrastructure sites in the United States. For ~$0.03 per longitude-latitude coordinates queries, the API provides a near-complete record of the locations and meta-data of up to 20 social infrastructure sites within a 1 square-kilometer area, the average size of an Americanâ€™s neighborhood (Donaldson 2013). 

The first goal is to identify the best, most cost effective strategy for mapping social infrastructure.

So, that means identifying the best keywords.

Ideally, for each 2 km<sup>2</sup> grid cell, we want to run 19 searches, for:

(1) "libraries", community center, places of worship, or city hall," 

(2) "parks, squares, or fountains," 

(3) "cafes, coffeeshops, bookstores, barbershops, or beauty salons," and 

(4) "sports field, recreation center, museums, art galleries, zoos, aquariums, gardens." 


## 2.6 Searches

Based on this, we're going to deploy 19 search terms, saved in the ```sites``` vector.

```{r google_real_query, eval = FALSE}
# Public Facilities
sites <- c("library",
           "community center",
           "place of worship",
           "city hall", 
           
           # Parks & Green Space 
           "park",
           "fountain",
           "square",
           
           # Commercial
           "bookstore",
           "cafe","coffeeshop",
           "beauty salon",
           "barbershop",
           
           # Recreation
           "sports field",
           "recreation center",
           "aquarium",
           "art gallery",
           "zoo",
           "museum", 
           "garden")
# 81*0.03*18 = ~ $40
#270 * 0.03 * 18

# Area
#sqrt(2000^2 / (pi))
#1125^2*pi
# 1125 is roughly the radius of a 2 square kilometer circle

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 2000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat)

# Make a folder to hold results!
dir.create("query/actual")

# Gather each of these sites 1 per 2 square kilometers
for(i in 1:length(sites) ){

  print(sites[i])  
  
  # Get places of worship
  mypoints %>%
    split(.$cell_id) %>%
    # Run the query in a loop!
    map_dfr(~google_places(
      location = c(.$lat[1], .$lng[1]),
      keyword = sites[i],
      radius = 1125, # circumference of a circle that extends that far
      key = mykey)$result, .id = "grid") %>%
    # Save Query!
    saveRDS(paste("query/actual/", "search", sites[i], ".rds", sep = ""))
}

# Bind all searches together
data.frame(file = dir("query/actual", full.names = TRUE)) %>%
  split(.$file) %>%
  map_dfr(~read_rds(.$file), .id = "file") %>%
  saveRDS("query/sites.rds")

```

## 2.7 Process Points

Next, let's convert our search query results into nice `sf`-formatted points, which we can save as a `.geojson` file for easy access.

```{r make_query_points, eval = FALSE}
# Let's identify which search each site appeared in MOST;
# these will be the final sites we attribute them to.
# Format
meta <- dget("meta.txt")

# Import fishnet grid
fish <- read_sf("shapes/fishnet.geojson") %>%
  filter(size == 1000) %>%
  st_transform(crs = meta$aea$proj)


mysites <- read_rds("query/sites.rds") %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
  st_transform(crs = meta$aea$proj) %>%
  # For each place id,
  # Consolidate any duplicates
  group_by(place_id) %>%
  summarize(
    file = table(file) %>% sort(decreasing = TRUE) %>% names() %>% .[1],
    name = unique(name),
    business_status = unique(business_status),
    rating = unique(rating),
    types = unique(types),
    user_ratings_total = unique(user_ratings_total),
    permanently_closed = unique(permanently_closed),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
           map(~paste(., collapse = "; ")) %>%
           unlist()) %>%
  mutate(file = file %>% str_remove("query/actual/search") %>% str_remove(".rds")) %>%
  mutate(group = file %>% recode_factor(
    "library" = "Community Space",
    "community center" = "Community Space",
    "city hall" = "Community Space", 
    
  
    
    "place of worship" = "Places of Worship",
    
    # Parks & Green Space 
    "park" = "Parks",
    "fountain" = "Parks",
    "square" = "Parks",
    "garden" = "Parks",
    
    # Commercial
    "bookstore" = "Social Businesses",
    "cafe" = "Social Businesses",
    "coffeeshop" = "Social Businesses",
    "beauty salon" = "Social Businesses",
    "barbershop" = "Social Businesses",
    
    # Recreation
    "sports field" = "Recreation & Education",
    "recreation center" = "Recreation & Education",
    "aquarium" = "Recreation & Education",
    "art gallery" = "Recreation & Education",
    "zoo" = "Recreation & Education",
    "museum" = "Recreation & Education")) %>%
  # Exclude any points that aren't directly in a cell
  # (sometimes Google returns a fews points nearby but not within the cell;
  # we're not interested in any points that are OUTSIDE the overall Boston grid.
  st_join(fish, left = FALSE) %>%
  # Switch back to wgs for portability
  st_transform(crs = meta$wgs$proj) %>%
  # Save to file
  st_write("query/sites_points.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```


<br>
<br>

## 2.8 Final Sites

Next, we uploaded our data to Google MyMaps, created a human coded map, conducted ground-truthing and data cleaning, and then finally returned the data from that map to R, saving it as `sites_validated.csv`.

```{r validated_sites, eval=FALSE}
# To get the validation zone sites,
read_csv("query/sites_validated.csv") %>%
  # If there is a zone, cut it
  select(-zone, -cell_id) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  # Get coordinates back
  mutate(st_coordinates(.$geometry) %>% as_tibble() %>% select(x = 1, y = 2)) %>%
  # Join in the cell_id
  st_join(read_sf("shapes/grid_covariates.geojson") %>% select(cell_id)) %>%
  # And connect the correct zone
  left_join(by = "cell_id", y = read_csv("helper/cell_zones.csv")) %>%
  write_csv("query/sites_validated.csv")
```


We can access our site data at any time as `.csv` and convert it into an `sf` object using `st_as_sf(coords = c("x", "y"), crs = 4326)`.

```{r validated_view}
sites <- read_csv("query/sites_validated.csv") %>% 
  st_as_sf(coords = c("x","y"), crs = 4326) %>%
  select(id, name, type, status, source, neighborhood, cell_id, 
         google_id, search, geometry) 

grid <- read_sf("shapes/grid_covariates.geojson")

boston <- read_sf("shapes/boston.geojson")

ggplot() +
  geom_sf(data = boston, fill = "white", color = "black") +
  geom_sf(data = grid, fill = NA, color = "lightgrey") +
  geom_sf(data = sites, color = "red")
```

How do we filter to certain subsets of sites?


```{r validated_demo}
# Get points in validation zone,
# Ignoring cells that get cut off by water,
# as they are qualitatively different.
# A few analyses use just this, to validate the querying strategy
read_csv("query/sites_validated.csv") %>%
  filter(zone == c("validation"))

# Get points in validation zone,
# including those cells in validation zone
# where part of cell gets cut off by water
# Some analyses use this data,
# to avoid unnecessarily losing data
read_csv("query/sites_validated.csv") %>%
  filter(zone == c("validation", "excluded_validation"))

# Get actual social infrastructure:
read_csv("query/sites_validated.csv") %>%
  # Eliminating the sites that did not get the gold star
  filter(!status %in% c("duplicated", "not found", "not social infrastructure")) %>%
  head(3)

# Or just look at all the sites like so!
read_csv("query/sites_validated.csv") %>%
  head()
```

And which cells did we sample for groundtruthing?

```{r validated_cell_sample}
# Import grid cells sampled for ground truthing
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  # Zoom into the 10 we were able to do for this study.
  filter(coders == "masters") %>%
  as_tibble()
# Here's the list of our sampled cells!
blocks
```
Let's finally create a version *just* containining the *real* social infrastructure sites, having removed the ones that weren't social-infrastructure.

```{r get_social_infra, eval=FALSE}
# To get the validation zone sites,
read_csv("query/sites_validated.csv") %>%
  filter(status %in% c("checked", "checked, visited", "new", "new, visited")) %>% 
  write_csv("query/social_infrastructure_sites_boston.csv")
```

# 3. Tally Rates

## Function for Tallying Rates

```{r tally_function}
# Clear data
rm(list = ls())
# Let's write up a script
# which we can run at any time to generate an 
# up-to-date set of points and grid cells for tallying

'
# Get stack of points, including
sites <- bind_rows(
  # A set of just the total points (having excluded the Others like museums)
  read_csv("query/sites_validated.csv") %>%
    filter(type != "Other") %>%
    mutate(type = "Total"),
  # And the rest of points, delineated by type
  read_csv("query/sites_validated.csv") %>%
    filter(type != "Other"))

# Get sites
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  as_tibble() %>%
  select(cell_id, zone, pop_density_int) %>%
  # Stack data.frame on itself 6 times, creating several versions per type we want to tally 
  expand_grid(., type = c("Total", "Community Spaces", "Places of Worship", 
                          "Social Businesses", "Parks"))

# Let us write a function to tally up any set of points supplied
tallyup = function(points, zones = grid$zone %>% unique()){
  
  # Filter to match the specified zone above in zones
  mygrid <- grid %>%
    # Defaults to all zones in grid object
    filter(zone %in% zones)
  
  
  # If points is grouped,
  # maintain the groups
  if(!is.null(attr(points, "group"))){
    # Extract the grouping variable names
    thegroups <- attr(points, "group") %>% select(-.rows)
    
    # Now make as many grids as there are values in those groups,
    # so that each grid contains the values and variable names of those groups
    mygrid <- expand_grid(thegroups, mygrid)
    
    # Extract the names of those variables
    mygroups <- thegroups %>% names() %>%
      # Add these key ones
      c(., "type", "cell_id") %>%
      # Keep just unique variables
      unique()
    
  }else{
    mygroups <- c("type", "cell_id")
  }
  
  # Otherwise, just use these variables to group
  points %>%
    group_by(across(.cols = c(mygroups))) %>%
    # Tally up total points per cell per type
    summarize(count = sum(!is.na(id))) %>%
    # Join tally into the grid
    right_join(by = mygroups, y = mygrid) %>%
    # If NA, that means there are no sites; so that should get a zero
    mutate(count = if_else(is.na(count), 0, as.numeric(count))) %>%
    # Calculate rate of folks in the grid cell,
    # using interpolated population density per grid cell
    mutate(rate = count / pop_density_int * 1000) %>% 
    ungroup() %>%
    return()
}

' %>%
  write_lines("helper/tallydata.r")
```

## Tallying

```{r get_tally, eval=FALSE}
# Clear data
rm(list = ls())
# Load data/functions for tallying
source("helper/tallydata.r")

# Figure 2 data
bind_rows(
  # Let's gather 4 types of points....
  # API Sites not found in the validation zone
  sites %>%
    filter(zone == "validation") %>%
    filter(status == "not found") %>%
    mutate(panel = "API Sites\nNot Found"),
  # API sites in validation zone that weren't social infrastructure
  sites %>%
    filter(zone == "validation") %>%
    filter(status == "not social infrastructure") %>%
    mutate(panel = "Not social\ninfrastructure"),
  # API sites in validation zone that were checked
  sites %>%
    filter(zone == "validation") %>%
    filter(status %in% c("checked", "checked, visited")) %>%
    mutate(panel = "Verified\nAPI Sites"),
  # API sites in validation zone that were newly found
  sites %>%
    filter(zone == "validation") %>%
    filter(status %in% c("new", "new, visited")) %>%
    mutate(panel = "Verified\nNew Sites")) %>%
  # Save a copy
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  st_write("data/figure_2_points.geojson", delete_dsn = TRUE)


fig2 <- read_sf("data/figure_2_points.geojson") %>%
  # For each verification level, 
  group_by(panel) %>%
  # tally up sites in validation grid cells!
  tallyup(zones = "validation")


# Get sampled blocks for ground-truthing
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  with(cell_id)

# API Sites in the zone we analyzed
fig3 <- bind_rows(
  sites %>%
    filter(zone %in% c("validation", "excluded_validation")) %>%
    filter(source == "google_api") %>%
    mutate(panel = "Google API"),
  # Online Map sites
  sites %>%
    filter(zone %in% c("validation", "excluded_validation")) %>%
    filter(status %in% c("checked", "checked, visited",
                         "new", "new, visited")) %>%
    mutate(panel = "Online"),
  # Ground-truthed map sites
  sites %>%
    filter(zone %in% c("validation", "excluded_validation")) %>%
    filter(status %in% c("checked, visited",
                         "new, visited")) %>%
    mutate(panel = "Ground Truthed")
) %>%
  group_by(panel) %>%
  tallyup(zones = c("validation", "excluded_validation")) %>%
  # If ground-truthed, keep just the ground-truthed cells
  filter(
    (panel == "Ground Truthed" & cell_id %in% blocks) |
      # Otherwise, keep them all
    panel != "Ground Truthed")

# Get data for figure 4
fig4 <- sites %>%
  filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
  filter(source == "google_api") %>%
  mutate(panel = "Google API") %>%
  group_by(panel) %>%
  tallyup(zones = c("validation", "excluded_validation", "boston")) 


# Finally, let's do the source data for Table 1 and Appendix Table A1.

# Grab the 1-square kilometer fishnet grid
fish <- read_sf("shapes/grid_covariates.geojson") %>%
  # Filter to cells within boston
  filter(zone != "outside_of_boston")

# Load in the raw points
mypoints <- read_sf("query/sites_points.geojson") %>% select(-cell_id)

# How many points per cell would we expect if things were distributed equally?
taba1 <- fish %>%
  st_join(mypoints) %>%
  group_by(cell_id) %>%
  summarize(
#    sites = sum(!is.na(group)),
    # Since we're not going to count museums, ditch this category.
    sites = sum(!is.na(group) & file != "museum"),
    # Commmunity Space
    community = sum(group == "Community Space", na.rm = TRUE),
    library = sum(file == "library", na.rm = TRUE),
    community_center = sum(file == "community center", na.rm = TRUE),
    city_hall = sum(file == "city hall", na.rm = TRUE),
    
    place_of_worship = sum(file == "place of worship", na.rm = TRUE),
    # Parks
    parks = sum(group == "Parks", na.rm = TRUE),
    park = sum(file == "park", na.rm = TRUE),
    fountain = sum(file == "fountain", na.rm = TRUE),
    square = sum(file == "square", na.rm = TRUE),
    # Social Business
    social = sum(group == "Social Businesses", na.rm = TRUE),
    
    bookstore = sum(file == "bookstore", na.rm = TRUE),
    cafe = sum(file == "cafe", na.rm = TRUE),
    coffeeshop = sum(file == "coffeeshop", na.rm = TRUE),
    beauty_salon = sum(file == "beauty salon", na.rm = TRUE),
    barbershop = sum(file == "barbershop", na.rm = TRUE),
    # Recreation
    sports_field = sum(file == "sports field", na.rm = TRUE),
    recreation_center = sum(file == "recreation center", na.rm = TRUE),
    aquarium = sum(file == "aquarium", na.rm = TRUE),
    art_gallery = sum(file == "art gallery", na.rm = TRUE),
    zoo = sum(file == "zoo", na.rm = TRUE),
    museum = sum(file == "museum", na.rm = TRUE),
    garden = sum(file == "garden", na.rm = TRUE),
    
    rec = sum(group == "Recreation & Education", na.rm = TRUE),
    # Keep interpolated population density
    pop_density_int = unique(pop_density_int),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  as_tibble() %>% select(-geometry) %>%
  pivot_longer(cols = c(sites:rec), names_to = "type", values_to = "count") %>%
  # Calculate rate of sites per 1000 persons in the grid cell
  mutate(rate = count / pop_density_int * 1000) %>%
  mutate_at(vars(count, rate), funs(if_else(is.infinite(.) | is.nan(.),
                                          NA_real_, as.numeric(.) ))) %>%
  select(-pop_density_int)
  

# Save to file!
list(`0` = taba1, `2` = fig2, `3` = fig3, `4` = fig4) %>%
  bind_rows(.id = "figure") %>%
  write_csv("data/figure_rates.csv")

rm(list = ls())
```

## Icons

This analysis uses several publicly-available Fontawesome icons, courtesy of the amazing `fontawesome` package. Let's download them!

```{r get_icons, eval = FALSE}
# Make a folder to hold icons
dir.create("icons")

# Some code we hypothetically need....
htmltools::tagList(fontawesome::fa_html_dependency())

# Load image processing and rendering packages
library(fontawesome)
library(rsvg)
library(magick)

# For general use

fa_png(name = "handshake-angle",
       fill = "grey", fill_opacity = 0.25, 
       file = "icons/hands-helping.png")
fa_png(name = "store",
       fill = "#0D0887", fill_opacity = 0.25, 
       file = "icons/store-alt-blue.png")
fa_png(name = "place-of-worship",
       fill = "#8405A7", fill_opacity = 0.25, 
       file = "icons/place-of-worship-purple.png")
fa_png(name = "mug-saucer",
       fill = "#D35171", fill_opacity = 0.25, 
       file = "icons/coffee-red.png")
fa_png(name = "tree",
       fill = "#d98d2b", fill_opacity = 0.25, 
       file = "icons/tree-yellow.png")

# For ground-truthing in Figure A8

fa_png(name = "handshake-angle",
       fill = "grey", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/hands-helping-grey-circle.png")
fa_png(name = "store",
       fill = "#0D0887", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/store-alt-blue-circle.png")
fa_png(name = "place-of-worship",
       fill = "#8405A7", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/place-of-worship-purple-circle.png")
fa_png(name = "mug-saucer",
       fill = "#D35171", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/coffee-red-circle.png")
fa_png(name = "tree",
       fill = "#d98d2b", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/tree-yellow-circle.png")
fa_png(name = "circle-xmark",
       fill = "#333333", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/times-circle.png")
fa_png(name = "circle-question",
       fill = "#787276", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/question-circle.png")
fa_png(name = "circle-dot",
       fill = "#787276", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "icons/dot-circle.png")
```

## Table 1

```{r table1}
# Clear Data
rm(list = ls())

# Load pacakges
library(tidyverse)
library(knitr)
library(kableExtra)
library(viridis)

# Get colors
mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"

# Get data!
dat <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 0) %>%
  # Zoom into the overall tallies
  filter(type %in% c("sites", "parks", "social", "community", "place_of_worship")) %>%
  # Calculate descriptive stats
  group_by(type) %>%
  summarize(
    nobs = sum(count, na.rm = TRUE),
    mean = mean(rate, na.rm = TRUE),
            sd = sd(rate, na.rm = TRUE),
            min = min(rate, na.rm = TRUE),
            p25 = quantile(rate, probs = 0.25, na.rm = TRUE),
            median = median(rate, na.rm = TRUE),
            p75 = quantile(rate, probs = 0.75, na.rm = TRUE),
            max = max(rate, na.rm = TRUE),
            total = sum(rate, na.rm = TRUE),
            cells = sum(rate > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(nobs:cells),
               names_to = "group", values_to = "stat") %>%
  mutate(group = group %>% recode_factor(
    "nobs" = "Sites",
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25th %", "median" = "50th %", 
    "p75" = "75th %", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells with Sites")) %>%
  mutate(type = type %>% recode_factor(
    "sites" = "Total Sites",
    "community" = "Community Spaces",
    "place_of_worship" = "Places of Worship",
    "social" = "Social Businesses",
    "parks" = "Parks"),
    type = factor(type, levels = levels(type) %>% rev())) 

tab <- dat %>%
  pivot_wider(id_cols = type, names_from = group, values_from = stat) %>%
  mutate_at(vars(Mean:Total), list(~round(., 1))) %>%
  # Get order of rows from factor
  mutate(order = type %>% as.numeric()) %>%
  arrange(desc(order)) %>%
  select(-order) %>%
  # Extract number of observations & title from HTML
  mutate(type = type %>% str_extract(pattern = paste(c("Total Sites", "Community Spaces", "Places of Worship", "Social Businesses", "Parks"), collapse = "|"))) %>%
  mutate(` ` = "") %>%
  select(` `, Type = type, Sites, Mean:Total, "Cells with Sites")

# Take a peek at the raw data!
tab
```

```{r table1_latex, eval = FALSE}
# Now, print this kable function to generate the LaTex code behind our table!
options(knitr.table.format = "latex")

tab %>%
  kable(format = "latex", booktabs = TRUE, 
        caption = "Descriptive Statistics of API-generated Social Infrastructure Rates") %>%
  kable_styling() %>%
  row_spec(1, bold = T, color = "white", background = "black") %>%
  row_spec(2, bold = T, color = "white", background = mycolors[1]) %>%
  row_spec(3, bold = T, color = "white", background = mycolors[2]) %>%
  row_spec(4, bold = T, color = "white", background = mycolors[3]) %>%
  row_spec(5, bold = T, color = "white", background = mycolors[4]) %>%
  column_spec(1, background = "white", image = spec_image(
    c("thumbnails/hands-helping-grey.png", "thumbnails/store-alt-blue.png",
      "thumbnails/place-of-worship-purple.png",
      "thumbnails/coffee-red.png", "thumbnails/tree-yellow.png"),
    width = 50, height = 50)) %>%
  column_spec(2, background = "white", color = "black") %>%
  cat()

```

## Table A4

```{r tablea4}
# Clear data!
rm(list = ls())

# Load packages!
library(tidyverse)
library(knitr)
library(kableExtra)
library(viridis)

# Load data
dat <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 0) %>%
  # omit the summed groups, so we can look at their subindicators
  filter(!type %in% c("sites", "parks", "social", "community", "rec")) %>%
  # Calculate Descriptive Stats!
  group_by(type) %>%
  summarize(
    nobs = sum(count, na.rm = TRUE),
    mean = mean(rate, na.rm = TRUE),
    sd = sd(rate, na.rm = TRUE),
    min = min(rate, na.rm = TRUE),
    p25 = quantile(rate, probs = 0.25, na.rm = TRUE),
    median = median(rate, na.rm = TRUE),
    p75 = quantile(rate, probs = 0.75, na.rm = TRUE),
    max = max(rate, na.rm = TRUE),
    total = sum(rate, na.rm = TRUE),
    cells = sum(rate > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(nobs,mean:cells),
               names_to = "class", values_to = "stat") %>%
  mutate(class = class %>% recode_factor(
    "nobs" = "Sites",
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25%", "median" = "50%", 
    "p75" = "75%", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells (any)")) %>%
  mutate(group = type %>% recode_factor(
    "library" = "community_space",
    "city_hall" = "community_space",
    "community_center" = "community_space",
    "place_of_worship" ="place_of_worship",
    "bookstore" = "social_business", 
    "cafe" = "social_business", 
    "coffeeshop" = "social_business", 
    "barbershop" = "social_business", 
    "beauty_salon" = "social_business", 
    "park" = "park", 
    "fountain" = "park", 
    "square" = "park", 
    "garden" = "park", 
    "recreation_center" = "park", 
    "sports_field" = "park",
    .default = "other"),
    group = group %>%
      recode_factor(
        "community_space" = "Community Spaces",
        "place_of_worship" = "Places of Worship", 
       "social_business"  = "Social Businesses", 
      "park" =  "Parks", 
      "other" = "Other")) %>%
  mutate(type = type %>% recode_factor(
    #"sites" = "Total Sites\n(n = 1318)",
    "library" = "Libraries\n(n = 108)",
    "city_hall" = "City Hall Facilities\n(n = 43)",
    "community_center" = "Community Centers\n(n = 74)",
    "place_of_worship" = "Places of Worship\n(n = 224)",
    
    "bookstore" = "Bookstores\n(n = 62)",
    "cafe" = "Cafes\n(n = 163)",
    "coffeeshop" = "Coffeeshop\n(n = 0)",
    
    "barbershop" = "Barbershops\n(n = 0)",
    "beauty_salon" = "Beauty Salons\n(n = 0)",
    
    "park" = "Parks\n(n = 288)",
    "fountain" = "Fountains\n(n = 34)",
    "square" = "Squares\n(n = 200)",
    
    "garden" = "Gardens\n(n = 12)",
    "recreation_center" = "Recreation Center\n(n = 0)",
    "sports_field" = "Sports Fields\n(n = 0)", 
    "aquarium" = "Aquariums\n(n = 0)",
    "art_gallery" = "Art Galleries\n(n = 0)", 
    "museum" = "Museum\n(n = 1)",
    "zoo" = "Zoo\n(n = 0)"),
    type = factor(type, levels = levels(type) %>% rev())) %>%
  mutate(coverage = if_else(str_detect(type, "n = 0|n = 1[)]"),
                             "low", "high")) %>%
  pivot_wider(id_cols = c(group, type), names_from = class, 
              values_from = stat) %>% 
  mutate_at(vars(Sites:Total), list(~round(., 1))) %>% 
  # Get order of rows from factor
  mutate(type_order = type %>% as.numeric(),
         group_order = group %>% as.numeric()) %>%
  #filter(!is.na(type)) %>%
  # order
  arrange(desc(type_order), group_order) %>%
 # Extract number of observations & title from HTML
  mutate(#nobs = type %>% str_extract(pattern = "[0-9]+"),
         type = type %>% str_extract(".*\n") %>% str_remove("\n")) %>%
  mutate(` ` = "") %>%
  select(group, ` `, Type = type, Sites, Mean, SD = `Std. Dev.`, Min:Total, Cells = "Cells (any)") %>%
  # Add image source links
  mutate(image = group %>% recode(
    "Community Spaces" = "thumbnails/store-alt-blue.png",
    "Places of Worship" = "thumbnails/place-of-worship-purple.png",
    "Social Businesses" = "thumbnails/coffee-red.png",
    "Parks" = "thumbnails/tree-yellow.png",
    "Other" = "thumbnails/hands-helping-grey.png")) %>%
  mutate(Sites = as.numeric(Sites))

# Take a peek at the raw data!
dat
```

To build our table in LaTeX, use the output from the following function.

```{r tablea1_latex, eval = FALSE}
options(knitr.table.format = "latex")

# GEt colors
mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"

# Make table!
tab %>%
  select(-group, -image) %>%
  kable(format = "latex", booktabs = TRUE, 
        caption = "Descriptive Statistics of API-generated Social Infrastructure Rates (for all Searches)") %>%
  kable_styling() %>%
  pack_rows("Community Spaces", 1, 3, 
            background = mycolors[1], color = "white", hline_before = FALSE) %>%
  pack_rows("Places of Worship", 4, 4, 
            background = mycolors[2], color = "white", hline_before = FALSE) %>%
  pack_rows("Social Businesses", 5, 9, 
            background = mycolors[3], color = "white", hline_before = FALSE) %>%
  pack_rows("Parks", 10, 15, 
            background = mycolors[4], color = "black", hline_before = FALSE) %>%
  pack_rows("Other", 16, 19, 
            background = "gray", color = "black", hline_before = FALSE) %>%
  # Get shading for variables, plus bolding
  column_spec(2, bold = tab$Sites > 10, 
              background = if_else(tab$Sites > 10, "#C5C6D0", "white")) %>%
  # Get shading for all other columns  
  column_spec(3:12, 
              background = if_else(tab$Sites > 10, "#C5C6D0", "white"),
              color = if_else(tab$Sites > 10, "black", "#808080")) %>%
  column_spec(column = 1, background = "white", 
              image = spec_image(tab$image, height = 100, width = 100), 
              width = "1cm", latex_valign = "m") %>%
  collapse_rows(columns = 1, valign = "middle", latex_hline = "major") %>%
  footnote(general = "Rates measured per 1000s persons per 1 km$^{2}$. Keywords which returned high rates of social infrastructure highlighted and shaded \textbf{\textcolor{mygrey}{grey}}. \textbf{Total} depicts total rate of each type of social infrastructure in Boston, per 1000 residents. \textbf{Cells} describe total cells with any of that type found.",
           threeparttable = TRUE) %>%
  cat()
# Not printed, since it's mostly unreadable to non-LaTeX folk, 
# but if you paste it into overleaf, it will look real nice!
```

<br>
<br>

## Figure 1: Point Map!

Let's generate our first figure, a map of all points in Boston found by the API!

```{r figure1}
# Load packages!
library(tidyverse)
library(ggtext)
library(ggimage)
library(magick)
library(viridis)
library(sf)

# Clear data
rm(list = ls())

# Load projections
meta <- dget("meta.txt")

# Load mapping data
source("helper/mapdata.r")

# Grab the 1-square kilometer fishnet grid
fish <- grid %>%
  filter(zone %in% c("validation", "excluded_validation", "boston", "excluded_boston"))
  
mypoints <- read_csv("query/sites_validated.csv") %>%
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  st_transform(crs = meta$aea$proj)  %>%
  # Zoom into just Google API Results
  filter(!is.na(google_id)) %>%
  # Zoom into main types
  filter(type != "Other") %>%
  # Keep just points in study region
  st_join(fish %>% select(zone, geometry), left = FALSE) %>%
  # Add labels  
  group_by(type) %>%
  mutate(ncells = n()) %>%
  mutate(type = type %>% recode_factor(
  "Community Spaces" =   "Community Spaces<br>(n = 192)",
  "Places of Worship" = "Places of Worship<br>(n = 206)",
  "Social Businesses" = "Social Businesses<br>(n = 197)",
  "Parks" = "Parks<br>(n = 433)")) %>%
  ungroup()

# Let's import a little airport image to make 
# it really clear why we don't map that area
airport <- data.frame(x =  -71.0108, 
           y =  42.36255,
           x_aea = 1912748,
           y_aea = 532544.3,
           image = "icons/airport.png")

# Import labels
areas <- read_rds("icons/key_areas.rds") %>%
  st_transform(crs = meta$aea$proj)

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)


g1 <- ggplot() +
  geom_sf(data = boston, color = "darkgrey",size = 5) +
  geom_sf(data = boston, color = "black",fill = "#ffe3be", size = 0.5) +
  geom_sf(data = fish, mapping = aes(linetype = "1 km<sup>2</sup> grid cell"),
          fill = "grey", color = "white", size = 0.5, alpha = 0.5) +
  geom_sf(data = mypoints,
          mapping = aes(fill = type), shape = 21, color = "white", size = 3, alpha = 0.75) + 
  geom_image(data = airport, aes(x = x_aea, y = y_aea, image= image), 
             size = 0.05) +
  scale_color_manual(values = c(mycolors)) +
  scale_linetype_manual(values = "solid") +
  scale_fill_manual(values = c(mycolors)) +
  guides(fill = guide_legend(
    order = 1, 
    override.aes = list(
      size = 5, 
      shape = 21, color = "black"),
    linetype = guide_legend(
      order = 2, 
      override.aes = list(shape = 22, alpha = 0.75, size = 5))),
    color = FALSE) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0.5),
        plot.background = element_rect(fill = "white", color = "white"),
        legend.position  = c(0.77, 0.20),
        legend.text = element_markdown(size = 14),
        legend.background = element_rect(fill = "white", color = NA)) +
  labs(subtitle = paste("Social Infrastructure Sites (n = ", nrow(mypoints), ") in Boston", sep = ""),
       fill = NULL,linetype = NULL) +
    ggspatial::annotation_north_arrow(height = unit(0.75, "cm"), width = unit(0.75, "cm")) +
  ggspatial::annotation_scale(height = unit(0.25, "cm"), pad_x = unit(2, "cm"))  +
  geom_sf_label(data = areas, mapping = aes(label = neighborhood),
                fill = "white", color = "black", alpha = 0.75, size = 5)

# Total number of points has decreased since previous version, 
# because we have now excluded points on the opposite side of the Charles from Boston
# while more accurately captures' the social infrastructure available to the Boston-side of the Charles
ggsave(g1, filename = "figures/figure_1.png", dpi = 750, width = 6, height = 7.5)

rm(list = ls())
```


## Figure 2: Validation Map

Next, let's generate the validation map. This one has many parts, so bear with me!

```{r figure_2}
# Clear data
rm(list = ls())

# Load packages
library(tidyverse)
library(sf)
library(ggspatial)
library(ggimage)

# First, let's get our data!


# Get projections
meta <- dget("meta.txt")

# Get area names
areas <- read_rds("icons/key_areas.rds") %>%
  filter(neighborhood %in% c("Downtown", "Roxbury", "Dorchester"))


# Get tiles
tiles <- read_sf("shapes/grid_covariates.geojson") %>%
  filter(zone %in% c("validation"))

# Get transit (train lines)
lines <- read_sf("shapes/train_lines.geojson") %>%
  st_intersection(tiles %>% select(geometry))
# Get train stops
zone <- read_sf("shapes/train_stops.geojson") %>%
  st_intersection(tiles %>% select(geometry))

# Add 5 basic colors
mycolors <- viridis::plasma(n = 5, begin = 0, end = 0.7)

# Get points
points <- read_sf("data/figure_2_points.geojson") 

# Get tallied rates
stat <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 2) %>%
  group_by(panel, type) %>%
  summarize(count = sum(count)) %>%
  arrange(type, panel) %>%
  mutate(panel = factor(panel, levels = c("Verified\nAPI Sites", "Verified\nNew Sites",
                                 "API Sites\nNot Found", "Not social\ninfrastructure") %>% rev())) %>%
  mutate(type = factor(type, levels = c("Total", "Community Spaces", "Places of Worship", "Social Businesses", "Parks"))) %>%
  mutate(image = type %>% recode_factor(
    "Total" = "icons/hands-helping.png",
    "Community Spaces" = "icons/store-alt.png",
    "Places of Worship" = "icons/place-of-worship.png",
    "Social Businesses" = "icons/coffee.png",
    "Parks" = "icons/tree.png"))

# Get the coordinates for our bounding box
mybox <- list(
  xmin = -71.11,
  xmax = -71.045,
  ymin = 42.275,
  ymax = 42.37) %>%
  unlist() %>%
  # Convert to a bounding box
  st_bbox() %>%
  # Make a polygon sf object from it
  st_as_sfc()  %>%
  st_set_crs(4326) %>%
  # Convert it to Albers Equal Area Conic projection
  st_transform(crs = meta$aea$proj) %>%
  # And grab the final bounding box under that projection
  st_bbox()

# Get total points
nobs <- points %>%
  as_tibble() %>%
  group_by(type) %>%
  summarize(count = n()) %>%
  split(.$type) %>%
  map(~paste("(n = ", .$count, ")", sep = "")) %>%
  unlist()

# Finally, we're going to write another 'visualize' function
# that will create a map.
# We'll use this to generate several maps given varied input points.
visualize = function(points){
  ggplot() +
    geom_sf(data = tiles,  fill = NA, color = "grey", size = 3) +
    geom_sf(data = tiles, fill = "#373737", color = "white", size = 0.1) +
    geom_sf(data = points, mapping = aes(fill = panel), 
            shape = 21, color = "white", size = 2, alpha = 0.80) +
    geom_sf(data = lines, mapping = aes(color = line), 
            color = "white", alpha = 0.75, size = 1.7) +
    geom_sf(data = lines, mapping = aes(color = line, linetype = "*Transit Lines\nby Color"), 
            alpha = 0.95, size = 1.25) +
    theme_void(base_size = 14) +
    theme(plot.background = element_rect(fill = NA, color = "white"),
          plot.title = element_text(hjust = 0.5),
          plot.title.position = "plot",
          plot.subtitle = element_text(size = 14, hjust = 0.5),
          plot.margin = margin(0,0,0,0,"cm"),
          legend.key.height = unit(1.25, "cm")) +
    # Color in T-lines
    scale_color_manual(
      breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
      labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
      values = c("#FE6100", "#DC267F", "#34AA6F", "#648FFF", "#6F94AC", "darkgrey"),
      guide = "none") +
    # Color in points!
    scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                      breaks = c("Verified\nAPI Sites", "Verified\nNew Sites",
                                 "API Sites\nNot Found", "Not social\ninfrastructure"),
                      labels = c("Verified\nAPI Sites", "Verified\nNew Sites",
                                 "API Sites\nNot Found","Not social\ninfrastructure"),
                      guide = guide_legend(order = 1, override.aes = list(size = 5, color = "black"))) +
    # Lines
    scale_linetype_manual(values = "solid", guide = guide_legend(order = 2, override.aes = list(size = 3, color = NA, fill = NA))) +
    coord_sf(crs = meta$aea$proj,
             xlim = mybox[c("xmin", "xmax")],
             ylim = mybox[c("ymin", "ymax")]) +
    # Add labels
    labs(linetype = NULL,color = NULL, fill = NULL)
  
}

# Let's also write a function to generate
# a miniature map for each type of Social Infrastructure
miniature = function(mytype){
  points %>% 
    filter(type == mytype) %>%
    visualize() +
    labs(subtitle = mytype) +
    guides(fill = "none", linetype = "none") +
    geom_text(mapping = aes(x = 1905000,  y = 522000, label = nobs[mytype]), 
              hjust = 0, size = 4) %>%
    return()
}


# Generate one big map to rule them all!
g1 <- points %>% visualize() +
  labs(fill = "Site Verified?",
       subtitle = paste("Total Sites ", nobs["Total"], sep = "")) +
  # Add labels (and move them around a little so they don't cover up the data)
  geom_sf_label(data = areas, mapping = aes(label = neighborhood), 
              alpha = 0.8, nudge_x = c(750, 0, 500), nudge_y = c(000, 0, 500)) # Roxbury

# Generate four mini-maps for each type of social infrastructure
g2 <- miniature("Community Spaces")
g3 <- miniature("Places of Worship")
g4 <- miniature("Social Businesses")
g5 <- miniature("Parks")

# Make 4 panels of bar chars for each subtype
g6 <- stat %>%
  filter(type != "Total") %>%
  ggplot(mapping = aes(x = panel, y = count, fill = panel, label = count, image = image)) +
  geom_image(mapping = aes(x = "API Sites\nNot Found", y = 150), 
             by = "width", size = 0.5, nudge_x = 0.75, alpha = 0.5) +
  geom_col(color = "#373737", width = 0.5) +
  geom_text(hjust = 0, nudge_y = 5, color = "#373737") +
  facet_wrap(~type, ncol =4) +
  coord_flip() +
  scale_y_continuous(breaks = c(0, 100, 200, 300), limits = c(0, 330)) +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black") %>% rev(), guide = "none") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "darkgrey"),
        strip.text = element_blank(),
        axis.line = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.spacing.x = unit(0.5,"cm")) +
  labs(x = NULL, y = "# of Sites")

# Make one giant bar chart for total social infrastructure
g7 <- stat %>%
  filter(type == "Total") %>%
  ggplot(mapping = aes(x = panel, y = count, fill = panel, label = count, image = image)) +
  geom_image(mapping = aes(x = "API Sites\nNot Found", y = 325), 
             by = "width", size = 0.5, nudge_x = 0.75, alpha = 0.5) +
  geom_col(color = "#373737", width = 0.5) +
  geom_text(hjust = 0, nudge_y = 5, color = "#373737") +
  shadowtext::geom_shadowtext(mapping = aes(x = as.numeric(panel) + 0.4, y = 0, 
                                      label = str_replace(panel, "\n", " ")),
                          hjust = 0, color = "#373737", bg.color = "white", bg.r = 0.2) +
  facet_wrap(~type, ncol =4) +
  coord_flip() +
  scale_y_continuous(breaks = c(0,100,200,300,400,500,600), limits = c(0, 640)) +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black") %>% rev(), guide = "none") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "darkgrey"),
        #strip.text = element_blank(),
        axis.line = element_blank(),
        axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        panel.spacing.x = unit(0.5,"cm")) +
  labs(x = NULL, y = "# of Sites")

# Bind the total map and total bar chart together to make the first row
first <- ggpubr::ggarrange(g1, g7,
                           ncol = 2, widths =  c(0.65, 0.35), 
                           labels = c("A", ""), hjust = c(-0.5, 0.5)) 
# Bind the minimaps together to make the second row
second <- ggpubr::ggarrange(g2, g3, g4, g5, ncol = 4, legend = "none")
# Bind the first and second row together with the 4-panel bar charts
# to make the final visual
total <- ggpubr::ggarrange(
  # List visuals
  first, second, g6, 
  # Specify traits
  nrow = 3, heights = c(0.45, 0.25, 0.30),
  labels = c("", "B", ""), vjust = c(0, -0.25, 0)) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

# Save to file!
ggsave(total, filename = "figures/figure_2.png", dpi = 500, width = 8, height = 10)

# Clear data
rm(list = ls())
```



## Visualization Functions

Next, we'll write a few functions used below to make visuals for Figure 3 and appendix figures.

```{r get_visualization_functions}
library(tidyverse)
library(sf)
library(ggtext)
library(ggpubr)

# Write a function to count the total cells and make panel labels
relabel = function(data){
  mylabels <- data %>%
    select(panel, cell_id) %>%
    distinct() %>%
    group_by(panel) %>%
    summarize(ncells = n()) %>%
    mutate(panel_label = panel %>% recode_factor(
      "online" = "<b>Online<br>Map</b><br><br>
           <i>(Manual<br>Online<br>Searches)</i><br>",
      "ground_truthed" = "<b>Ground<br>Truthed<br>Map</b><br><br>
           <i>(In-Person<br>Visits)</i><br>")) %>%
    mutate(panel_label = paste(panel_label, "<br>", ncells, " cells", sep = ""),
           # Order the factor
           panel_label = fct_reorder(panel_label, panel, .desc = TRUE)) %>%
    select(panel, panel_label)
  # Join in the labels
  data %>%
    left_join(by = "panel", y = mylabels) %>%
    # Recode types
    mutate(type = type %>% recode_factor(
      "Total" = "Total",
      "Community Spaces" = "Community\nSpaces",
      "Places of Worship" = "Places\nof Worship",
      "Social Businesses" = "Social\nBusinesses",
      "Parks" = "Parks")) %>%
    return()
}


# Let's write a jumbo function to generate necessary components.
generate = function(tally){
  
  # Get model list 'm'
  m <- tally %>%
    # For each type-panel pair (10 total)
    group_by(total, type, panel, panel_label) %>%
    # nest the data
    nest() %>%
    mutate(
      # Run a model for each
      model = map(data, ~lm(y ~ x, data = .)),
      # Gather regression points for each grouping
      points = map(model, ~moderndive::get_regression_points(.) %>%
                     mutate(residual = abs(residual))),
      # Get correlation
      cor = map(data, ~cor(.$y, .$x, use = "pairwise.complete.obs") %>% 
                  round(2) %>% paste("r = ", ., sep = "")) %>% unlist(),
      # Get goodness of fit statistics
      map_dfr(model, ~broom::glance(.) %>%
                select(r2 = r.squared, sigma) %>%
                mutate(r2 = paste(round(r2*100, 0), 
                                  "% accuracy (R2)", sep = ""),
                       sigma = paste("Avg. Error = ", round(sigma, 1), sep = ""))),
      # Get largest xvalue
      xmax = map(points, ~max(.$x, na.rm = TRUE)) %>% unlist(),
      xmin = map(points, ~min(.$x, na.rm = TRUE)) %>% unlist()) %>%
    ungroup() %>%
    # Calculate the best range of x to use for predictions
    group_by(type) %>%
    mutate(xmax = max(xmax),
           xmin = min(xmin)) %>%
    ungroup() %>%
    mutate(
      # Generate 'newdata' to feed predictions for each panel
      newdata = map2(.x = xmin, .y = xmax, .f = ~tibble(x = seq(.x, .y, length.out = 20))),
      # Generate predictions
      predictions = map2(.x = model, .y = newdata, 
                         .f = ~predict(.x, se = TRUE, newdata = .y) %>%
                           as_tibble() %>%
                           mutate(x = .y$x) %>%
                           select(yhat = fit, se = se.fit, x) %>%
                           mutate(upper_ci = yhat + se * 1.96,
                                  lower_ci = yhat - se * 1.96)),
      
      # Let's make a copy of points
      x = map(points, ~.$x),
      y = map(points, ~.$y)) %>%
    # Now, let's do some more calculations, using the results outputs
    unnest(c(x, y)) %>%
    ungroup() %>%
    # Now for each panel/group set,
    group_by(total, panel) %>%
    nest()  %>%
    mutate(yposition = map(data, ~tibble(
      yrange = range(.$y, na.rm = TRUE) %>% mean(),
      y100 = max(.$y, na.rm = TRUE) - yrange / 8,
      ymin = min(.$y, na.rm = TRUE),
      y75 = ymin + (y100 - ymin)*0.75,
      y50 = ymin + (y100 - ymin)*0.5,    
      ymax = max(.$y, na.rm = TRUE),
      ynew = min(.$y, na.rm = TRUE) + yrange))) %>%
    unnest() %>%
    ungroup() %>%
    # Now for each type of social infrastructure
    group_by(total, type) %>%
    nest() %>%
    mutate(xposition = map(data, ~tibble(
      xrange = range(.$x, na.rm = TRUE) %>% mean(),
      #xmin = min(.$x, na.rm = TRUE) + xrange / 20,
      #xmax = max(.$x, na.rm = TRUE),
      xnew = min(.$x, na.rm = TRUE) + xrange)) ) %>%
    unnest() %>%
    ungroup() %>%
    select(-x, -y) %>%
    distinct() %>%
    # Add a unique ID 
    mutate(group = 1:n())
  
  # Get position data
  mypositions <- m %>%
    select(total, group, type, panel, panel_label, total, contains("y"), contains("x"))
  
  # Get bands
  mybands <- m %>%
    select(total, group, type, panel, panel_label, predictions) %>%
    unnest()
  
  # Get points
  mypoints <- m %>%
    select(total, group, type, panel, panel_label, points) %>%
    unnest()
  
  # Get text
  mystat <- m %>%
    select(total, group, type, panel, panel_label,
           cor, r2, sigma, contains("y"), contains("x"))
  
  # Get scales
  xscales <- mypositions %>%
    select(type, xmin, xmax, xrange) %>%
    distinct() %>%
    mutate(action = paste("scale_x_continuous(", "limits = c(", xmin, ", ", xmax, ")", ")", sep = "")) %>%
    # Name them
    split(.$type) %>%
    map(~.$action) %>%
    map(~parse(text = .) %>% eval())
  
  yscales <- mypositions %>%
    select(type, panel_label, ymin, ymax, yrange) %>%
    distinct() %>%
    mutate(action = paste("type == ", "'", type, "' & panel_label == ", "'", panel_label, "'", " ~ ", 
                          "scale_y_continuous(", "limits = c(", ymin, ", ", ymax, ")", ")", 
                          sep = "")) %>%
    with(action) %>%
    as.list() %>%
    map(~parse(text = .) %>% eval())
  
  # Return output
  list(m, mypositions, mybands, mypoints, mystat, xscales, yscales) %>%
    set_names(c("m", "mypositions", "mybands", "mypoints", "mystat", "xscales", "yscales")) %>%
    return()
}

visualize = function(g, jitter = FALSE){
  #install.packages("devtools")
  #devtools::install_github("teunbrand/ggh4x")
  library(ggh4x)
  
  if(jitter == TRUE){
  ggviz <- ggplot() +
    # Put jittered points
    geom_jitter(data = g$mypoints,
               mapping = aes(x = x, y = y, fill = type, group = group),
               shape = 21, color = "white", size = 3)
  
  }else{
    
  ggviz <- ggplot() +
    # Put lines from yhat to y
    geom_linerange(data = g$mypoints, mapping = aes(x = x, ymin = y, ymax = y_hat), color = "grey", size = 0.6, alpha = 0.75) +
    
    # Put raw points
    geom_point(data = g$mypoints,
               mapping = aes(x = x, y = y, fill = type, group = group),
               shape = 21, color = "white", size = 3)
    
  }
  
  ggviz +
    # Get lines
    geom_line(data = g$mybands, 
              mapping = aes(x = x, y = yhat, group = group), color = "black", linetype = "dashed", size = 1) +
    # Get ribbons
    geom_ribbon(data = g$mybands,
                mapping = aes(x = x, ymin = lower_ci, ymax = upper_ci, 
                              fill = type, group = group), color = "white",
                alpha = 0.5) +
    # Text
    shadowtext::geom_shadowtext(data = g$mystat, mapping = aes(x = xmin, y = y100, label = r2, group = group), 
                          fill = NA, bg.color = "white", bg.r = 0.2, 
                          hjust = 0, color = "black", size = 3) +
    shadowtext::geom_shadowtext(data = g$mystat, mapping = aes(x = xmin, y = y75, label = sigma, group = group), 
                          fill = NA, bg.color = "white", bg.r = 0.2, 
                          hjust = 0, color = "black", size = 3) +
    shadowtext::geom_shadowtext(data = g$mystat, mapping = aes(x = xmin, y = y50, label = cor, group = group), 
                          fill = NA, bg.color = "white", bg.r = 0.2, 
                          hjust = 0, color = "black", size = 3) +
    # Split into nice grid
    facet_nested_wrap(panel_label ~ type, scales = "free", drop = TRUE, ncol = 5,
                      strip = strip_split(
                        position = c("right", "top"),
                        background_x = list(
                          element_rect(fill = "black", color = "#373737", size = 0.5),
                          element_rect(fill = "#0D0887", color = "#373737", size = 0.5),
                          element_rect(fill = "#8405A7", color = "#373737", size = 0.5),
                          element_rect(fill = "#D35171", color = "#373737", size = 0.5),
                          element_rect(fill = "#d98d2b", color = "#373737", size = 0.5)),
                        background_y = list(
                          element_rect(fill = "grey", color = "#373737", size = 0.1),
                          element_rect(fill = "grey", color = "#373737", size = 0.1)))) + 
    facetted_pos_scales(x = g$xscales,
                        y = g$yscales) +
    scale_color_manual(values = c("black", "#0D0887", "#8405A7",
                                  "#D35171","#d98d2b")) +
    scale_fill_manual(values= c("black", "#0D0887", "#8405A7",
                                "#D35171","#d98d2b")) +
    
    guides(fill = "none", color = "none") +
    theme_classic(base_size = 12) +
    theme(
      axis.line = element_blank(),
      panel.border = element_rect(fill = NA, color = "#373737"),
      axis.ticks.x = element_line(color = "#373737", size = 0.2),
      axis.ticks.y = element_line(color = "#373737", size = 0.2),
      axis.text = element_text(color = "#373737"),
      strip.text.x = element_text(hjust = 0.5, size = 10, color = "white", face = "bold"),
      strip.text.y = ggtext::element_markdown(size = 10, angle = 0, hjust = 0),
      plot.subtitle = element_text(hjust = 0.5, size = 12)) +
    labs(x = "Rates of Human-Coded Sites per Grid Cell",
         y = "Google API Rates of Sites per Grid Cell") %>%
    return()
}

# Save these to file so we can call them at any time
save(generate, relabel, visualize, file = "helper/functions_fig3.rdata")
```

## Figures 3, B2-B5

Next, we're going to make a nice comparison of API vs. online vs. groundtruthed cell rates.

```{r figure3}
# Load packages
library(tidyverse)
library(sf)
library(ggtext)
library(ggpubr)
library(ggh4x)

# Load visualization functions
load("helper/functions_fig3.rdata")

# Get rates
tally <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 3) %>%
  # Simplify names
  mutate(panel = panel %>% tolower() %>% str_replace_all(" ", "_")) %>%
  pivot_wider(id_cols = c(zone, type, cell_id, pop_density_int),
              names_from = panel,
              values_from = rate) %>%
  pivot_longer(cols = c(online,ground_truthed),
               names_to = "panel", values_to = "x", 
               # Narrow into just our many online cells and few ground-truthed cells
               values_drop_na = TRUE) %>%
  rename(y = google_api) %>%
  mutate(total = if_else(type == "Total", TRUE, FALSE)) %>%
  # Affix labels
  relabel() 

# Generate Figure 3!
g1 <- tally %>% generate() %>% visualize() +
  labs(subtitle = "Cells by Actual Rates of Social Infrastructure per 1,000 residents")

# Save!
ggsave(g1, filename = "figures/figure_3.png", dpi = 500, width = 10, height = 5)
# Make a copy, to be Figure B2, for comparison with Figures B3-B5.
file.copy(from = "figures/figure_3.png", to = "figures/figure_B2.png")

# Generate Figure B3!
# This one rescales rates as z-scores before making lines
g2 <- tally %>%
  # For each group, rank cells from smallest to highest
  group_by(type, panel) %>%
  mutate_at(vars(x, y),
            # By quintiles
            funs(scale(.) %>% as.numeric())) %>%
  ungroup() %>%
  generate() %>% visualize() +
  labs(subtitle = "Cells Ranked by Z-score (Standard Deviations from the Mean)")

# Save to file!
ggsave(g2, filename = "figures/figure_B3.png", dpi = 500, width = 10, height = 5)


# Generate Figure B4
# This one rescales rates by decile before making lines
g3 <- tally %>%
  # For each group, rank cells from smallest to highest
  group_by(type, panel) %>%
  mutate_at(vars(x, y),
            # By decile
            funs(ntile(., 10))) %>%
  ungroup() %>%
  generate() %>% visualize(jitter = TRUE) +
  labs(subtitle = "Cells Ranked by Decile (1 = Lowest, 10 = Highest)")

# Save to file!
ggsave(g3, filename = "figures/figure_B4.png", 
       dpi = 500, width = 10, height = 5)


# Generate Figure B5!
# This one rescales rates by quintile before making lines
g4 <- tally %>%
  # For each group, rank cells from smallest to highest
  group_by(type, panel) %>%
  mutate_at(vars(x, y),
            # By quintile
            funs(ntile(., 5))) %>%
  ungroup() %>%
  generate() %>% visualize(jitter = TRUE) +
  labs(subtitle = "Cells Ranked by Quintile (1 = Lowest, 5 = Highest)")

# Save to file!
ggsave(g4, filename = "figures/figure_B5.png", 
       dpi = 500, width = 10, height = 5)

# Clear data!
rm(list = ls())
```

## Figure 4: Tile Map

```{r figure4}
# clear data
rm(list = ls())

# Load Map data
meta <- dget("meta.txt")

# Gather typical map data files
source("helper/mapdata.r") 

# Adjust the grid
grid <- grid %>%
  filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
  select(cell_id, pop_black, pop_hisplat, geometry) %>%
  mutate(race = if_else(pop_black > 0.50 | pop_hisplat > 0.50,
                        true = "> 50% Black/Hispanic",
                        false = "Other")) 
# Get a nice outline of the grid
boston <- grid %>%
  summarize(geometry = st_union(geometry))

# Import tallied rates for figure 4
tally <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 4) %>%
  # Classify cells by present of ANY sites
  mutate(sig = if_else(count > 0, "Any", "None", missing = "None")) %>%
  select(-geometry) %>%
  left_join(by = "cell_id", y = grid) %>%
  st_as_sf(crs = meta$aea$proj) %>%
    mutate(type = type %>% recode_factor(
  "Total" = "Total Sites\n(n = 1018)",
  "Community Spaces" =   "Community Space\n(n = 192)",
  "Places of Worship" = "Places of Worship\n(n = 206)",
  "Social Businesses" = "Social Businesses\n(n = 197)",
  "Parks" = "Parks\n(n = 423)")) %>%
  ungroup()

#tally %>% 
#  group_by(type) %>%
#  summarize(count = sum(count))

# Get colors
mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"


# Make an object that will tell ggspatial to only put the legend in one plot,
# so as not to take up too much space
legenddata <- tally %>% 
  filter(type == "Total") %>%
  select(type) %>%
  slice(1)

g1 <- ggplot() +
  geom_sf(data = boston, color = "darkgrey", size = 1.25) +
  geom_sf(data = tally, mapping = aes(fill = rate, size = sig, color = sig)) +
  geom_sf(data = tally %>% filter(rate > 0), fill = NA, color = "white", size = 0.15) +
  # Add fishnet grid for race
  geom_sf(data = grid %>% filter(race != "Other"), mapping = aes(linetype = race),
          fill = NA, size = 0.5, color = "black") +
  facet_wrap(~type, ncol = 5,shrink = TRUE) +
  viridis::scale_fill_viridis(option = "plasma", na.value = "grey",
                              trans = "log",
                              labels = function(x) round(x, 2)) +
  scale_size_manual(values = c(0.2, 0.1)) +
  scale_color_manual(values = c("white", "#777777")) +
  scale_linetype_manual(values = "solid")  +
  theme_void(base_size = 12) +
  theme(legend.position = "bottom",
        legend.title = element_text(hjust = 1, size = 10),
        plot.subtitle = element_text(hjust= 0.5, vjust = 2.5),
        strip.text.x = ggtext::element_markdown(size = 11),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = ggtext::element_markdown(size = 10, hjust = 0.5)) +
  # Edit legends
  guides(linetype = guide_legend(order = 2, override.aes = list(
    shape = 22, fill = "white", color = "black", size = 0.5)),
    size = "none",color = "none",
    fill = guide_colorsteps(order = 1, barwidth = 15, barheight = 1, show.limits = TRUE)) +
  # Add labels
  labs(fill = "Sites per 1000 residents", linetype = NULL) +
  
  # Add north arrow and scale
  ggspatial::annotation_north_arrow(data = legenddata, height = unit(0.4, "cm"), width = unit(0.4, "cm")) +
  ggspatial::annotation_scale(data = legenddata, pad_x = unit(2, "cm")) +
  # Add some more theming
  theme(strip.text.x = element_blank(),
        plot.margin = margin(t = 0, r = 0,l = 0, b = 0, unit = "cm"))


bostonpoc <- grid %>%
  filter(race != "Other") %>%
  as_tibble() %>%
  select(cell_id) %>% unlist()

# Let's quickly generate a nice transparent version of our icons
library(rsvg)
library(magick)

# write a function to update images 
update = function(file){
  core <- file %>% str_remove("icons[/]") %>% str_remove("[.]png")
  i <- magick::image_read(file) 
  image_colorize(i, 75, "white") %>%
    image_write(paste("icons/", core, "-white.png", sep = ""), quality = 100)
}

# Repeat on each of these files
c("icons/hands-helping.png", "icons/store-alt.png", "icons/place-of-worship.png",  "icons/coffee.png", "icons/tree.png") %>%
  lapply(update)

# Generate a final data.fram to hold images
myimages <- tibble(
  tally %>%
  as_tibble() %>%
  group_by(type) %>%
  summarize(
    mean = mean(rate, na.rm = TRUE) %>% round(2),
    mean_poc = mean(if_else(condition = cell_id %in% bostonpoc,
                true = rate, false = NA_real_), na.rm = TRUE) %>% round(2)),
  
  geometry = boston$geometry %>% st_centroid(),  
  st_coordinates(geometry) %>% as_tibble() %>% select(x = 1, y = 2),
  color = c("black", mycolors),
  image = c("hands-helping-white.png", "store-alt-white.png", "place-of-worship-white.png",  "coffee-white.png", "tree-white.png") %>%
    paste("icons/", ., sep = ""),
  size = c(.6, .5, .5, .6, .3),
)

# Make a derivative part
myblocks <- myimages %>%
  group_by(type, color) %>%
  summarize(y = 0:3)

# Make the second visual!
g2 <- ggplot() +
  # Add a second layer
  geom_tile(data = myblocks, mapping = aes(x = type, y = y, fill = type, color = type), size = 5) +
  # Get image
  ggimage::geom_image(data = myimages, mapping = aes(image = image, x = type, y = 2.5), 
                      size = myimages$size) +
  # Add overall label
  geom_text(data = myimages, mapping = aes(x = type, y = 1.5, label = "Average Rate"),
            size = 3, color = "white") +
  # Add overall rate
  geom_text(data = myimages, mapping = aes(x = type, y = 1, label = mean), size = 6, color = "white") +
  # Add label for communities of color
  geom_text(data = myimages, mapping = aes(x = type, y = 0.5, label = "Neighborhoods of Color"),
            size = 3, color = "white") +
  # Add rate for communities of color
  geom_text(data = myimages, mapping = aes(x = type, y = 0, label = paste("(", mean_poc, ")", sep = "")), 
            size = 6, color = "white") +
  scale_fill_manual(breaks = myimages$type %>% levels(), values = c("black", mycolors), guide = "none") +
  scale_color_manual(breaks = myimages$type %>% levels(), values = c("black", mycolors), guide = "none") +
  scale_x_discrete(position = "top") +
  facet_wrap2(~type, scales = "free", ncol = 5, 
              strip = ggh4x::strip_themed(text_x = list(
                
                element_text(face = "bold", size = 10, color = "black"),
                element_text(face = "bold", size = 10,color = mycolors[1]),
                element_text(face = "bold", size = 10,color = mycolors[2]),
                element_text(face = "bold", size = 10,color = mycolors[3]),
                element_text(face = "bold", size = 10,color = "#D29204")))) +
  theme_void(base_size = 14) +
  theme(
    #strip.text = ggtext::element_markdown(size = 10, hjust = 0.5, padding = unit(0, "cm")),
    plot.subtitle = element_text(hjust = 0.5, vjust = 5),
    panel.spacing = unit(0, "cm"),
    panel.spacing.x = unit(0, "cm"),
    plot.margin = margin(t = 0.5, r = 0.0, l = 0.0, b = 0, unit = "cm"))  +
  labs(subtitle = "Social Infrastructure Rates in Boston City Blocks")

# Join them together!
combo <- ggpubr::ggarrange(
  g2, g1,
  ncol = 1, heights = c(0.8, 0.8))

# And save it!
ggsave(combo, filename = "figures/figure_4.png", dpi = 500, width = 7.5, height = 5)
```

<br>
<br>

## Figure 5 & C4: Race & Ethnicity

```{r figure5}
# Load packages
library(tidyverse)
library(sf)
library(ggh4x)

# Clear data
rm(list = ls())

# Load Map data
meta <- dget("meta.txt")

# Gather typical map data files
source("helper/mapdata.r") 

# Adjust the grid
grid <- grid %>%
  filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
  select(cell_id, pop_black, pop_hisplat, geometry) %>%
  # Repeat grid 4 times
  expand_grid(., threshold = c(30, 40, 50, 60, 70)) %>%
  mutate(race = case_when(
    
    pop_black > threshold/100 ~  "Black/Hispanic",
    pop_hisplat > threshold / 100 ~ "Black/Hispanic",
    TRUE ~ "Other"))

# Import tallied rates for figure 4
tally <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 4) %>%
  # Classify cells by present of ANY sites
  mutate(sig = if_else(count > 0, "Any", "None", missing = "None")) %>%
  select(-geometry) %>%
  # Impute zeros with very small value to make it log-transformable
  group_by(type) %>%
  mutate(rate = if_else(rate == 0, sort(unique(rate))[2] / 2, rate)) %>%
  ungroup() %>%
  left_join(by = "cell_id", y = grid) %>%
  st_as_sf(crs = meta$aea$proj) %>%
    mutate(type = type %>% recode_factor(
  "Total" = "Total Sites\n(n = 1018)",
  "Community Spaces" =   "Community Spaces\n(n = 192)",
  "Places of Worship" = "Places of Worship\n(n = 206)",
  "Social Businesses" = "Social Businesses\n(n = 197)",
  "Parks" = "Parks\n(n = 423)")) %>%
  ungroup() %>%
   mutate(threshold = threshold %>% recode_factor(
    `30` = ">30%\nBlack / Hispanic\n(n = 64 vs. 98)", 
    `40` = ">40%\nBlack / Hispanic\n(n = 49 vs. 113)", 
    `50` = ">50%\nBlack / Hispanic\n(n = 30 vs. 132)", 
    `60` = ">60%\nBlack / Hispanic\n(n = 19 vs. 143)", 
    `70` = ">70%\nBlack / Hispanic\n(n = 5 vs. 157)"))

# Calculate the means for each group!
mydiff <- tally %>%
  as_tibble() %>%
  group_by(type, race, threshold) %>%
  summarize(mean = mean(rate, na.rm = TRUE),
            n = n()) %>%
  ungroup() %>%
  mutate(group = if_else(race == "Other", "Other", as.character(type))) %>%
  mutate(race = race %>% recode_factor(
    "Black/Hispanic" =  "Black/\nHispanic",
    "Other" = "Other"))  %>%
  group_by(type) %>%
  mutate(ybase = mean(mean) / 4,
         ymid = mean(mean)) %>%
  ungroup() 

# Run the t-tests, by nesting!
mytest <- tally %>%
  as_tibble() %>%
  group_by(type, threshold) %>%
  nest() %>%
  mutate(
    model = map(data, ~infer::t_test(x = ., rate ~ race, order = c("Black/Hispanic", "Other")))) %>%
  select(model) %>%
  unnest() %>%
    mutate(sig = if_else(p_value < 0.05, "p < 0.05", "insignificant")) %>%
  mutate(diff = paste(round(estimate, 2), gtools::stars.pval(p_value), sep = "")) %>%
  # Join in the y-axis point for the label
  left_join(by = c("type"), y = mydiff %>% 
              select(type, ymid) %>%
              distinct())

# Write a new visualization function
visualize = function(mydiff, mytest){
  ggplot() +
    geom_col(data = mydiff,
             mapping = aes(x = as.numeric(race), y = mean, fill = group, color = race),
             size = 0.5, width = 0.75) +
    geom_line(data = mydiff,
              mapping = aes(x = as.numeric(race), y = mean, group = 1)) +
    geom_point(data = mydiff, 
               mapping = aes(x = as.numeric(race), y = mean, fill = group, color = race),
               shape = 21, fill = "white", size = 2) +
    geom_text(data = mydiff %>%
                filter(race == "Black/\nHispanic"),
              mapping = aes(x = as.numeric(race), y = ybase, group = group,
                            label = round(mean, 2)), color = "white") +
    geom_text(data = mydiff %>%
                filter(race != "Black/\nHispanic"),
              mapping = aes(x = as.numeric(race), y = ybase, group = group,
                            label = round(mean, 2)), color = "black") +
    geom_label(data = mytest,
               mapping = aes(x = 1.5, y = ymid, label = diff), fill = "white", alpha = 0.75) +
    scale_x_continuous(breaks = c(1, 2),
                       labels = c("Black/\nHispanic", "Other")) +
    theme_bw(base_size = 14) +
    theme(strip.text.y = element_text(size = 14, angle = 0, hjust = 0),
          axis.text.x = element_text(size = 14),
          legend.text = ggtext::element_markdown(size = 14),
          strip.background.y = element_blank(),
          plot.caption = ggtext::element_markdown(size = 10, hjust = 0),
          strip.background.x = element_rect(fill = "black", color = "white"),
          strip.text.x = element_text(size = 12, color = "white"),
          panel.grid = element_blank(),
          plot.subtitle = element_text(hjust = 0.5)) +
    scale_color_manual(values = c("black", "darkgrey")) +
    scale_fill_manual(
      breaks = c(mytest$type %>% levels(), "Other"),
      values = c("black", "#0D0887", "#8405A7", "#D35171", "#d98d2b", "grey")) +
    guides(color = "none", fill = "none") +
    labs(subtitle = "Lower Rates of Social Infrastructure in Black/Hispanic Neighborhoods", 
         y = "Average Rate of Social Infrastructure\nper 1,000 residents",
         x = "Boston Cells by Resident Race & Ethnicity")  %>%
    return()
}

# Make the jumbo visual!
g1 <- visualize(mydiff, mytest) +
  facet_grid2(
      rows = vars(type), 
      cols = vars(threshold), scales = "free_y",
      strip = ggh4x::strip_themed(
        text_y = list(
          element_text(face = "bold", size = 10, color = "black"),
          element_text(face = "bold", size = 10,color = "#0D0887"),
          element_text(face = "bold", size = 10,color = "#8405A7"),
          element_text(face = "bold", size = 10,color = "#D35171"),
          element_text(face = "bold", size = 10,color = "#d98d2b")))) 

# SAve first one to file
ggsave(g1, filename = "figures/figure_C4.png", dpi = 500, width = 10, height = 6.8)

# Now zoom into just a subset of them;
# we'll use this one in the main results
g2 <- visualize(mydiff %>% filter(threshold %>% str_detect("50[%]")), 
          mytest  %>% filter(threshold %>% str_detect("50[%]"))) +
  facet_wrap2(
      #cols = vars(threshold), 
      ~type, scales = "free_y",ncol = 5,
      strip = ggh4x::strip_themed(
        text_x = list(
          element_text(face = "bold", size = 10, color = "black"),
          element_text(face = "bold", size = 10,color = "#0D0887"),
          element_text(face = "bold", size = 10,color = "#8405A7"),
          element_text(face = "bold", size = 10,color = "#D35171"),
          element_text(face = "bold", size = 10,color = "#d98d2b")),
        background_x = element_rect(fill = NA, color = NA))) 
    
# Save to file!
ggsave(g2, filename = "figures/figure_5.png", dpi = 500, width = 12, height = 5)

# Clear data
rm(list = ls())
```

## Figure 6: Neighborhood Comparison

```{r figure6}
# Clear
rm(list = ls())

# Get projection
meta <- dget("meta.txt")

#Download neighborhood shapefile [here](https://data.boston.gov/dataset/boston-neighborhoods1).

# Import tallied rates for figure 4
tally <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 4) %>%
  select(-geometry) %>%
  left_join(by = "cell_id",
            y = read_sf("shapes/grid_covariates.geojson") %>%
              select(cell_id, geometry)) %>%
  st_as_sf(crs = 4326)  %>%
  mutate(type_label = type %>% recode_factor(
    "Total" = "Total Sites\n(n = 1018)",
    "Community Spaces" =   "Community Spaces\n(n = 192)",
    "Places of Worship" = "Places of Worship\n(n = 206)",
    "Social Businesses" = "Social Businesses\n(n = 197)",
    "Parks" = "Parks\n(n = 423)")) 

# Import and calculate average rate per neighborhood
neighbor <- read_sf("shapes/neighborhoods.geojson") %>%
  st_join(tally) %>%
  group_by(name, type, type_label) %>%
  summarize(rate = mean(rate, na.rm = TRUE)) %>%
  ungroup()  %>%
  as_tibble() %>%
  select(-geometry)

# update file
neighbor <- neighbor %>%
  # Add a nice rounded label
  mutate(label = round(rate, 2)) %>%
  # Add a nice easy neighborhood name
  mutate(name = if_else(name == "South Boston Waterfront", 
                        "S.B. Waterfront", name))  %>%
  # Calculate an artificial endpoint to add to every scale
  group_by(type) %>%
  mutate(yedge = max(rate) + mean(rate) / 1.5) %>%
  ungroup() %>%
  # Calculate the range, divided by 10, for easily nudging labels
  group_by(type) %>%
  mutate(ynudge = (max(rate) - min(rate)) / 10)

# Tally them up!
mysum <- neighbor %>%
  group_by(type, type_label) %>%
  summarize(max = max(rate, na.rm = TRUE),
            min = min(rate, na.rm = TRUE),
            range = max - min,
            y = max - range / 2,
            x = "Roxbury") %>%
  mutate(myname = type %>% recode(
    "Total" = "hands-helping-grey",
    "Community Spaces" = "store-alt-blue",
    "Places of Worship" = "place-of-worship-purple",
    "Social Businesses" = "coffee-red",
    "Parks" = "tree-yellow")) %>%
  mutate(icon = paste("icons/", myname, ".png", sep = "")) 
  
# Make image!
g1 <- ggplot() +
  # Add lines
  geom_linerange(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, ymin = 0, ymax = rate),
    color = NA) +
  # Add background images
  geom_image(
    data = mysum, 
    mapping = aes(x = x, y = y, image = icon),
    size = 0.6) +
  # Add lines again over top
  geom_linerange(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, ymin = 0, ymax = rate)) +
  # Add a fake invisible point just a little bit ahead of each line
  # This is a trick that will tell ggplot to show us a little more of each x-axis,
  # so we can more clearly see the labels we're about to add
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate + ynudge*5, 
                  color = variable), size = 2.9, color = NA) +
  geom_label(
    data = neighbor,
    mapping = aes(x = reorder(name, rate),
                  y = rate + ynudge, label = label), 
    hjust = 0, label.padding = unit(0.05, "cm"), 
    size = 3.5, color = "#59515E", # smoke
    fill = NA, label.size = NA) +
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate), size = 3) +
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, color = type_label), size = 2.9) +
  scale_color_manual(values = c("black", "#0D0887", "#8405A7", "#D35171", "#d98d2b")) +
  facet_grid2(~type_label, scales = "free",
              strip = ggh4x::strip_themed(
        text_x = list(
          element_text(face = "bold", size = 10, color = "black"),
          element_text(face = "bold", size = 10,color = "#0D0887"),
          element_text(face = "bold", size = 10,color = "#8405A7"),
          element_text(face = "bold", size = 10,color = "#D35171"),
          element_text(face = "bold", size = 10,color = "#d98d2b")),
        background_x = element_rect(fill = NA, color = NA))) +
  coord_flip() +
  theme_bw(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    axis.text.x = ggtext::element_markdown(size = 11),
    axis.text.y = element_text(size = 13),
    strip.text.y = ggtext::element_markdown(size = 14),
    strip.text.x = ggtext::element_markdown(size = 14),
    strip.background = element_blank(),
    plot.subtitle = element_text(size = 15, hjust = 0.5),
    panel.background = element_rect(fill = "white", color = "white"),
    panel.border = element_blank(),
    panel.spacing = unit(0.25, "cm")) +
  labs(x = NULL,
       y = "Average Rate of Social Infrastructure per 1,000 residents (per square kilometer)",
       subtitle = "Social Infrastructure by Neighborhood") +
  guides(color = "none")

# Save to file
ggsave(g1, filename = "figures/figure_6.png", 
       dpi = 500, width = 10, height = 6)

# Clear data
rm(list = ls())
```

<br>
<br>

# Appendix Figures

Finally, let's generate the appendix figures!

## Figure A1: Search Methods

There are three ways to search for data. First, we could use a search term, which is a query of any kind you want. This tends to return popular results, meaning it undercounts social infrastructure substantially, instead returning the most popular churches in the city, for example. Second, you could use keywords. This is highly effective, but you can only use one.

```{r figurea1, eval = FALSE}
# Let's write a function to format results
convert_points = function(mydata){
  mydata$results %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
    st_transform(crs = aea)  %>%
    # Grab just the distinct results
    group_by(place_id) %>%
    summarize_at(vars(name, business_status, 
                      #plus_code.compound_code, 
                      rating, types, user_ratings_total, #vicinity, 
                      #permanently_closed,
                      geometry),
                 funs(unique(.))) %>%
    ungroup() %>%
    # Classify the type
    mutate(types = .$types %>%
             map(~paste(., collapse = "; ")) %>%
             unlist()) %>%
    return()
  
}

# Get projections
meta <- dget("meta.txt")
# In case you've deleted it (I had)
# re-summon your Google Places API key from the environment
mykey <- Sys.getenv("GOOGLEWAY_KEY")

# Get University Location
neu <- data.frame(lon = c(-71.0945748), lat = c(42.3403499)) %>%
  st_as_sf(crs = meta$wgs$proj, coords = c("lon", "lat")) %>%
  st_transform(crs = meta$aea$proj)
# Located in grid cell 124

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 1000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat) %>%
  filter(cell_id == 124)

# Let's do this for all of Boston
run_string <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  search_string = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_keyword <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_type <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  place_type = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

dat <- bind_rows(
  run_string %>%
    convert_points() %>% mutate(id = "String Search"), 
  run_keyword %>% 
    convert_points() %>% mutate(id = "Keyword Search"), 
  run_type %>% 
    convert_points() %>% mutate(id = "Type Search"),
  neu %>%
    mutate(id = "Northeastern University")) 

g1 <- ggplot() +
  geom_sf(data = points, color = "darkgrey", fill = "white",
          size = 31, shape = 21, alpha = 0.5) +
  geom_sf(data = fish, color = "black", fill = NA) +
  geom_sf(data = fish %>% filter(id == 124), fill = "darkgrey", color = "black") +
  geom_sf(data = dat, mapping = aes(color = id), size = 3, alpha = 0.5) +
  geom_sf(data = neu, size = 3, color = "black") +
  coord_sf(xlim = c(1904498, 1911007),
           ylim = c(523391, 531900)) +
  theme(panel.background = element_rect(fill = "tan"),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  labs(color = "Results by Type\nof Search",
       caption = "Note: Black point and grey cell represents 1 square kilometer range\naround Northeastern University, the center of each 770 meter radius search\nper square kilometer.")
# Save!
ggsave(g1, filename = "viz/figure_A1.png", dpi = 500)

# Clear data
remove(mykey, mypoints, neu, g1, run_type, run_keyword, run_string, convert_points)
```

## Figure A2: Compare grid cell sizes

```{r figurea2, eval = FALSE}
# Load common map data
source("helper/mapdata.r")

myquery <- read_rds("query/test/test_points.rds")  %>%
    # Convert to Equal Area projection for plotting
  st_transform(crs = meta$aea$proj) 

# number of points lost vs. dollars lost

# Import all different fishnet grids
myfish <- read_sf("shapes/fishnet.geojson") %>%
  # Convert to Equal Area projection for plotting
  st_transform(crs = meta$aea$proj) %>%
  # Recode variable names
  mutate(type = size %>% recode_factor(
    "1000" = "<b>1 km<sup>2</sup></b>
    <br> <br>
    Error:<br>
    <b>Cells</b><sup>1</sup>:   0 / 260<br>
    <b>Points</b><sup>2</sup>: 0 (0%)<br>
    <b>Net gain</b><sup>3</sup>: 54.1",
    "2000" = "<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3",
    "5000" = "<b>5 km<sup>2</sup></b><br><br>Error:<br>7 / 17<br>206 (48%)<br>19.6",
    "10000" = "<b>10 km<sup>2</sup></b><br><br>Error:<br>3 / 6<br>321 (76%)<br>-1222")) %>%
  # Join in places of worship
  st_join(myquery) %>%
  # Tally how many there are per grid cell
  group_by(cell_id, type) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  mutate(level = case_when(
    count == 0 ~ "0",
    count > 0 & count <= 5 ~ "0-5",
    count > 5 & count <= 10 ~ "6-10",
    count > 10 & count <= 15 ~ "11-15",
    count > 15 & count <= 20 ~ "16-20",
    count > 20 & count <= 30 ~ "21-30",
    count > 30 ~ "31+") %>%
      factor(levels = c("0", "0-5", "6-10","11-15","16-20","21-30", "31+"))) %>%
  mutate(indicator = if_else(count > 20, "Problematic", "Fine"))

# Create a box for annotations
mybox <- data.frame(
  xmin = 1900828.7,
  ymin = 514765.5,
  xmax = 1920828.7,
  ymax = 544765.5,
  label = "Most Efficient",
  type = factor("<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3"))

# Visualize it!
g1 <- ggplot() +
  geom_sf(data = myfish, mapping = aes(fill = level,
                                       color = indicator, size = indicator)) +
  geom_sf(data = myquery, color = "#648FFF", size = 1.5, alpha = 0.5) + 
  scale_color_manual(values = c("white", "black")) +
  scale_size_manual(values = c(0.75, 1.25)) +
  geom_sf(data = myfish %>%
            filter(count > 20), 
          mapping = aes(color = indicator, size = indicator),
          fill = NA) +
  geom_sf(data = county, color = "black",size = 0.75, fill = NA) +
  facet_wrap(~type, ncol = 4) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = ggtext::element_markdown(size = 14, hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        
        strip.text.x = ggtext::element_markdown(size = 12, hjust = 0.5),
        plot.margin = margin(0,0,0,0, "cm")) +
  viridis::scale_fill_viridis(discrete = TRUE, 
                              option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(
    fill = "# of Places\nof Worship\nper cell",
    subtitle = "<b>Places of Worship</b> (<span style='color:#648FFF'><b>blue</b></span>, n = 422) by Size of Grid Cell<br>across Populated Census Tracts in Suffolk County") +
    
    guides(fill = guide_legend(override.aes = list(
    color = c("white", "white", "white", "white", "black", "black"),
    size = c(0.75,0.75,0.75,0.75,1.5, 1.5))),
         color = "none", size = "none") +
 geom_rect(
    data = mybox,
    mapping = aes(xmin = xmin - 2000,
                  ymin = ymin,
                  xmax = xmax + 1000,
                  ymax = ymax),
    fill = NA, color = "black",linetype = "dashed", size = 1) +
   geom_text(
    data = mybox,
    mapping = aes(x = (xmin + xmax)/2,
                  y = ymin - 2000,
                  label = label),
    fill = NA, color = "black",linetype = "dashed", size = 5) 
  
# Save to file!
ggsave(g1, filename = "figures/figure_A2.png", dpi = 500, 
         width = 9.5, height = 4.5)

# Remove data
rm(list = ls())
```

The above visual uses data from the following tables...

```{r, eval = FALSE}
# Number of purple cells vs. total cells
myfish %>% 
  as.data.frame() %>%
  mutate(count = if_else(count > 20, count - 20, 0)) %>%
  group_by(type, indicator) %>%
  summarize(sum = sum(count))
# Points missed
```

```{r, eval = FALSE}
#Total points recorded / Total points missed
#Cost of points recorded / cost of points missed
#Payoff Ratio: 

data.frame(
  label = c("1 km", "2 km", "5 km", "10 km"),
  cells_gained = c(260, 77, 10, 3),
  cells_error = c(0,   4, 7, 3),
  price = 0.03,
  points_total = 422,
  points_gained = c(422, 413, 216, 101)) %>%
  mutate(cells_total = cells_gained + cells_error,
         points_error = points_total - points_gained) %>%
  # How much does it cost, on average, to record a point?
  # Cost of gain + cost of error
  
  # points gained per dollar spent
  # Gained this many points given this cost
  mutate(valued_added = points_gained / (cells_total*price),
         valued_lost = points_error / (cells_total*price),
         
         net_valued_added = valued_added - valued_lost)
```

<br>

## Figure A3: Map Rates by Grid

We're going to do a cool descriptive sensitivity test. First, we need to tally rates for the two separate grids. Does the size of collection grid used (1km vs 2km) affect the eventual rates, and if so, how much?

```{r figurea3_data, eval = FALSE}
# Clear data
rm(list = ls())
# Get projections
meta <- dget("meta.txt")

# Let's run a test using our Places of Worship query results,
# which we used as a test-run with 1km collection grid and an actual-run on the 2 km collection grid

# gathered from a 1km2 collection grid
query1 <- read_rds("query/test/test_points.rds") %>%
    st_transform(crs = meta$aea$proj) %>%
    select(-any_of("cell_id"))


# gathered from a 2km2 collection grid
query2 <- read_sf("query/sites_points.geojson") %>%
  filter(group == "Places of Worship") %>%
  st_transform(crs = meta$aea$proj) %>%
  select(-any_of("cell_id"))


# Get 1 km2 tallying grid
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  st_transform(crs = meta$aea$proj)

# Write a quick function to tally up a set of points-per-grid cell
tallyup = function(data){
  data %>%
    # Count up total points per cell
    group_by(cell_id) %>%
    summarize(
      sites = sum(!is.na(place_id)),
      # Use interpolated population density
      across(.cols = c(pop_density_int, geometry), .fns = ~unique(.x))) %>%
    ungroup() %>%
    # Calculate rate of sites per 1000 persons in the grid cell
    mutate(rate = sites / pop_density_int * 1000) %>%
    return()
}

# Tally up points gathered from the 1-km^2 collection grid
tally1 <- grid %>%
  st_join(query1) %>%
  tallyup()

# Tally up points gathered from the 2-km^2 collection grid
tally2 <- grid %>%
  st_join(query2) %>%
  tallyup()



# Bind them together
bind_rows(tally1, tally2, .id = "size") %>%
  group_by(size) %>%
    # Log, but deal with zeros
  mutate(y = case_when(
    # Replace zeros with half the value of the smallest non-zero value
    rate == 0 ~ rate %>% unique() %>% sort(FALSE) %>% .[2] / 2,
    # otherwise, keep the same
    TRUE ~ rate),
    # Log transform
    ylogged = log(y)) %>%
  ungroup() %>%
  # Join in covariates 
  left_join(by = "cell_id",
            y = read_sf("shapes/grid_covariates.geojson") %>%
              as_tibble() %>%
              select(cell_id, zone, 
                     pop_women:pop_age_65_plus,
                     social_capital:linking)) %>%
  st_write("shapes/rates_by_collection_grids.geojson", delete_dsn = TRUE)

rm(list = ls())
```

Now, make the corresponding visual for the grid.

```{r figurea3}
library(tidyverse)
library(sf)
library(ggtext)
# Get projections
meta <- dget("meta.txt")

# Get tallies!
tally <- read_sf("shapes/rates_by_collection_grids.geojson") %>%
  filter(!zone %in% c("excluded_validation", "excluded_boston", "outside_of_boston"))

# Caculate mean per grid size!
stats <- tally %>%
  as_tibble() %>%
  group_by(size) %>%
  summarize(mean = mean(rate, na.rm = TRUE) %>% round(2))

# Looks like there is a very minor, not especially significant difference between the two.
test <- tally %>%
  as_tibble() %>%
  with(t.test(ylogged ~ size)) %>%
  broom::tidy() %>% 
  mutate_at(vars(estimate1, estimate2), list(~exp(.)))


# Calculate their relative ranking
cor1 <- tally %>%
  as_tibble() %>%
  group_by(size) %>%
  arrange(rate) %>%
  mutate(rank = 1:n()) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(cell_id), names_from = size, values_from = rank) %>%
  summarize(cor = cor(`1`, `2`))

# Repeat for second grid
cor2 <- tally %>%
  as_tibble() %>%
  pivot_wider(id_cols = c(cell_id), names_from = size, values_from = rate) %>%
  summarize(cor = cor(`1`, `2`))

# Make some labels...
mylabels <- as_labeller(c(
  "1" = paste("<b>1 km<sup>2</sup> Collection Grid<br>1 km<sup>2</sup> Tally Grid</b><br>Mean Rate: ", stats$mean[1]),
  "2" = paste("<b>2 km<sup>2</sup> Collection Grid<br>1 km<sup>2</sup> Tally Grid</b><br>Mean Rate: ", stats$mean[2])))

# How much do results change when using a 1 km collection grid versus a 2 km tally grid?
g1 <- ggplot() +
   geom_sf(data = tally, mapping = aes(fill = rate, group = size),
          fill = NA, color = "#373737", size = 1) +
  geom_sf(data = tally, mapping = aes(fill = rate, group = size),
          color = "white") +
  facet_wrap(~size, labeller = mylabels) +
  theme_void(base_size = 14) +
  theme(
    plot.caption = element_markdown(face = "italic", size = 12, hjust = 0),
    strip.text.x = element_markdown(size = 12, hjust = 0.5, 
                                    colour = "#373737"),
    plot.margin = margin(0,0,0,0, "cm"),
legend.title = element_markdown(face = "italic", size = 13, hjust = 0)) +
  viridis::scale_fill_viridis(option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(fill = "<b>Rates of<br>Places<br>of Worship</b><br><br><i>per 1000<br>residents<br>per km<sup>2</sup></i>")


# Analyze the difference of means
ggsave(g1, filename = "figures/figure_A3.png", dpi = 300, width = 5, height = 3.5)

# Clear data
rm(list = ls())
```


## Figure A4: Correlations by Grid

Are there any major demographic differences? Do they correlate as expected with covariates?

```{r figurea4}
# Let's calculate some correlatiosns!
mycor <- read_sf("shapes/rates_by_collection_grids.geojson") %>%
  filter(zone %in% c("validation", "boston")) %>%
  as_tibble() %>%
  pivot_longer(cols = c(
    pop_density_int, pop_women, pop_age_65_plus, 
    pop_black, pop_white, pop_asian, pop_hisplat,
    pop_some_college, median_income, 
    pop_unemployed, median_monthly_housing_cost, 
    social_capital:linking), names_to = "covariate", values_to = "x") %>%
  group_by(size, covariate) %>%
  summarize(cor = cor(y, x, use = "pairwise.complete.obs")) %>%
  ungroup() %>%
  # Relabel terms
  mutate(covariate = covariate %>% recode_factor(
    "pop_density_int" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost",
    "linking" = "Linking\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "bonding" = "Bonding\nSocial Capital",
    "social_capital" = "Overall\nSocial Capital"),
    size = size %>% recode_factor(
      "1" = "1 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid",
      "2" = "2 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid"))

# Visualize it!
g1 <- mycor %>%
  ggplot(mapping = aes(x = size, y = covariate, fill = cor, label = round(cor, 2))) +
  geom_tile(color = "#373737", size = 0.1) +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", high = "#648FFF", mid = "white", 
                       midpoint = 0, limits = c(-1, 1),
                       breaks = c(-1, -0.75, -0.5, -0.25, 0, .25, .5, .75, 1))  +
  theme_classic() +
  theme(
    plot.title = element_markdown(hjust = 0.5, size = 14),
    plot.subtitle = element_markdown(hjust = 0.5, size = 12),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_text(size = 10),
        axis.text.x = ggtext::element_markdown(size = 10, hjust = 0.5),
    legend.title = ggtext::element_markdown(size = 10, hjust= 0),
    legend.text = ggtext::element_markdown(size = 12, hjust= 0),
        panel.border = element_blank(),
        legend.position = "right") +
  guides(fill = guide_colorsteps(barheight = 15, barwidth = 2,
                                 tick.colour = "#373737", frame.colour = "#373737", 
                                 show.limits = TRUE)) +
  labs(x = NULL, y = NULL, fill = "<b>Correlation</b><br>(<i>Pearson's R</i>)",
       title = "Demographic Correlations Unchanged<br>by Grid Collection Size",
       subtitle = "Correlations between Logged Rates<br>of Places of Worship vs. Demographics",
       caption = "Note: Demographics represent averages of tract level covariates.")

# save to file
ggsave(g1, filename = "figures/figure_A4.png", width = 6, height = 6.5, dpi = 300)

# Clear data
rm(list = ls())
```

<br>
<br>

## Figure A5: Map of Cell Zones 

Using a dataset that we make later, let's just grab the cell IDs of the cells we use in our study range, so we can highlight them in the visual below.

```{r figurea5}
# Clear data
rm(list = ls())

# Get basic map data
source("helper/mapdata.r")

# Write a quick function to always 
# get the correct number of cells in each zone
relabel = function(data){
  
  mylabels <- data %>%
    as_tibble() %>%
    group_by(
      zone,
      zone_name = zone %>% recode_factor(
        "validation" = "Validation Cells",
        "boston" = "Boston Cells",
        "excluded_validation" = "Boston Cells",
        "excluded_boston" = "Ineligible Cells",
        "outside_of_boston" = "Outside Boston")) %>%
    summarize(
      # Count 'em up!
      count = n()) %>%
    ungroup() %>%
    # Aggregate!
    group_by(zone_name) %>%
    mutate(count = sum(count)) %>%
    ungroup() %>%
    # make the label!
    mutate(
      # Get the order of nodes
      zone_order = as.numeric(zone_name),
      # Get lables
      zone_label = paste(
        "<b>", zone_name, "</b>", "<br>", 
        "(n = ", count, ")", "<br>", 
        if_else(zone_name == "Outside Boston", "(Excluded)", ""), sep = ""),
      # Order the labels
      zone_label = fct_reorder(.f = zone_label, .x = zone_order)) %>%
  select(zone, zone_label)
  
  data %>%
    left_join(by = "zone", y = mylabels) %>%
    return()
}

# Add variable to grid
grid <- grid %>%
  relabel()

library(ggtext)


g1 <- ggplot() +
  geom_sf(data = tracts, mapping = aes(size = "Tracts"), 
          color = "black",  fill = "grey") +
  geom_sf(data = grid, mapping = aes(fill = zone_label), 
          color = "white", size = 0.25, alpha = 0.25) +
  scale_color_manual(values = c("#FFB000", "#DC267F", "#785EF0", "#FE6100")) +
  scale_fill_manual(values = c("#FFB000", "#DC267F", "#785EF0", "#FE6100"))  +
  geom_sf(data = county, mapping = aes(size = "County"), 
          color = "black", fill = NA) +
  scale_size_manual(values = c(1, 0.1)) +
  labs(
    fill = paste("Cells (n = ", nrow(grid), ")<br> in Suffolk County", sep = ""),
    size = "Borders",
    title = expression(paste("1 ", km^2,  " grid cells for tallying sites")),
    subtitle = "<i>(n = 163 eligible cells in Boston)</i>") +
  guides(color = "none", size = guide_legend(override.aes = list(
    fill = c("white", "grey"))))  +
  ggspatial::annotation_north_arrow(height = unit(0.5, "cm"),
                                    width = unit(0.5, "cm")) +
  ggspatial::annotation_scale(pad_x = unit(3.5, "cm")) +
  theme_void(base_size = 14) +
  theme(plot.title = element_text(face = "bold", size = 15),
        plot.subtitle = element_markdown(face = "italic", size = 14, hjust = 0),
        legend.text = element_markdown(size = 12),
        legend.title = element_markdown(size = 12),
        legend.position = "right") 

# There are 261 cells of 1 square kilometer in Suffolk County
ggsave(g1, filename = "figures/figure_A5.png", dpi = 500, height = 4, width = 4.5)

# Clear it
rm(list = ls())
```

<br>
<br>

## Figure A6: Representativeness

We're going to select our study area for validation.

We collected a random stratified sample of 20 grid cells, and conducted ground truthing in each grid cell.

To do so, we split up the population of grid cells geographically by X neighborhoods, and then in terms of four additional key demographic traits, including population density, median income, the white/non-white population, and % residents with some or more college education. For each variable, we binned grid cells into 3 equally sized quantiles. Then, we calculated the proportion of grid cells which fit into each strata. (An example strata might be Jamaica Plains, upper 3rd share in terms of population density, lower 3rd share of median income, upper 3rd share of non-white residents, middle 3rd share of college educated residents.) Finally, we used these proportions as sampling probabilities, and took 1000 different samples of 20 grid cells using these probabilities, to ensure that in each of these 1000 different possible samples, our samples would reflect the frequency of these categories in the population at large.

The final sample of 20 grid cells was quite representative of the Boston area in terms of most key demographic traits. Finally, I randomly assigned 10 of these grid cells to masters students and 10 grid cells to undergraduates.


```{r figurea6}
# Clear data
rm(list = ls())
# Make a folder called sampling!
dir.create("data/sampling")

# Import our grid cells 
fish <- read_sf("shapes/grid_covariates.geojson") %>%
  # Filter to just the inner-neighborhoods of Boston, which are most comparable
  filter(zone %in% c("validation", "excluded_validation")) %>%
  # split up each variable into thirds
  mutate(pop_density_cat = ntile(pop_density, 3),
         median_income_cat = ntile(median_income, 3),
         pop_white_cat = ntile(pop_white, 3),
         pop_some_college_cat = ntile(pop_some_college, 3)) %>%
  # Grouping by neighborhood and each demographic category,
  # get the distribution of residents in the city
  group_by(neighborhood, pop_density_cat, 
           median_income_cat, pop_white_cat, 
           pop_some_college_cat) %>%
  mutate(count = n()) %>%
  ungroup() %>%
  mutate(prop = count / sum(count, na.rm = TRUE))

# Visualize these cells
#ggplot() +
#  geom_sf(data = fish, mapping = aes(fill = neighborhood))

# Next, we're going to randomly sample grid cells in blocks...
fish %>%
  as_tibble() %>%
  select(-geometry) %>%
  infer::rep_slice_sample(n = 20, reps = 4000, replace = FALSE, weight_by = .$prop) %>%
  # Our random sample must include Northeastern
  # So using the cells that remain,
  filter(replicate %in% replicate[cell_id == 124]) %>%
  # Eventually, we found the following grid cells,
  # which became our actual groundtruth sample;
  
  # Bind in the actual sampled grid cell
  bind_rows(
    fish %>%
      as_tibble() %>%
      filter(cell_id %in% read_sf("shapes/grid_sampled.geojson")$cell_id) %>%
      mutate(replicate = 0)) %>%
  saveRDS("data/sampling/grid_reps.rds")

# For demographic variables of interest...
myvars <- c("pop_density", "pop_women","pop_age_65_plus",
            "pop_black","pop_white", "pop_asian", "pop_hisplat",
            "pop_some_college", "median_income", "pop_unemployed",
            "median_monthly_housing_cost")

# Run t-tests against population
testit = function(baseline, sample){
  # Let's gather the following variables...

  baseline %>%
    as_tibble() %>%
    select(cell_id, any_of(myvars)) %>%
    # Get population average
    summarize(
      cell_id = 0,
      across(.cols = any_of(myvars), .fns = ~mean(.x, na.rm = TRUE))) %>%
    # Get individual values in sample
    bind_rows(
      read_sf("shapes/grid_covariates.geojson") %>%
        as_tibble() %>%
        select(cell_id, any_of(myvars)) %>%
        filter(cell_id %in% sample)) %>%
    # Run a t-test and return the p-value
    summarize(
      across(.cols = any_of(myvars),
             .fns = ~t.test(.x[-1], mu = .x[1]) %>% broom::tidy() %>% 
               mutate(pop = .x[1]) %>% nest())) %>%
    pivot_longer(cols = -c(), names_to = "variable", values_to = "value") %>%
    group_by(variable) %>%
    summarize(unnest(value)) %>%
    mutate_at(vars(estimate, statistic, p.value, conf.low, conf.high, pop),
              list(~round(.,4))) %>%
    mutate(stars = p.value %>% gtools::stars.pval()) %>%
    select(variable, estimate, statistic, p_value = p.value, stars, 
           lower_ci = conf.low, upper_ci = conf.high, pop) %>%
    return()
}

# Get a series of cells to compare against the population
v <- read_sf("shapes/grid_covariates.geojson") %>%
  filter(zone == "validation") %>% with(cell_id)
a <- read_sf("shapes/grid_sampled.geojson") %>%  with(cell_id)
m <- read_sf("shapes/grid_sampled.geojson") %>% filter(coders == "masters") %>% with(cell_id)


# Let's gather a series of results
bind_rows(
# First, let's compare the validation area against All of Boston
  read_sf("shapes/grid_covariates.geojson") %>%
    filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
    testit(sample = v) %>%
    mutate(type = "Validation Area"),
# then compare the sampled cells against all of boston  
  read_sf("shapes/grid_covariates.geojson") %>%
    filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
    testit(sample = a) %>%
    mutate(type = "20 Sampled Cells"),
  # then compare the masters cells against all of Boston
  read_sf("shapes/grid_covariates.geojson") %>%
    filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
    testit(sample = m) %>%
    mutate(type = "10 Ground-Truthed Cells")
) %>%
  mutate(group = paste(type, variable)) %>%
  write_csv("data/sampling/representativeness.csv")

# Write a short function to relable variables
relabel = function(data){
  data %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost")) %>%
  return()
}

# Now get population level data
mypop <- read_sf("shapes/grid_covariates.geojson") %>%
  filter(zone %in% c("validation", "excluded_validation", "boston")) %>%
  select(cell_id, any_of(myvars)) %>%
  pivot_longer(cols = c(myvars), names_to = "variable", values_to = "value") %>%
  relabel()

# And get sample difference of means stats
mysample <- read_csv("data/sampling/representativeness.csv") %>%
  relabel()

# And vizualize!
g1 <- mysample %>%
  ggplot(mapping = aes(x = variable, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci, 
                       color = type, group = group)) +
  geom_jitter(data = mypop, 
              mapping = aes(x = variable, y = value,
                            ymin = NA_real_, ymax = NA_real_,
                            color = NA, group = NA), 
              alpha = 0.2, color = "black") +
  geom_violin(data = mypop, 
              mapping = aes(x = variable, y = value,
                            ymin = NA_real_, ymax = NA_real_,
                            color = NA, group = NA), 
              alpha = 0.75, fill = "darkgrey", color = "black") +
  
  geom_point(position = position_dodge2(width = 1), alpha = 0.75, size = 5,
             shape = 22, fill = "black", 
             mapping = aes(x = variable, y = pop, 
                                       ymin = NA_real_, ymax = NA_real_,
                           color = "Boston (Population)")) +
  geom_linerange(position = position_dodge2(width = 1), alpha = 0.95, size = 2) +
  geom_point(position = position_dodge2(width = 1), alpha = 0.95, size = 5) +
  facet_wrap(~variable, scales = "free", ncol = 2) +
  theme_bw(base_size = 14) +
  theme(strip.text = element_blank(), 
        strip.background = element_blank(),
        legend.position = c(0.8, 0.03), 
        legend.box.margin = margin(0, 0, 0, 0, "cm")) +
  coord_flip() +
  scale_color_manual(values = c("#648FFF", "#785EF0","#DC267F", "black"),
                     breaks = c("10 Ground-Truthed Cells","20 Sampled Cells", 
                                "Validation Area","Boston (Population)")) +
  scale_linetype_manual(values = c("solid")) +
  guides(color = guide_legend(override.aes = list(
    
    shape = c(21, 21, 21, 22),
    fill = c("#648FFF", "#785EF0","#DC267F","black")))) +
  theme(plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        plot.subtitle = element_text(hjust = 0.5)) +
  labs(color = NULL, linetype = NULL,
       y = "Traits of Grid Cells,   \naveraged from Census Tracts", x = NULL,
       subtitle = "Representativeness in Validation Samples\nSample Mean vs. Population Mean (95% Sample Confidence Intervals)")

# Save!
ggsave(g1, filename = "figures/figure_A6.png",
       dpi = 500, width = 7.5, height = 7)
# Clear!
rm(list = ls())
```

## Figure A7: Map of Sampled Cells

```{r figurea7}
library(tidyverse)
library(sf)

# Get projeciton
meta <- dget("meta.txt")

grid <- read_sf("shapes/grid_covariates.geojson") %>%
  # Zoom into inner boston
  filter(zone %in% c("validation"))

# Import grid cells sampled for ground truthing
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  select(cell_id, coders, geometry) 

# Import original points
mygoogle <- read_sf("query/sites_validated.csv") %>%
  filter(!is.na(google_id)) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Import online-checked points
mychecked <- read_sf("query/sites_validated.csv") %>%
  filter(status %in% c("checked", "checked, visited",
                       "new", "new, visited")) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Import ground-truthed points
myground <- read_sf("query/sites_validated.csv") %>%
  filter(status %in% c("checked, visited", "new, visited")) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Import neighborhood boundaries within study area
shapes <- read_sf("shapes/neighborhoods.geojson")

mycolors <- viridis::plasma(n = 3, begin = 0.2, end = 0.8)

library(ggtext)

g1 <- ggplot() +
  # Add base census tracts layer
  geom_sf(data = shapes, fill = "black", 
          color = "darkgrey", size = 0.2) +
  # Add points over that
  geom_sf(data = mygoogle, mapping = aes(fill = "Google Sites <sup>1</sup>"), 
          size = 1.5, color = "black", shape = 21) +
  geom_sf(data = mychecked, mapping = aes(fill = "Checked Online <sup>2</sup>"), 
          size = 1.5, color = "black", shape = 21) +
  geom_sf(data = myground, mapping = aes(fill = "Ground Truthed <sup>3</sup>"), 
          size = 1.5, color = "white", shape = 21) +
  # Add grid overtop
  geom_sf(data = grid, mapping = aes(color = "Study Area <sup>4</sup>"),
          size = 0.5, alpha = 0.5, fill = "white") +
  # Add masters blocks sampled overtop
  geom_sf(data = blocks, size = 0.5, alpha = 0.5, fill = "#6A00A8",
         mapping = aes(color = "Sampled Cells <sup>5</sup>")) +
  theme_bw(base_size = 14) +
  scale_fill_manual(values = c("#FCA636", "#CC4678","#6A00A8"),
                    breaks = c("Google Sites <sup>1</sup>",
                               "Checked Online <sup>2</sup>",
                               "Ground Truthed <sup>3</sup>")) +
  scale_color_manual(
    values = c("black","#6A00A8"),
    breaks = c("Study Area <sup>4</sup>",
               "Sampled Cells <sup>5</sup>")) +
  coord_sf(crs = meta$aea$proj, 
           xlim = c(1905020, 1914783),
           ylim = c(521064.4, 532540.7)) +
  theme(
    plot.caption = element_markdown(hjust = 0, size = 12),
    legend.text = element_markdown(size = 14),
    legend.key = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = "white"),
    panel.grid = element_blank(), 
    axis.text = element_blank(),
    axis.ticks = element_blank()) +
  guides(
    order = 2,
    color = guide_legend(
      override.aes = list(
        size = 2,
        shapes = 22,
        stroke = 0,
        alpha = 0.1,
        fill = c("white", "#6A00A8"),
        color = c("black", "#6A00A8"),
        linetype = "solid")),
    fill = guide_legend(
      order = 1,
      override.aes = list(
      size = c(4, 4, 4),
      shapes = c(1,1,1),
      color = c("black", "black", "white"),
      stroke = c(0.5, 0.5, 0.5),
      alpha = c(1, 1, 1),
      fill = c("#FCA636", "#CC4678","#6A00A8"),
      linetype = c("blank", "blank", "blank")))) +
  theme(plot.subtitle = element_text(hjust = 0.5), 
        plot.title.position = "plot") +
  labs(
    subtitle = "Mapping Social Infrastructure in Boston",
    fill = "Social Infrastructure",
    color = "Grid Cells")

# save it!
ggsave(g1, filename = "figures/figure_A7.png", dpi = 500, width = 5.5, height = 4)

# Clear it!
rm(list = ls())
```

<br>
<br>

## Data Prep for Figure A8

```{r figurea8_data, eval = FALSE}
library(tidyverse)
library(sf)
library(ggspatial)

# Get projections
meta <- dget("meta.txt")

# Get names of our 10 grid cells 
sample <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  as_tibble() %>%
  select(cell_id)

read_csv("data/figure_rates.csv") %>%
  filter(figure == 3) %>%
  filter(panel == "Ground Truthed") %>%
  select(cell_id, type, rate, count) %>%
  # Join in grid data
  right_join(by = "cell_id", 
             y = read_sf("shapes/grid_covariates.geojson") %>%
  filter(cell_id %in% sample$cell_id)) %>%
  st_as_sf(crs = 4326) %>%
  mutate(neighborhood = case_when(
    cell_id == 183 ~ "North End",
    cell_id == 124 ~ "Fenway (Northeastern)",
    cell_id == 142 ~ "Fenway (Boston U.)",
    cell_id == 82 ~ "Jamaica Plains (Pondside)",
    TRUE ~ neighborhood)) %>%
  st_write("shapes/grid_sampled_viz.geojson", delete_dsn = TRUE)

read_csv("query/sites_validated.csv") %>%
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  st_transform(crs = meta$aea$proj) %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])  %>%
  filter(status != "duplicated") %>%
  # Adjust the type variable to include more
  mutate(type = case_when(
    status == "not social infrastructure" ~ "Not social infrastructure",
    status == "not found" ~ "API Sites Not Found",
    TRUE ~ type)) %>%
  mutate(type = factor(type, levels = c("Total", "Community Spaces", "Places of Worship", 
                  "Social Businesses", "Parks", "Other",
                  "Not social infrastructure", "API Sites Not Found"))) %>%
  mutate(icon = type %>% recode_factor(
    "Total" = "icons/hands-helping-grey-circle.png",
    "Community Spaces" = "icons/store-alt-blue-circle.png",
    "Places of Worship" = "icons/place-of-worship-purple-circle.png",
    "Social Businesses" = "icons/coffee-red-circle.png",
    "Parks" =  "icons/tree-yellow-circle.png",
    "Other" = "icons/dot-circle.png",
    "Not social infrastructure" = "icons/times-circle.png",
    "API Sites Not Found" = "icons/question-circle.png"))  %>%
  as_tibble() %>%
  select(-geometry) %>%
  st_write("query/sites_ground_truthing.geojson", delete_dsn = TRUE)
```

## Figure A8: Ground-Truthing Map

```{r figurea8}
library(tidyverse)
library(sf)
library(ggspatial)
library(ggtext)
library(magick)
library(ggimage)

# Get projections
meta <- dget("meta.txt")

# Load in the tallied rates
mytally <- read_sf("shapes/grid_sampled_viz.geojson") %>%
  filter(type == "Total")

# Load in the sampled grid cells!
myblocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") 

# Get streets per cell
mystreets <- read_sf("shapes/streets.geojson") 

# Get buildings per cell
mybuildings <- read_sf("shapes/buildings.geojson") 

# Get hydrological data
mywater <- read_sf("shapes/water.geojson")
myriver <- read_sf("shapes/rivers.geojson")

# Get elevation
myelevation <- read_sf("shapes/elevation.geojson")

# Get trainlines per cell
mylines <- read_sf("shapes/train_lines.geojson") %>%
  # Just train lines overlapping blocks
  st_join(myblocks %>% select(geometry), left = FALSE) %>%
  # Now crop them to box shape
  st_intersection(myblocks %>% select(cell_id, geometry))

# Get train stops per cell
mystops <- read_sf("shapes/train_stops.geojson") %>%
  # Filter to just those within the cell
  st_join(myblocks %>% select(cell_id), left=  FALSE) 

# Get universities per cell
myuni <- read_sf("shapes/universities.geojson") %>%
  st_join(myblocks %>% select(cell_id), left = FALSE)

# Get 100-m-interpolated estimates of racial demographics
tinyblocks <- read_sf("shapes/race_interpolated.geojson") 

# Get groundtruthed points
mypoints <- read_sf("query/sites_ground_truthing.geojson") 

# Let's write one more visualization function, to produce multiple visuals
visualize = function(cell, base_size){
  
  thetally <- mytally %>%
    filter(cell_id == cell)
  
  theblock <- myblocks %>%
    filter(cell_id == cell)
  
  mybox <- theblock %>%
    st_transform(crs = meta$aea$proj) %>% st_bbox()
  
  g1 <- ggplot() +
    geom_sf(data = theblock, 
            fill = "black", color = "black", size = 5) +
    geom_sf(data = tinyblocks %>% filter(cell_id == cell), 
            mapping = aes(fill = pop_nonwhite_int*100, group = id), color = NA) +
    scale_fill_gradient(low = "white", high = "darkorchid",
                        limits = c(0, 100)) +
    geom_sf(data = myriver %>% filter(cell_id == cell), color = "steelblue") +
    geom_sf(data = mywater %>% filter(cell_id == cell), fill = "lightblue", color = "steelblue") +
    geom_sf(data = myelevation %>% filter(cell_id == cell), fill = NA, color = "white", alpha = 0.75) +
    geom_sf(data = mybuildings %>% filter(cell_id == cell), fill = "#3e3d53", color = "black") +
    geom_sf(data = mystreets %>% filter(cell_id == cell), fill = NA, color = "#7f7d9c", size = 1.25) +
    # Add T-lines
    geom_sf(data = mylines %>% filter(cell_id == cell), color = "white", size = 3) +
    geom_sf(data = mylines %>% filter(cell_id == cell), mapping = aes(color = line), size = 2) +
    # Add T-stops
    geom_sf(data = mystops %>% filter(cell_id == cell), color = "white", size = 4) +
    geom_sf(data = mystops %>% filter(cell_id == cell), mapping = aes(color = line), size = 3) +
    # Give our T-lines appropriate colors
    scale_color_manual(
      # Sort the legend in this order
      breaks = c("ORANGE", "RED", "GREEN", "BLUE", "SILVER", "MULTIPLE"),
      # Give them the following colors
      values = c("darkorange", "firebrick", "seagreen", "deepskyblue", "grey", "black")) +
    # Layer images over
    geom_image(data = mypoints %>% filter(cell_id == cell) %>% as_tibble(), 
               mapping = aes(x = x, y = y, image = icon), size = 0.075) +
    coord_sf(xlim = c(mybox["xmin"] + 45, mybox["xmax"] - 45),
             ylim = c(mybox["ymin"] + 45, mybox["ymax"] - 45), crs = meta$aea$proj) +
    labs(fill = "% Non-White Residents",
         subtitle = paste("Cell ", theblock$cell_id, "\nin ", theblock$neighborhood, sep = ""),
         caption = paste("Rate: ", round(thetally$rate, 2), sep = "")) +
    theme_void(base_size = base_size) + 
    theme(plot.margin = margin(0,0,0,0, "cm"),
          plot.subtitle = element_text(
            size = base_size, hjust = 0.5
            # lineheight = 0.2,
            # vjust = 0.1
            # margin = margin(0,0,0,0,"cm")
            ),
          plot.caption = element_text(
            size = base_size, hjust = 0.5, lineheight = 0.2, vjust = 0.9,
            margin = margin(0,0,0,0,"cm")),
          panel.border = element_rect(fill = NA, color = "black", size = 2),
          legend.margin = margin(0.1,0.1,0.1,0.1,"cm"),
          legend.text = element_text(size = base_size*0.8, hjust = 0,vjust = 0, lineheight = 0),
          legend.title = element_text(size = base_size, hjust = 0, lineheight = 0)) +
    guides(color = "none",
           fill = "none")
  
  return(g1)
}

# Run that function for each sampled grid cell
allviz <- data.frame(cell_id = myblocks$cell_id) %>%
  split(.$cell_id) %>%
  map(~visualize(.$cell_id, base_size = 12))

# Combine them into one visual
combo <- ggpubr::ggarrange(
  plotlist = allviz,
  ncol = 5, nrow = 2,
  legend = "none")

# Sand save it to file!
ggsave(combo, filename = "figures/figure_A8.png", 
       dpi = 500, width = 10, height = 5.5)

# Clear data
rm(list = ls())
```


## Figure B1: Correlations

```{r figureb1}
library(tidyverse)
library(corrr)
# Get variables of interest
myvars <- c("Total Sites", "Community Spaces", 
            "Places of Worship", "Social Businesses", "Parks")
# Generate rates of social infrastructure per cell,
# and evaluate correlatiosn between different ypes
grid <- read_csv("data/figure_rates.csv") %>%
  filter(figure == 4) %>%
  select(cell_id, rate, type) %>%
  pivot_wider(id_cols = c(cell_id), names_from = type, values_from = rate) %>%
  rename(`Total Sites` = Total) %>%
  select(-cell_id) %>% 
  # Get pearson's r
  correlate(diagonal = 1) %>%
  # Stretch into tidy format
  stretch() %>%
  # Classify by strength of correlation
  mutate(strong = if_else(r > 0.5, "Strong", "Weak")) %>%
  mutate_at(vars(x,y),
            list(~factor(., levels = myvars %>% rev())))
# Visualize it!
g1 <- grid %>%
  ggplot(mapping = aes(x = x, y = y, fill = r, label = round(r, 2))) +
  geom_tile(color = "black", size = 0.1) +
  geom_tile(data = . %>% filter(strong == "Strong"),
            color = "black", size = 1) +
  geom_text() +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = ggtext::element_markdown(
          size = 11, hjust = 0),plot.caption.position = "plot",
        legend.title = ggtext::element_markdown(size = 14),
        plot.subtitle = element_text(hjust = 0.5),
        axis.text.x = element_text(hjust = 1, angle = 20)) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    breaks = c(-1, -.75, -.5, -.25, 0, .25, .5, .75, 1),
    low = "#DC267F", high = "#648FFF", mid = "white", midpoint = 0,
    guide = guide_colorbar(
    barheight = 15, frame.colour = "black", ticks = TRUE, ticks.linewidth = 2,
    ticks.colour = c(rep("white", 4), rep("black", 5), rep("white", 4), rep("black", 5)))) +
  labs(fill = "Correlation<sup>1</sup>", x = NULL, y = NULL, 
  subtitle = "Correlations of Social Infrastructure Rates among Boston City Blocks")

# SAve!
ggsave(g1, filename = "figures/figure_B1.png", dpi = 500, width = 7, height = 5)
# Clear data!
rm(list = ls())
```
<br>
<br>

## Statistical Analysis

```{r, eval = FALSE}
# Write modeling funciton
modelme = function(data){
  # If there's only 2020 in the dataset, it's a normal model
  if(data$year %>% unique() %>% length() == 1){
  data %>%
     lm(formula = sc ~ rate + 
          income_inequality + median_income + pop_nonwhite + 
          pop_some_college + pop_density, data = .) %>%
      return()
    # If there are multiple years, then we need fixed effects
  }else{
    data %>%
      lm(formula = sc ~ rate + 
          income_inequality + median_income + pop_nonwhite + 
          pop_some_college + pop_density + factor(year), data = .) %>%
      return()
  }
}
# Write a VIF funciton, 
# with appropriate squaring adjustment if model has categorical variables
get_vif = function(model){
  myvif <- car::vif(model)
 if(is.matrix(myvif)){
   myvif[,3]^2 %>% return()
 }else{
   myvif %>% return()
 }
}

# Load and analyze data!
read_csv("data/figure_rates.csv") %>%
  filter(figure == 4) %>%
  select(cell_id, type, rate) %>%
  # Join these rates into the year-by-year grid of covariates
  right_join(by = "cell_id", y = read_sf("shapes/grid_covariates_annual.geojson")) %>%
  # Only need these zones
  filter(zone %in% c("boston", "validation", "excluded_validation")) %>%
  mutate(type = type %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.))) %>%
  # drop geograph
  as_tibble() %>%
  select(-geometry, -republican_percent) %>% 
  # pivot to get social capital variables all in one row
  pivot_longer(cols = c(social_capital:turnout_percent), names_to = "sc_type", values_to = "sc") %>%
  # Now keep just observations from 2020. 
  # OR all observations containing a social capital index (which stretch over time; no other indicators do) 
  filter(year == 2020 | (sc_type %in% c("social_capital", "bonding", "bridging", "linking"))) %>%
  # Add covariate
  mutate(pop_nonwhite = 1 - pop_white) %>%
  # For each grouping of test data, standardize variables to get standardized effects
  group_by(sc_type) %>%
  mutate_at(vars(sc, rate, income_inequality,
                 median_income, pop_nonwhite, pop_some_college,
                 pop_density), funs(scale(.) %>% as.numeric())) %>%
     # Relabel the 'ingredients'!
  mutate(overall_type = sc_type %>% recode_factor(
    "social_capital" = "Social Capital",
    "bonding" = "Social Capital",
    "bridging" = "Social Capital",
    "linking" = "Social Capital",
    "turnout_percent" = "Voting",
    "democrat_percent" = "Voting",
    "country_clubs" = "(Quasi)\nBridging\nGroups",
    "veterans" = "(Quasi)\nBridging\nGroups",
    "fraternal" = "(Quasi)\nBridging\nGroups",
    
    "religious" = "Bridging\nGroups",
    "business" = "Bridging\nGroups",
    "charitable" = "Bridging\nGroups",
    "civic" = "Bridging\nGroups",
    "collectors" = "Bridging\nGroups",
    "union" = "Bridging\nGroups",
    
    "church_board" = "Linking\nGroups",
    "local_board" = "Linking\nGroups",
    "school_board" = "Linking\nGroups"),
    sc_type = sc_type %>% dplyr::recode_factor(
    "social_capital" = "Social\nCapital",
    "bonding" = "Bonding\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "linking" = "Linking\nSocial Capital",
    
    "turnout_percent" = "(%) Voter Turnout",
    "democrat_percent" = "(%) Voted Democrat",
    
    "country_clubs" = "Country Clubs",
    "veterans" = "Veterans Associations",
    "fraternal" = "Fraternal Orders",
    "religious" = "Religious Orgs",
    
    "business" = "Business Clubs",
    "charitable" = "Charitable Orgs",
    "civic" = "Civic Orgs",
    "collectors" = "Collectors Clubs",
    "union" = "Unions",
    
    "church_board" = "Church Board",
    "school_board" = "School Board",
    "local_board" = "Local Govt Body"),    

    type = factor(type, levels = c(
      "Total", "Community\nSpaces", "Places of\nWorship",
      "Social\nBusinesses", "Parks") %>% rev()),
  # Last, make a group variable that will tell us what figure each is for  
  figure = case_when(
    str_detect(overall_type, "Capital") ~ "Social Capital",
    str_detect(overall_type, "Voting") ~ "Voting",
    str_detect(overall_type, "Group") ~ "Group Membership")) %>%
  # nest!
  group_by(figure, overall_type, type, sc_type) %>%
  nest() %>%
  # Get model
  mutate(model = map(data, ~modelme(.)),
         vif = map(model, ~get_vif(.) %>% max()) %>% unlist(),
         gof = map_dfr(model, ~broom::glance(.) %>% select(-statistic, -p.value)),
         stats = map_dfr(model, ~moderndive::get_regression_table(.) %>% 
                        # Zoom into term for SI
                       filter(term == "rate") %>%   
                       mutate(label = paste(
                         round(estimate, 4) %>% format(scientific = FALSE),
                         gtools::stars.pval(p_value), sep = ""))))  %>%
  # Now narrow into these columns
  select(figure, overall_type, sc_type, stats, vif, gof) %>%
  # unnest the results, so that each of our 95 rows/models gets 1 set of results
  unnest(c(stats, gof))  %>%

  # save results!
  write_csv("data/models.csv")

# Let's write a visualization function
visualize = function(data){
  data %>%
    ggplot(mapping = aes(x = type, y = estimate, 
                         ymin = lower_ci, ymax = upper_ci,
                         fill = if_else(estimate > 0, "Positive", "Negative"),
                         label = label)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
    geom_col(width = 0.5) +
    geom_linerange() +
    geom_point() +
    geom_text(nudge_x = 0.40) +
    scale_fill_manual(values = c("#DC267F", "#648FFF"),
                      breaks = c("Negative", "Positive")) +
    theme_bw(base_size = 14) +
    coord_flip() +
    theme(
      panel.grid = element_blank(),
      panel.border = element_rect(color = "grey", fill = NA),
      panel.spacing.x = unit(0.5, "cm"),
      axis.title.x = element_text(hjust = 0.5),
      axis.title.y = element_text(hjust = 0.5),
      axis.text.x = element_text(size = 10, hjust = 0.5),
      strip.text.x = element_text(color = "white"),
      strip.text.y = element_text(size = 12, hjust = 0, angle = 0),
      strip.background.y = element_blank(),
      plot.subtitle = element_text(size = 15, hjust = 0.5),
      plot.caption = element_text(size = 10, hjust = 0),
      legend.position = "bottom") +
    labs(subtitle = NULL, x = NULL, y = NULL, fill = NULL) %>%
    return()
}

save(visualize, file = "helper/visualize.rdata")

# clear data
rm(list = ls())
```

## Figure C1

```{r figurec1}
# Now, one by one, let's produce these plots!
load("helper/visualize.rdata")

dat <- read_csv("data/models.csv")  %>%
  filter(figure == "Social Capital") %>%
  mutate(sc_type = factor(sc_type, levels = c("Social\nCapital", "Bonding\nSocial Capital",
                                              "Bridging\nSocial Capital", "Linking\nSocial Capital"))) %>%
  mutate(type = factor(type, levels = c("Total", "Community\nSpaces", "Places of\nWorship",
                                        "Social\nBusinesses", "Parks")%>% rev()))
g1 <- dat %>% visualize() +
    ggh4x::facet_grid2(
      #rows = vars(overall_type),
      cols = vars(sc_type),
      strip = strip_themed(
        background_x = list(
           element_rect(fill = "#0D0887", color = "#373737", size = 0.5),
           element_rect(fill = "#8405A7", color = "#373737", size = 0.5),
           element_rect(fill = "#D35171", color = "#373737", size = 0.5),
           element_rect(fill = "#d98d2b", color = "#373737", size = 0.5)))) +
  scale_y_continuous(expand = expansion(c(0,0)), n.breaks = 4, minor_breaks = NULL) +
  labs(
    subtitle = "Association between Social Infrastructure and Social Capital",
    y = "Standardized Effect of Site Rates (Z-score) on Social Capital (Z-score) \n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Direction of Effect (Beta)",
    x = "Type of Social Infrastructure Measure")

ggsave(g1, filename = "figures/figure_C1.png", dpi = 500, width = 12, height = 5.5)

rm(list = ls())
```

## Figure C2

```{r figurec2}
# Now, one by one, let's produce these plots!
load("helper/visualize.rdata")

dat <- read_csv("data/models.csv")  %>%
  filter(figure == "Voting") %>%
  mutate(sc_type = factor(sc_type, levels = c("(%) Voter Turnout", "(%) Voted Democrat"))) %>%
  mutate(type = factor(type, levels = c("Total", "Community\nSpaces", "Places of\nWorship",
                                        "Social\nBusinesses", "Parks")%>% rev()))
g1 <- dat %>% visualize() +
    ggh4x::facet_grid2(
      #rows = vars(overall_type),
      cols = vars(sc_type),
      strip = strip_themed(
        background_x = list(
           element_rect(fill = "#0D0887", color = "#373737", size = 0.5),
           element_rect(fill = "#8405A7", color = "#373737", size = 0.5),
           element_rect(fill = "#D35171", color = "#373737", size = 0.5),
           element_rect(fill = "#d98d2b", color = "#373737", size = 0.5)))) +
  scale_y_continuous(expand = expansion(c(0,0)), n.breaks = 4, minor_breaks = NULL) +
  labs(
    subtitle = "Association between Social Infrastructure and Voting",
    y = "Standardized Effect of Site Rates (Z-score) on Voting Outcomes (Z-score)\n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Direction of Effect (Beta)",
    x = "Type of Social Infrastructure")

ggsave(g1, filename = "figures/figure_C2.png", dpi = 500, width = 12, height = 5.5)

rm(list = ls())
```

## Figure C3

```{r figurec3}
# Clear data
rm(list = ls())

# Now, one by one, let's produce these plots!
load("helper/visualize.rdata")

dat <- read_csv("data/models.csv")  %>%
  filter(figure == "Group Membership") %>%
  mutate(type = factor(type, levels = c("Total", "Community\nSpaces", "Places of\nWorship",
                                        "Social\nBusinesses", "Parks") %>% rev())) %>%
  mutate(sc_type = factor(sc_type, levels = c("Country Clubs", "Veterans Associations", "Fraternal Orders",
                                              "Religious Orgs", "Business Clubs", "Charitable Orgs",
                                              "Civic Orgs", "Collectors Clubs", "Unions",
                                              "Church Board", "School Board", "Local Govt Body")))

design <- list(
  `row1` = c(1, 1, 2, 2, 3, 3),
  `row2` = c(4, 5, 6, 7, 8, 9),
  `row3` = c(10, 10, 11, 11, 12, 12)
) %>%
  as_tibble() %>%
  t() %>%
  as.matrix()

g1 <- dat %>% visualize() +
  ggh4x::facet_manual(facets = overall_type ~ sc_type, design = design, respect = TRUE,heights = c(1.2),
                      strip = strip_split(
                        position = c("right", "top"), 
                        background_x = list(
                          element_rect(fill = "#0D0887", color = NA), 
                          element_rect(fill = "#0D0887", color = NA),
                          element_rect(fill = "#0D0887", color = NA),
                          
                          element_rect(fill = "#8405A7", color = NA), element_rect(fill = "#8405A7", color = NA),
                          element_rect(fill = "#8405A7", color = NA), element_rect(fill = "#8405A7", color = NA),
                          element_rect(fill = "#8405A7", color = NA), element_rect(fill = "#8405A7", color = NA),
                          
                          element_rect(fill = "#D35171", color = NA), 
                          element_rect(fill = "#D35171", color = NA),
                          element_rect(fill = "#D35171", color = NA)),
                        text_x = list(
                          element_text(color = "white"),
                          element_text(color = "white"),
                          element_text(color = "white"),
                          
                          element_text(color = "white", hjust = 0.1), element_text(color = "white", hjust = 0.5),
                          element_text(color = "white", hjust = 0.1), element_text(color = "white", hjust = 0.5),
                          element_text(color = "white", hjust = 0.1), element_text(color = "white", hjust = 0.5),

                          element_text(color = "white"),
                          element_text(color = "white"),
                          element_text(color = "white")))) +
  scale_y_continuous(expand = expansion(c(0,0)), n.breaks = 4, minor_breaks = NULL) +
  scale_x_discrete(labels = function(x) paste(str_replace(x, "\n", "<br>"), "<br>", sep = "")) +
  labs(
    subtitle = "Association between Social Infrastructure and Membership Rates in Community Groups", x = " ",
    fill = "Direction of Effect (Beta)",
    y = "Standardized Effect of Site Rates (Z-score) on Membership Rates (Z-score) \n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    x = "Social Infrastructure Measure") +
  theme(legend.margin = margin(0,0,0,0,"cm"),
        legend.box.margin = margin(0,0,0,0,"cm"),
        axis.text.y = element_markdown(margin = margin(0.15,0.15,0,0, "cm")))  +
  guides(fill = "none")

ggsave(g1, filename = "figures/figure_C3.png", width = 12, height = 8)

# Clear data
rm(list = ls())
```



# Z. Supplementary Figures

## Figure Z1: Boston Tracts

Next, let's visualize where the tracts in Boston compare to all of Suffolk County.

```{r}
# Load in necessary data
source("helper/mapdata.r")

viz <- ggplot() +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "darkgrey", size = 5, fill = "white") +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "black", fill = "white") +
  geom_sf(data = tracts %>% filter(geoid %in% meta$boston_tracts),
          mapping = aes(size = "Tracts"),
          fill = "tan", color = "black") +
  scale_size_manual(values = c(1.15, 0.1)) +
  geom_sf(data = county, color = "black", size = 1.5, fill = NA) +
  theme_void(base_size = 14) +
  labs(size = "Borders",
       subtitle = paste("Boston Census Tracts", 
                        " (n = ", nrow(tracts), ")", sep = ""))

# Print to file!
ggsave(viz, filename = "figures/other/figure_Z1.png", 
       width= 3, height = 3.25)

rm(list = ls())
```


```{r, echo = FALSE, out.width="100%", fig.cap = "Boston Tracts"}
knitr::include_graphics("figures/other/figure_Z1.png", dpi = 100)
```

<br>
<br>


## Figure Z2: Test Query Points Mapped

```{r}
# Get common mapping data
source("helper/mapdata.r")

# Process the set of queries
myquery <- read_rds("query/test/test_points.rds")

# So, this approach gathered 3 times as many results as there actually were - a really hefty amount of duplication, due to the small grid cell size

# Create buffers
points <- grid %>%
  mutate(geometry = st_buffer(st_centroid(geometry), dist = 1410/2)) %>% 
  st_as_sf() 

viz <- ggplot() +
  geom_sf(data = points %>% 
            st_buffer(dist = 770) %>% 
            st_as_sf(), color = "black", alpha = 0.1, size = 0.05) +
  geom_sf(data = grid, fill = "firebrick", color = "white", size = 0.1, alpha = 0.1) +
  geom_sf(data = myquery, color = "firebrick", size = 1.5, alpha = 0.5) + 
  geom_sf(data = county, color = "black",size = 1, fill = NA) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(subtitle = "Places of Worship (red, n = 422)\n per 1 kilometer grid cells (n = 260)\nacross Populated Census Tracts\nin Suffolk County, Massachusetts")

# Save to file
ggsave(viz, filename = "figures/other/figure_Z2.png", dpi = 300, width = 4, height = 6)

# Clear data
rm(list = ls())
```

<br>
<br>

## Figure Z3: Duplicates Barchart

```{r}
# Format
meta <- dget("meta.txt")
mysites <- read_rds("query/sites.rds") %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
    st_transform(crs = meta$aea$proj)
  
# Count and visualize how often duplicates or triplicates, etc. occur
viz <- mysites %>%
  as_tibble() %>%
  # Per place,
  # How many rows are there?
  group_by(place_id) %>%
  count() %>%
  ungroup() %>%
  # Arrange from highest to lowest 
  arrange(desc(n)) %>%
  # Per level (duplicates, triplicates, etc.)
  # How many times does this occur?
  group_by(n) %>%
  count() %>%
  # Plot it!
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site\n(1 = One time, 2 = Duplicate, 3 = Triplicate, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5))

ggsave(viz, filename = "figures/other/figure_Z3_duplicates_barchart.png", dpi = 500, width = 6, height = 3)

rm(list = ls())
```

<br>
<br>

## Figure Z4: Duplicates by Search

```{r}
# Format
meta <- dget("meta.txt")
mysites <- read_rds("query/sites.rds") %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
    st_transform(crs = meta$aea$proj)

# How often did different searches record the same places
viz <- mysites %>%
  tibble() %>%
  # Per file,
  # How many distinct places are the?
  group_by(file) %>%
  select(place_id) %>%
  distinct() %>%
  ungroup() %>%
  # Per place,
  # How many DISTINCT rows are there?
  group_by(place_id) %>%
  count() %>%
  # Sort
  arrange(desc(n)) %>%
  # Per level (duplicate/triplicate/etc.)
  # How often do they occur?
  group_by(n) %>%
  count() %>%
  # Visualize!
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site in Multiple Searches\n(1 = 1 Search, 2 = 2 Searches, 3 = 3 Searches, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5))

ggsave(viz, filename = "figures/other/figure_Z4_duplicates_by_search_barchart.png", dpi = 500, width = 6, height = 4)

rm(list = ls())
```

That's surprisingly good. Fewer duplicates than before, it seems.
This means that just 42 sites appeared across multiple categories.

<br>
<br>


## Figure Z5: Cell IDs Mapped

```{r}
# Load common map data
source("helper/mapdata.r")

g1 <- ggplot() +
  # Nice grey suffolk county tract layer
  geom_sf(data = tracts, fill = NA, color = "lightgrey") +
  # Overlay boston outline
  geom_sf(data = boston, fill = NA, color = "black") +
  # Get grid
  geom_sf(data = grid, 
          mapping = aes(fill = zone), alpha = 0.2, color = "white") +
  # Add labels
  geom_sf_text(data = grid,
               mapping = aes(label = cell_id), size = 2)  +
  theme_void() +
  theme(legend.position = c(0.85, 0.20)) 

ggsave(g1, filename = "figures/other/figure_Z5.png", 
       dpi = 300, width = 6, height = 6.5)

rm(list = ls())
```

<br>
<br>


## Figures Z6-Z7: Voting Maps

```{r}
library(tidyverse)
library(viridis)

# Load in geographic data
precincts <- read_sf("shapes/precincts.geojson")
wards <- read_sf("shapes/wards.geojson")

# Map Votes for Democrats
g1 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = precincts,
          size = 0.2, color = "white",
          mapping = aes(fill = democrat_percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voted\nDemocrat",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g1, filename = "figures/other/figure_Z6.png", dpi = 500, height = 6, width = 9)

# Map Votes for Republicans
g2 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = precincts,
          size = 0.2, color = "white",
          mapping = aes(fill = turnout_percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voter Turnout",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g2, filename = "figures/other/figure_Z7.png", dpi = 500, height = 6, width = 9)

# Clear Data
rm(list = ls())
```


<br>
<br>


## Size of Neighborhood

Recently, several communities have tried to estimate the average size of a community in the US, based on resident perceptions. The results have been varied. [Kwame	Donaldson at the US Census Bureau](https://www.census.gov/content/dam/Census/programs-surveys/ahs/working-papers/how_big_is_your_neighborhood.pdf
) relied on census questions, such as those which ask residents whether there is a beach in their community, and then estimated the distance from the resident's home to the nearest beach. His Census Bureau working paper estimated the average size of a community, as perceived by respondents, at a radius of between ***520 and 1060 meters.*** This number varies somewhat by region. We'll use 1000 meters in this study as a conservative estimate, to catch everyone.

<br>
<br>

## Sample Code for Multi-Round Queries

```{r, eval = FALSE}
# For example, this is how you would run MULTI-part queries

# Get all data
super_query = function(mysearch){
  # Run initial query
  # To get first 20 results
  myquery <- google_places(
    search_string = mysearch,
    key = mykey)
  
  # Supply token from first query
  # to ask for next 20 results
  myquery2 <- google_places(
    search_string = mysearch,
    page_token = myquery$next_page_token,
    key = mykey)

  # Supply token from second query
  # to ask for final 20 results
  myquery3 <- google_places(
    search_string = mysearch,
    page_token = myquery2$next_page_token,
    key = mykey)
  
  # Bind results as a list and return
  list(myquery, myquery2, myquery3) %>%
    return()
}
  
# Test it out
result <- super_query(mysearch = "Parks in Boston, MA")

result

# Extract latitude and longitude
data.frame(
  lng = myquery$results$geometry$location$lng,
  lat = myquery$resultsgeometry$location$lat,
)

# Investigate results
myquery$results

# Map them all together
myquery$results$types %>%
  map(~paste(., collapse = "; ")) %>% 
  unlist()

# In summary, it's a lot of work.
# We focused on the first 20 results,
# since extra results means extra expenses,
# and the potential for expenses due to failed queries with no reward.
```



