---
title: "Replication Code"
subtitle: "for manuscript<br>Trust but Verify: Validating New Measures for Mapping Social Infrastructure in Cities"
author: "Code by Timothy Fraser"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE)
library(rmdformats)
library(tidyverse)
```

# 0. Setup

## 0.1 Packages

To get started, please install these packages!

```{r install_packages}
# Gather a vector of packages currently installed
mypackages <- installed.packages()[,1]

# Get a list of all packages needed
needed <- c(
  # Data wrangling
  "tidyverse", 
  # Modeling
  "gtools", "car", "lmtest",
  # Visualization
  "viridis","ggpubr", "ggtext", 
  # Mapping
  "sf", "ggspatial", "gstat",
  # Census Data
  "tigris", "tidycensus", "censusapi",
  # Images and Icons
  "magick", "fontawesome", "rsvg",
  # Queries
  "googleway",
  # Tables
  "knitr", "kableExtra"
)

# Show me which needed packages have 'not yet' been installed
notyet <- needed[!needed %in% mypackages]

# If there are any packages that have not yet been installed,
if(length(notyet) > 0){
  # Please install them!
  install.packages(notyet)
  print("New packages installed. You're all set!")
}else{
  print("No new packages needed. You're all set!")
}

# Clear data
rm(list = ls())
```

## 0.2 Load Packages

Next, please load main packages for data wrangling. Other packages will be loaded as needed below.

```{r load_packages}
library(tidyverse)
library(sf)
library(viridis)
```

## 0.3 Make Metadata

First, we can gather a few types of data quickly, using data to be made *later in this code!*



Then, this analysis will use several frequently used vectors (eg. GIS projections). We're going to record their details in a list, so we can quickly summon them at any time!

```{r metadata}
list(
  # WGS projection details
  "wgs" = list(
      "name" = "EPSG:4326 (WGS 84) projection",
      "proj" = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs",
      "url" = "https://spatialreference.org/ref/epsg/wgs-84/"),
  # Albers Equal Area Conic projection details  
  "aea" = list(
    "name" = "North American Albers Equal Area Conic Projection",
    "proj" = "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs",
    "url" = "https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/" ),
  # Cell IDs for contiguous study region in inner Boston
  "inner_cells" = read_csv("helper/cell_types.csv") %>% 
    filter(type == "validation") %>% with(cell_id),
  # Cell IDs for sites within greater Boston area
  "boston_cells" = read_csv("helper/cell_types.csv") %>% 
    filter(type != "outside_boston") %>% with(cell_id),
  # Tract GEOIDs for tracts within Boston (excluding Chelsea, etc.)
  "boston_tracts" = read_lines("helper/boston_tracts.txt")
  
) %>% 
  # We'll use dput to 'put' this nested list into a text file,
  # and then we can retrieve it using dget()
  dput(file = "meta.txt")

# Clear data
rm(list = ls())
```


```{r}
# For example
dget("meta.txt")
```

This allows us to quickly access projections, like so:

```{r}
dget("meta.txt")$wgs$proj
```

<br>
<br>

## 0.4 Set API Keys


Next, we're going to make a file called `keys.R`, where we are going to put any keys we use in our environment. However, since every user has a different API key for various products, you're going to have to make your own for the code below to work!

Steps:

1. Make a file called `keys.R` in the main project directory.

2. Save the following code in it.

```{r, eval = FALSE}
Sys.setenv(CENSUS_API_KEY = "MY_API_KEY_HERE!")
```

3. Run the following code, which tells `R` to run that script, without anyone having to see specifically what your API key is :)

```{r}
# Load api keys
source("keys.R")
```


# 1. Geographic Data

Census block data is currently only available through tidycensus from the 2010 decennial census. Further, this data is not complete, since the Census bureau is still double checking numbers for many variables. So, we primarily work with data from the tract or block group levels instead.

<br>
<br>

## 1.1 Gather Census Variables

First, let's make a data.frame listing all the variable IDs we want, each with a corresponding variable name we can use later in the code.

```{r}
# First, let's get a record of all the variables we want!
tribble(
  ~id, ~variable,
    "B01003_001",  "pop", # Total Population
    # Age
    "B01001_020",  "pop_age_65_66_male",
    "B01001_021",  "pop_age_67_69_male",
    "B01001_022",  "pop_age_70_74_male",
    "B01001_023",  "pop_age_75_79_male",
    "B01001_024",  "pop_age_80_84_male",
    "B01001_025",  "pop_age_85_over_male",
    
    "B01001_044",  "pop_age_65_66_female",
    "B01001_045",  "pop_age_67_69_female",
    "B01001_046",  "pop_age_70_74_female",
    "B01001_047",  "pop_age_75_79_female",
    "B01001_048",  "pop_age_80_84_female",
    "B01001_049",  "pop_age_85_over_female",
    
    "B01001_026",  "pop_women", # Gender
    "B02001_002",  "pop_white",
    "B02001_003",  "pop_black", # Estimate!!Total!!Black or African American alone
    "B02001_004",  "pop_natam", #Estimate!!Total!!American Indian andAlaska Native alone
    "B02001_005",  "pop_asian", #Estimate!!Total!!Asian alone
    "B02001_006",  "pop_pacific", #Estimate!!Total!!Native Hawaiian and Other Pacific Islander alone
    "B03001_003",  "pop_hisplat", # Hispanic or Latino
    
    "B06009_001",  "pop_edu_total",
    "B06009_002",  "pop_edu_no_hs",
    "B06009_003",  "pop_edu_hs",
    "B06009_004",  "pop_edu_some_college",
    "B06009_005",  "pop_edu_college",
    "B06009_006",  "pop_edu_grad",
    #"B06009_004",  "pop_some_college",
    "B19083_001",  "income_inequality", #Income inequality: Estimate!!Gini Index)
    
    "B23025_003",  "pop_labor_force", # Estimate!!Total!!In labor force!!Civilian labor force
    "B23025_005",  "pop_unemployed", #Estimate!!Total!!In labor f orce!!Civilian labor force!!Unemployed
    
    "B19013_001",  "median_income", #Estimate!!Median Household income (dollars)!
    "B25105_001",  "median_monthly_housing_cost",
    "B08128_006",  "employees_muni", #Estimate!!Total!!Local government workers
    "B08128_007",  "employees_state", #Estimate!!Total!!State government workers
    "B08128_008",  "employees_fed" #Estimate!!Total!!Federal government workers
) %>%
  mutate(geography = "tract_or_bg") %>%
  write_csv("data/covariates/variables.csv")
```

<br>
<br>

## 1.2 Download Census Data

```{r get_census_data, eval = FALSE}
# Load relevant packages
library(tidycensus)
library(censusapi)

# Next, let's write a quick function to format this data.
# This will work for tracts or block groups.
format = function(data){
  # Import data.frame of census variables  
  vars <- read_csv("data/covariates/variables.csv")

  data %>%
      # Grab these variables
  select(geoid = GEOID, variable_id = variable, estimate) %>%
  # Filter into just Suffolk County
  filter(str_sub(geoid, 1, 5) == "25025") %>% 
  # Join in the appropriate name for each id
  left_join(by = c("variable_id" = "id"), 
            y = filter(vars, geography == "tract_or_bg")) %>%
  # Pivot into a wide matrix, where each variable is a column
  pivot_wider(id_cols = geoid,
              names_from = variable,
              values_from = estimate) %>%
  # Tally up population in this age category
  mutate(pop_age_65_plus = pop_age_65_66_male + pop_age_67_69_male +
           pop_age_70_74_male + pop_age_75_79_male +
           pop_age_80_84_male + pop_age_85_over_male +
           pop_age_65_66_female + pop_age_67_69_female +
           pop_age_70_74_female + pop_age_75_79_female +
           pop_age_80_84_female + pop_age_85_over_female) %>%
  # Tally up total folks with some college education or more
  mutate(pop_some_college = pop_edu_some_college + pop_edu_college + pop_edu_grad) %>%
  # % percentage of population
  mutate_at(
    vars(
      pop_age_65_plus, pop_women,
      pop_white, pop_black, pop_natam,
      pop_asian, pop_pacific, pop_hisplat,
      pop_some_college,
      employees_muni, employees_state, employees_fed),
    # Normalize as a percent
    funs(. / pop)) %>%
  # Calculate percentage of residents unemployed
  mutate(pop_unemployed = pop_unemployed / pop_labor_force) %>%
  # Get age groups
  select(-c(pop_age_65_66_male, pop_age_67_69_male,
            pop_age_70_74_male, pop_age_75_79_male,
            pop_age_80_84_male, pop_age_85_over_male,
            pop_age_65_66_female, pop_age_67_69_female,
            pop_age_70_74_female, pop_age_75_79_female,
            pop_age_80_84_female, pop_age_85_over_female)) %>%
  return()
}


# Get census variables of interest
vars <- read_csv("data/covariates/variables.csv") %>%
  # Narrow to tract or block group level
  filter(geography == "tract_or_bg")


# Census Tracts

# Grab ids we want
vars$id %>%
  # And then query the US census API for those variables!
  get_acs(
    variables = .,
  year = 2019, survey = "acs5",
  state = "MA", county = "025", geography = "tract") %>%
  # Format the results
  format() %>%
  # and save
  write_csv("data/covariates/tracts_data.csv")

# Census Block Groups

# Grab the ids we want
vars$id %>%
  # And then query the US census API for those variables!
  get_acs(
    variables = ., survey = "acs5",
    year = 2019, state = "MA", county = "025",
    geography = "block group") %>%
  # Format the results with appropriate names and transformations  
  format() %>%
  # and save
  write_csv("data/covariates/bg_data.csv")
```

## 1.3 Download Social Capital Indices

```{r}
# Let's download the newest versions of the SCI
library(dataverse)
library(tidyverse)

dataverse::get_dataframe_by_name(
  dataset = "10.7910/DVN/OSVCRC", 
  filename = "index_tracts_V3_04_10_2022.tab",
  server = "dataverse.harvard.edu") %>%
  # Filter to just Boston
  filter(str_sub(geoid, 1,5) == "25025") %>%
  select(year, geoid, social_capital, bonding, bridging, linking) %>%
  # save to file!
  write_csv("data/covariates/sci_census_tracts_2022_04_10.csv")
```


## 1.4 Polygons

In this part of the script, we're going to compile all necessary pieces of data for later analysis.

```{r get_polygons, eval = FALSE}
# Get our projections file
meta <- dget("meta.txt")

## County Polygons

# Download the polygons for Suffolk County, MA
tigris::counties(cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Filter to Suffolk Count in Massachusetts
  filter(statefp == "25", name %in% c("Suffolk")) %>%
  # Zoom into just these variables
  select(name, geoid, area_land = aland, geometry) %>%
  # Write to file
  st_write("shapes/county.geojson", delete_dsn = TRUE)

## Census Tract Polygons

# Download the polygons for Suffolk County census tracts
tigris::tracts(state = "MA", county = "025", cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Keep only a subset of variables
  select(geoid, area_land = aland, geometry) %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("data/covariates/tracts_data.csv",
                         col_types = list(geoid = col_character()))) %>%
  # Join in social capital indices from 2020
  left_join(by = "geoid",
            y = read_csv("data/covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid)) %>%
              filter(year == 2020) %>% select(-year)) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Write to file
  st_write("shapes/tracts.geojson", delete_dsn = TRUE)



# Load in census tract level demographics from 2019
read_sf("shapes/tracts.geojson") %>%
  # Transform to the Equal Area Conic projection
  st_transform(crs = dget("meta.txt")$aea$proj) %>%
  # Keep just land tracts
  filter(area_land > 0) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk County
  filter(!str_sub(geoid, 6,7) %in% c(15:18)) %>%
  # Next, we're going to get the exact list of census tract GEOIDs in Boston
  with(geoid) %>%
  write_lines("helper/boston_tracts.txt")


# This is what creates our metadata value: boston_tracts


# Census block data is currently only available through tidycensus from the 2010 decennial census. Further, this data is not complete, since the Census bureau is still double checking numbers for many variables. So, I gathered data from the block group level instead.


## Census Block Group Polygons

# Download block group boundaries!
tigris::block_groups(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(geoid = GEOID, area_land = ALAND, geometry) %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("data/covariates/bg_data.csv",
                         col_types = list(geoid = col_character()))) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000))  %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = meta$wgs$proj) %>%
  # Write to file
  st_write("shapes/block_groups.geojson", delete_dsn = TRUE)

# Census Block Polygonns

# Download block boundaries.
tigris::blocks(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = meta$wgs$proj) %>%
  select(geoid = GEOID20, area_land = ALAND20, geometry) %>%
  # Write to file
  st_write("shapes/blocks.geojson", delete_dsn = TRUE)


# Boston Polygon Outline

# Create one big polygon outline of tracts within Boston
read_sf("shapes/tracts.geojson") %>% 
  st_transform(crs= meta$aea$proj) %>%
  summarize(geoid = "Boston", geometry = st_union(geometry)) %>%
  st_transform(crs = meta$wgs$proj) %>%
  st_write("shapes/boston.geojson", delete_dsn = TRUE)


read_sf("shapes/tracts.geojson") %>%
  st_transform(crs= dget("meta.txt")$aea$proj) %>%
  filter(geoid %in% read_lines("helper/boston_tracts.txt")) %>%
  summarize(geometry = st_union(geometry)) %>%
  st_transform(crs = dget("meta.txt")$wgs$proj)  %>%
  mutate(type = "Boston") %>%
  st_write("shapes/boston.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```

<br>
<br>


## 1.5 Fishnet Grids and Points

Next, we're going to make a series of fishnet grids for our analyses.

```{r make_fishnets, eval = FALSE}
# Get projections
meta <- dget("meta.txt")

# Import tracts
tracts <- read_sf("shapes/tracts.geojson") %>%
  # Transform to equal area conic projection
  st_transform(crs = meta$aea$proj)  

# Let's write a 'fishnet()' function to make grids of a given 'size'
# for each set of tract polygons supplied
fishnet = function(data, size){
 data %>%
  # make a cell size of 'XXXX' km squared 
  # (1 city block is ~100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = size, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(data) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(cell_id = 1:n()) %>%
  return()
}


# 1 city block is 1000 meters long;
# 2 blocks are 2000 meters long
# 5 blocks are 5000 meters long
# 10 blocks are 10000 meters long
tibble(size = c(1000, 2000, 5000, 10000)) %>%
  # For each size of grid,
  split(.$size) %>%
  # Generate a grid, saving the size in a 'size' column
  map_dfr(~fishnet(tracts, size = .$size), .id = "size") %>%
  # Transform back to normal coorindate projection
  st_transform(crs = meta$wgs$proj) %>%
  # Save fishnet grids to file
  st_write("shapes/fishnet.geojson", delete_dsn = TRUE)


# Get centroids from these points
read_sf("shapes/fishnet.geojson") %>%
  # Transform to Equal Area Conic projection!
  st_transform(crs = meta$aea$proj) %>%
  # Now let's extract the centroids of these cells too
  group_by(size, cell_id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
  # Transform back to normal coorindate projection
  st_transform(crs = meta$wgs$proj) %>%
  # Extract centroid coordinates
  mutate(lng = st_coordinates(geometry)[,1],
         lat = st_coordinates(geometry)[,2])  %>%
  # Save to file
  st_write("shapes/fishnet_points.geojson", delete_dsn = TRUE)
```

<br>
<br>

## 1.6 Classify Grid Cells

```{r}
list(
  # Validation study area cells
  validation = c(60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77,
                 78, 79, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98,
                 99, 106, 107, 108, 109, 110, 111, 123, 124, 125, 126, 127, 128, 
                 142, 143, 144, 145, 146, 147, 148, 161, 162, 163, 164, 
                 175, 183, 174),
  # Cells around validation area that are not very comparable to validation area,
  # due to being split by water, etc., thus dubbed "excluded"
  # Excluded from Validation area
  excluded_validation = c(50, 62, 72, 80, 81, 89, 100, 112, 129, 
                          149, 160, 165, 166, 176, 182),
  
  # Cells located within Boston city limits, 
  # but not in validation and not incomparable for some other reason
  boston = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 
             18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 
             33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48,
             49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 90, 91, 92, 103, 
             104, 105, 119, 120, 121, 122, 137, 138, 139, 140, 141, 156, 
             157, 158, 159, 172, 184, 185, 192, 193, 194, 195, 
             196, 203, 204, 205, 206, 207, 208,  218, 219, 225, 226),
  
  # Cells located within Boston city limits,
  # but not comparable (due to being airport, islands, etc.)
  # Excluded from Boston
  excluded_boston = c(12, 19, 38, 39, 63, 101, 102, 113, 114, 115, 116, 117, 118, 130, 
                      131, 132, 133, 134, 135, 136, 150, 151, 152, 153, 154, 155, 167, 168,
                      169, 170, 177, 178, 179, 180, 186, 187, 188, 189, 191, 197, 198, 199, 
                      200, 202, 209, 210, 214, 220, 227, 232, 233, 234,
                      181, 171,  173),
  
  
  # Cells located outside of Boston
  outside_of_boston = c(190, 201, 211, 212, 213, 215, 216, 217, 221, 
                        222, 223, 224, 228, 229, 230, 231, 235, 236, 
                        237, 238, 239, 240, 241, 242, 243, 244, 245, 
                        246, 247, 248, 249, 250, 251, 252, 253, 254, 
                        255, 256, 257, 258, 259, 260, 261)
) %>%
  map_dfr(~tibble(cell_id = .), .id = "type") %>%
  write_csv("helper/cell_types.csv")
```

## 1.7 Spatially Average to Grid Cell Level

### Block Group Membership Rates (Simply Analytics)

```{r}
library(sf)
# Simply Analytics made probabilistic estimates of membership 
# block-group-to-block-group based on demographics
bind_rows(
  read_sf("data/covariates/simply_analytics/bonding.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id, 
           fraternal = value0, religious = value1, 
           veterans = value2, country_clubs = value3) %>%
    as_tibble(),
  
  read_sf("data/covariates/simply_analytics/bridging.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           civic = value0, business = value1, collectors = value2, 
           union = value3, charitable = value4) %>%
    as_tibble(),
  
  read_sf("data/covariates/simply_analytics/linking.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           church_board = value2, school_board = value1, local_board = value0) %>%
    as_tibble()) %>%
  select(-geometry) %>%
  pivot_longer(cols = -c(geoid), names_to = "variable", 
               values_to = "x", values_drop_na = TRUE) %>%
  pivot_wider(id_cols = c(geoid), names_from = variable, values_from = x) %>%
  # Join back in geographies from one of them
  left_join(by = "geoid",
            y = read_sf("data/covariates/simply_analytics/linking.shp") %>%
              magrittr::set_colnames(value = names(.) %>% tolower()) %>%
              select(geoid = spatial_id, geometry)) %>%
  # Combine into one geojson file
  st_write("data/covariates/simply_analytics/bg_membership_rates.geojson", delete_dsn = TRUE)




# Import grid cells
read_sf("shapes/fishnet.geojson") %>%
  select(cell_id, size, geometry) %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = dget("meta.txt")$aea$proj) %>%
  # Join in overlapping block groups
  st_join(read_sf("data/covariates/simply_analytics/bg_membership_rates.geojson") %>%
            st_as_sf() %>%
            st_transform(crs = dget("meta.txt")$aea$proj)) %>%
  as_tibble() %>%
  # Average to grid per cell
  group_by(cell_id) %>%
  summarize_at(vars(fraternal:local_board),
               list(~mean(., na.rm = TRUE))) %>%
  ungroup() %>%
  write_csv("data/covariates/grid_membership_rates.csv")
```

### Precinct Voting Rates

First, let's download our precinct data from the Harvard Dataverse!

```{r, eval = FALSE}
library(tidyverse)
library(dataverse)

# Let's download from Tim Fraser's Boston voting dataset on Harvard Dataverse...
# DOI: 10.7910/DVN/MMSBGJ

# Get projections
meta <- dget("meta.txt")

# Download voting data for 2020 presidential election by precinct in Boston.
votes <- dataverse::get_dataframe_by_name(
  dataset = "doi:10.7910/DVN/MMSBGJ", 
  server = "dataverse.harvard.edu",
  filename = "boston_votes_data.tab")  %>%
  # Recode data values
  mutate(type = type %>% tolower() %>% recode_factor(
    "biden and harris" = "democrat_percent",
    "trump and pence" = "republican_percent",
    "ballots cast" = "turnout_percent")) %>%
  select(ward_precinct, type, percent) %>%
  pivot_wider(id_cols = c(ward_precinct), names_from = type, values_from = percent)

# Download Boston precinct polygons, as of 2017
# Originally sourced from:
# https://data.boston.gov/dataset/precincts/resource/95d37dea-9dec-4fda-a8a6-c7d9df7adc35
dataverse::get_dataframe_by_name(
  dataset = "doi:10.7910/DVN/MMSBGJ", 
  server = "dataverse.harvard.edu",
  filename = "boston_precinct_polygons.kml",
  .f = sf::read_sf) %>%
  select(ward_precinct = WARD_PRECINCT, geometry) %>%
  # Join in voting data
  left_join(by = "ward_precinct", y = votes) %>%
  # Convert and save
  st_as_sf(crs = meta$wgs$proj) %>%
  st_make_valid() %>%
  st_write("shapes/precincts.geojson", delete_dsn = TRUE)


# Aggregate precincts into ward polygons
read_sf("shapes/precincts.geojson") %>%
  select(ward_precinct, geometry) %>%
  mutate(ward = str_sub(ward_precinct, 1,2)) %>%
  group_by(ward) %>%
  summarize(geometry = st_union(geometry)) %>%
  ungroup() %>%
  st_write("shapes/wards.geojson", delete_dsn = TRUE)  


# Finally, let's spatially average voting measures 
read_sf("shapes/fishnet.geojson") %>%
  # to the 1 km^2 grid cell size
  filter(size == 1000) %>%
  select(cell_id, geometry) %>%
  st_transform(crs = meta$aea$proj) %>%
  # Joining precinct data into the grid
  st_join(read_sf("shapes/precincts.geojson") %>%
            st_transform(crs = meta$aea$proj)) %>%
  as_tibble() %>% 
  # And averaging per grid cell
  group_by(cell_id) %>%
  summarize(
    across(cols = c(democrat_percent, republican_percent, turnout_percent),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  select(cell_id, contains("percent")) %>%
  # Then saving the result to file
  write_csv("data/covariates/grid_voting_data.csv")

# While, we're at it, 
# let's gather two more types of data.


# First, using the following set of zipcode polygons,
# Let's get average measures by zipcode.
read_sf("shapes/zipcodes.geojson") %>% 
  st_transform(crs = meta$aea$proj) %>%
  # Joining precinct data into the grids
  st_join(read_sf("shapes/precincts.geojson") %>%
            st_transform(crs = meta$aea$proj)) %>%
  # And averaging per zipcode
  group_by(geoid) %>%
  summarize(
    across(cols = c(democrat_percent, republican_percent, turnout_percent),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  # Then saving the result to file
  write_csv("data/covariates/zipcode_voting_data.csv")

# Second, let's download Polling Places in Boston as Points
#https://data.boston.gov/dataset/polling-locations/resource/f5b67c80-98a9-4e51-bfa0-ffdb8961d547
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/f7c6dc9eb6b14463a3dd87451beba13f_5.kml?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(-c(Name:icon)) %>%
  st_write("shapes/polls.geojson", delete_dsn = TRUE)

# Clear Data
rm(list = ls())
```

### Tract Measures

Finally, we need to spatially average tract data to the census tract level.

```{r grid_covariates, message = FALSE, warning = FALSE,  eval = FALSE}
# Load packages
library(tidyverse)
library(sf)

# Import metadata (projections)
meta <- dget("meta.txt")

# Import census tracts and data
tracts <- read_sf("shapes/tracts.geojson") %>%
  # Multiply into grid from 2011 to 2020
  expand_grid(year = 2011:2020) %>%
  # cut 2020 social capital,
  select(-c(social_capital:linking)) %>%
  # and join in for every year instead!
  left_join(by = c("geoid", "year"),
            y = read_csv("data/covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid))) %>%
  # format as sf
  st_as_sf() %>%
    # Transform to equal area conic projection
  st_transform(crs = meta$aea$proj) 


# Import grid cells
read_sf("shapes/fishnet.geojson") %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = meta$aea$proj) %>%
  # Spatially average tract data to grid level
  # Join in the census tract data
  st_join(tracts) %>% 
  # Convert to dataframe
  as_tibble() %>%
  # For each grid cell,
  group_by(year, cell_id) %>%
  summarize(
    # Take average of these variables
    across(.cols = c(pop_density,
                     pop_women, pop_white, pop_black, pop_asian,
                     pop_hisplat, pop_natam, pop_pacific,
                     pop_some_college,
                     employees_muni, employees_state, employees_fed,
                     median_income, income_inequality, pop_unemployed,
                     median_monthly_housing_cost, pop_age_65_plus,
                     social_capital, bonding, bridging, linking),
           .fns = ~mean(.x, na.rm = TRUE))) %>%
  ungroup() %>%
  # Save to file!
  write_csv("data/covariates/grid_tract_measures.csv")
```
## 1.8 Combine Measures

```{r}
# Load packages
library(tidyverse)
library(sf)
library(gstat)

# Import metadata (projections)
meta <- dget("meta.txt")

# Import grid cells
grid <- read_sf("shapes/fishnet.geojson") %>%
  # For just 1-km^2 cells
  filter(size == 1000) %>%
  # Join in the milestone labels for each grid cell
  left_join(by = "cell_id", 
            y = read_csv("helper/milestones.csv")) %>%
  # Join in our cell-type classifier
  left_join(by = "cell_id",
            y = read_csv("helper/cell_types.csv")) %>%
  # Join in our tract measures (+ social capital indices for 10 years 2011 to 2020)
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_tract_measures.csv")) %>%
  # Join in our block-group org membership rates
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_membership_rates.csv")) %>%
  # Join in our precinct voting rates
  left_join(by = "cell_id",
            y = read_csv("data/covariates/grid_voting_data.csv")) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = meta$aea$proj) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  # In a few cases, tracts/block groups/blocks 'ostensibly' had no residents, 
  # but they're in Boston! So it's better to treat these as NA 
  # so no population density.
  mutate(pop_density = case_when(
    is.infinite(pop_density) ~ NA_real_,
    pop_density == 0 ~ NA_real_,
    TRUE ~ pop_density))

# Get just one year-long slice (pop-density is same in all of them)
shapes <- grid %>%
  filter(year == 2020) %>%
  select(cell_id, pop_density, geometry)

# Some areas of Boston *are* populated, by the census doesn't pick them up. 
# We'll use spatial interpolation to estimate their 
# population density based on surrounding grid cells.

# Spatially interpolate population density!
gstat::gstat(
    id = "cell_id", 
    formula = pop_density ~ 1, 
    # Use a inverse-distance-squared weighting
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(shapes) %>%
  # Bring back cell id
  mutate(cell_id = shapes$cell_id) %>%
  as.data.frame() %>%
  select(cell_id, pop_density_int = cell_id.pred) %>%
  # Join those results into the grid from 2011 to 2020!
  right_join(by = "cell_id", y = grid) %>%
  # Convert back to WGS for easy sharing
  st_as_sf(crs = meta$aea$proj) %>%
  st_transform(crs = meta$wgs$proj) %>%
  # Save  to file
  st_write("shapes/grid_covariate_annual.geojson", delete_dsn = TRUE)

# Now make a version with just the year 2020!
read_sf("shapes/grid_covariate_annual.geojson") %>%
  filter(year == 2020) %>% select(-year) %>%
  # Save to file!
  st_write("shapes/grid_covariates.geojson", delete_dsn = TRUE) 

# Clear data
rm(list = ls())
```


<br>
<br>

## 1.9 Frequent Mapping Data

Let's also create a simple script we can run at any time to load in the following common data.

```{r mapdata_script}
'
library(tidyverse)
library(sf)

# Import metadata
meta <- dget("meta.txt")

# Import tracts
tracts <- read_sf("shapes/tracts.geojson") %>%
  st_transform(crs= meta$aea$proj) 

# Import Boston boundaries
boston <- read_sf("shapes/boston.geojson") %>%
  st_transform(crs = meta$aea$proj)

# Load in Suffolk county background
county <- read_sf("shapes/county.geojson") %>%
  st_transform(crs= meta$aea$proj)

# Import fishnet grid
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  st_transform(crs= meta$aea$proj)
' %>%
  write_lines(file = "helper/mapdata.r")
```

Now, if we ever want to get those files, all at once, we can just use the `source()` function to run the `helper/mapdata.r` function.

```{r}
# For example, let's run it!
source("helper/mapdata.r")
# And clear the data
rm(list = ls())
```


## Figure Z1: Boston Tracts

Next, let's visualize where the tracts in Boston compare to all of Suffolk County.

```{r}
# Load in necessary data
source("helper/mapdata.r")

viz <- ggplot() +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "darkgrey", size = 5, fill = "white") +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "black", fill = "white") +
  geom_sf(data = tracts %>% filter(geoid %in% meta$boston_tracts),
          mapping = aes(size = "Tracts"),
          fill = "tan", color = "black") +
  scale_size_manual(values = c(1.15, 0.1)) +
  geom_sf(data = county, color = "black", size = 1.5, fill = NA) +
  theme_void(base_size = 14) +
  labs(size = "Borders",
       subtitle = paste("Boston Census Tracts", 
                        " (n = ", nrow(tracts), ")", sep = ""))

# Print to file!
ggsave(viz, filename = "figures/other/figure_Z1.png", 
       width= 3, height = 3.25)

rm(list = ls())
```


```{r, echo = FALSE, out.width="100%", fig.cap = "Boston Tracts"}
knitr::include_graphics("figures/other/figure_Z1.png", dpi = 100)
```

## Figure Z5: Cell IDs Mapped

```{r}
grid <- read_sf("shapes/grid_covariates.geojson")

g1 <- ggplot() +
  geom_sf(data = grid, 
          mapping = aes(fill = type), alpha = 0.75) +
  geom_sf_text(data = grid,
               mapping = aes(label = cell_id))  +
  theme_void() +
  theme(legend.position = "right") 

ggsave(g1, filename = "figures/other/figure_Z5.png", dpi = 300, width = 8, height = 7.5)
```


## Figure A5: Map of Cell Types 

Using a dataset that we make later, let's just grab the cell IDs of the cells we use in our study range, so we can highlight them in the visual below.

```{r}
# Get basic map data
source("helper/mapdata.r")

# Add variable to grid
grid <- grid %>%
  mutate(type_label = type %>% recode_factor(
    "validation" = "<b>Validation Cells</b><br>(n = 57)<br>",
    "boston" = "<b>Boston Cells</b><br>(n = 106)<br>",
    "excluded_validation" = "<b>Boston Cells</b><br>(n = 106)<br>",
    "excluded_boston" = "<b>Ineligible Cells<br>(n = 55)</b><br>",
    "outside_of_boston" = "<b>Outside Boston</b><br>(n = 43)<br>(Excluded)"))

library(tidyverse)
library(sf)
library(ggtext)

g1 <- ggplot() +
  geom_sf(data = tracts, mapping = aes(size = "Tracts"), 
          color = "black",  fill = "grey") +
  geom_sf(data = grid, mapping = aes(fill = type_label), 
          color = "white", size = 0.25, alpha = 0.25) +
  scale_color_manual(values = c("#FFB000", "#DC267F", "#785EF0", "#FE6100")) +
  scale_fill_manual(values = c("#FFB000", "#DC267F", "#785EF0", "#FE6100")) +
  geom_sf(data = county, mapping = aes(size = "County"), 
          color = "black", fill = NA) +
  scale_size_manual(values = c(1, 0.1)) +
  labs(
    fill = paste("Cells (n = ", nrow(grid), ")<br> in Suffolk County", sep = ""),
    size = "Borders",
    title = expression(paste("1 ", km^2,  " grid cells for tallying sites")),
    subtitle = "<i>(n = 163 eligible cells in Boston)</i>") +
  guides(color = "none", size = guide_legend(override.aes = list(
    fill = c("white", "grey"))))  +
  ggspatial::annotation_north_arrow(height = unit(0.5, "cm"),
                                    width = unit(0.5, "cm")) +
  ggspatial::annotation_scale(pad_x = unit(3.5, "cm")) +
  theme_void(base_size = 14) +
  theme(plot.title = element_text(face = "bold", size = 15),
        plot.subtitle = element_markdown(face = "italic", size = 14, hjust = 0),
        legend.text = element_markdown(size = 12),
        legend.title = element_markdown(size = 12),
        legend.position = "right") 

# There are 261 cells of 1 square kilometer in Suffolk County
ggsave(g1, filename = "figures/figure_A5.png", dpi = 500, height = 4, width = 4.5)
```

## Figures Z6-Z7: Voting Maps

```{r}
library(tidyverse)
library(viridis)

# Load in geographic data
precincts <- read_sf("shapes/precincts.geojson")
wards <- read_sf("shapes/wards.geojson")

# Map Votes for Democrats
g1 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = precincts,
          size = 0.2, color = "white",
          mapping = aes(fill = democrat_percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voted\nDemocrat",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g1, filename = "figures/other/figure_Z6.png", dpi = 500, height = 6, width = 9)

# Map Votes for Republicans
g2 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = precincts,
          size = 0.2, color = "white",
          mapping = aes(fill = turnout_percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voter Turnout",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g2, filename = "figures/other/figure_Z7.png", dpi = 500, height = 6, width = 9)

# Clear Data
rm(list = ls())
```

# 2. API Test Run

First, we conduct a test-run using the Google Maps Places API, accessed via the ```googleway``` package.

- This demonstration will use D. Cooley's helpful *googleway* vignette from November 2020, available [here](https://cran.r-project.org/web/packages/googleway/vignettes/googleway-vignette.html#google-places-api).

- You can check out pricing [here](https://cloud.google.com/maps-platform/pricing). The whole point of this demonstration is to figure out how much it will cost to run this.


## 2.1 API Key

First, we have to enable the Google Maps Places API. Second, we need to get an API Key. I made it restricted to work only from RStudio Cloud and only for the Places API. Let's set it below.

```{r, eval = FALSE}
# Load package
library(googleway)

# Let's load in your API key here
mykey <- Sys.getenv("GOOGLEWAY_KEY")

# Now set the API KEY
set_key(key = mykey, api = "places")

# View which API Keys we currently have registered here
# google_keys()
# If you need to use it, clear keys will clear those keys too.
#clear_keys() ## clear any previously set keys
```

## 2.2 Test Query (Places of Worship)

Next, we need to test out the API!

```{r, eval = FALSE}
# Make a 'query' folder to hold results
dir.create("query")

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 1000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat)

# Let's do this for all of Boston
testquery <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

# Get places of worship
mypoints %>%
  split(.$cell_id) %>%
  map_dfr(~google_places(
    location = c(.$lat[1], .$lng[1]),
    keyword = "place of worship",
    radius = 770, # circumference of a circle that extends that far
    key = mykey)$result, .id = "grid") %>%
  # Save to file
  saveRDS("query/test/test_query.rds")
```

## 2.3 Process Test Query

```{r}
# Let's write a quick function to process query output into point data
format_points = function(data){
  # Load metadata, for projections
  meta <- dget("meta.txt")
  
  data %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
  st_transform(crs = meta$wgs$proj)  %>%
  # Grab just the distinct results
  select(-grid) %>%
  group_by(place_id) %>%
  summarize_at(vars(name, business_status, 
                 #plus_code.compound_code, 
                 rating, types, user_ratings_total, vicinity, 
                 permanently_closed, geometry),
            funs(unique(.))) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
  map(~paste(., collapse = "; ")) %>%
  unlist()) %>%
  return()
}

# Now generate points
read_rds("query/test/test_query.rds") %>%
  format_points() %>%
  saveRDS("query/test/test_points.rds")
```

## 2.4 Test Query Diagnostics

How many unique addresses are there? Does every vicinity have an ID?

```{r}
myquery <- read_rds("query/test/test_query.rds")

# How often do we duplicate?
myquery %>%
  group_by(vicinity) %>%
  count() %>%
  ggplot(mapping = aes(x = reorder(vicinity, -n), y = n)) +
  geom_col(color = "white", size = 0.1, fill = "steelblue") +
  theme_classic(base_size = 14) +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank()) +
  labs(x = "378 unique place addresses found (n = 1209 search results)", 
       y = "# of Duplicate Results")
```

### Figure Z2: Test Query Points Mapped

```{r}
# Get common mapping data
source("helper/mapdata.r")

# Process the set of queries
myquery <- read_rds("query/test/test_points.rds")

# So, this approach gathered 3 times as many results as there actually were - a really hefty amount of duplication, due to the small grid cell size

# Create buffers
points <- grid %>%
  mutate(geometry = st_buffer(st_centroid(geometry), dist = 1410/2)) %>% 
  st_as_sf() 

viz <- ggplot() +
  geom_sf(data = points %>% 
            st_buffer(dist = 770) %>% 
            st_as_sf(), color = "black", alpha = 0.1, size = 0.05) +
  geom_sf(data = grid, fill = "firebrick", color = "white", size = 0.1, alpha = 0.1) +
  geom_sf(data = myquery, color = "firebrick", size = 1.5, alpha = 0.5) + 
  geom_sf(data = county, color = "black",size = 1, fill = NA) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(subtitle = "Places of Worship (red, n = 422)\n per 1 kilometer grid cells (n = 260)\nacross Populated Census Tracts\nin Suffolk County, Massachusetts")

# Save to file
ggsave(viz, filename = "figures/other/figure_Z2.png", dpi = 300, width = 4, height = 6)

# Clear data
rm(list = ls())
```

### Figure A2: Compare grid cell sizes

```{r, eval = FALSE}
# Load common map data
source("helper/mapdata.r")

myquery <- read_rds("query/test/test_points.rds")  %>%
    # Convert to Equal Area projection for plotting
  st_transform(crs = meta$aea$proj) 

# number of points lost vs. dollars lost

# Import all different fishnet grids
myfish <- read_sf("shapes/fishnet.geojson") %>%
  # Convert to Equal Area projection for plotting
  st_transform(crs = meta$aea$proj) %>%
  # Recode variable names
  mutate(type = size %>% recode_factor(
    "1000" = "<b>1 km<sup>2</sup></b>
    <br> <br>
    Error:<br>
    <b>Cells</b><sup>1</sup>:   0 / 260<br>
    <b>Points</b><sup>2</sup>: 0 (0%)<br>
    <b>Net gain</b><sup>3</sup>: 54.1",
    "2000" = "<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3",
    "5000" = "<b>5 km<sup>2</sup></b><br><br>Error:<br>7 / 17<br>206 (48%)<br>19.6",
    "10000" = "<b>10 km<sup>2</sup></b><br><br>Error:<br>3 / 6<br>321 (76%)<br>-1222")) %>%
  # Join in places of worship
  st_join(myquery) %>%
  # Tally how many there are per grid cell
  group_by(cell_id, type) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  mutate(level = case_when(
    count == 0 ~ "0",
    count > 0 & count <= 5 ~ "0-5",
    count > 5 & count <= 10 ~ "6-10",
    count > 10 & count <= 15 ~ "11-15",
    count > 15 & count <= 20 ~ "16-20",
    count > 20 & count <= 30 ~ "21-30",
    count > 30 ~ "31+") %>%
      factor(levels = c("0", "0-5", "6-10","11-15","16-20","21-30", "31+"))) %>%
  mutate(indicator = if_else(count > 20, "Problematic", "Fine"))

# Create a box for annotations
mybox <- data.frame(
  xmin = 1900828.7,
  ymin = 514765.5,
  xmax = 1920828.7,
  ymax = 544765.5,
  label = "Most Efficient",
  type = factor("<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3"))

# Visualize it!
g1 <- ggplot() +
  geom_sf(data = myfish, mapping = aes(fill = level,
                                       color = indicator, size = indicator)) +
  geom_sf(data = myquery, color = "#648FFF", size = 1.5, alpha = 0.5) + 
  scale_color_manual(values = c("white", "black")) +
  scale_size_manual(values = c(0.75, 1.25)) +
  geom_sf(data = myfish %>%
            filter(count > 20), 
          mapping = aes(color = indicator, size = indicator),
          fill = NA) +
  geom_sf(data = county, color = "black",size = 0.75, fill = NA) +
  facet_wrap(~type, ncol = 4) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = ggtext::element_markdown(size = 14, hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        
        strip.text.x = ggtext::element_markdown(size = 12, hjust = 0.5),
        plot.margin = margin(0,0,0,0, "cm")) +
  viridis::scale_fill_viridis(discrete = TRUE, 
                              option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(
    fill = "# of Places\nof Worship\nper cell",
    subtitle = "<b>Places of Worship</b> (<span style='color:#648FFF'><b>blue</b></span>, n = 422) by Size of Grid Cell<br>across Populated Census Tracts in Suffolk County") +
    
    #caption = "<sup>1</sup><b> Cells:</b> counts number of cells (out of total) which registered above 20 sites per cell, leading to points missed.
    #<br>
    #<sup>2</sup><b> Points:</b> counts total places of worship (and percentage) <i>missed</i> due to exceeding the 20 point limit per grid cell search.
    #<br>
    #<sup>3</sup><b> Net gain:</b> measures total points captured (per USD spent), <i>minus</i> total points missed (per USD spent).<br><i>Highest</i> number indicates most cost-effective grid. <b>Black dashed box</b> indicates most efficient grid.") +
  guides(fill = guide_legend(override.aes = list(
    color = c("white", "white", "white", "white", "black", "black"),
    size = c(0.75,0.75,0.75,0.75,1.5, 1.5))),
         color = "none", size = "none") +
 geom_rect(
    data = mybox,
    mapping = aes(xmin = xmin - 2000,
                  ymin = ymin,
                  xmax = xmax + 1000,
                  ymax = ymax),
    fill = NA, color = "black",linetype = "dashed", size = 1) +
   geom_text(
    data = mybox,
    mapping = aes(x = (xmin + xmax)/2,
                  y = ymin - 2000,
                  label = label),
    fill = NA, color = "black",linetype = "dashed", size = 5) 
  
# Save to file!
ggsave(g1, filename = "figures/figure_A2.png", dpi = 500, 
         width = 9.5, height = 4.5)

# Remove data
rm(list = ls())
```

The above visual uses data from the following tables...

```{r}
# Number of purple cells vs. total cells
myfish %>% 
  as.data.frame() %>%
  mutate(count = if_else(count > 20, count - 20, 0)) %>%
  group_by(type, indicator) %>%
  summarize(sum = sum(count))
# Points missed
```

```{r}
#Total points recorded / Total points missed
#Cost of points recorded / cost of points missed
#Payoff Ratio: 

data.frame(
  label = c("1 km", "2 km", "5 km", "10 km"),
  cells_gained = c(260, 77, 10, 3),
  cells_error = c(0,   4, 7, 3),
  price = 0.03,
  points_total = 422,
  points_gained = c(422, 413, 216, 101)) %>%
  mutate(cells_total = cells_gained + cells_error,
         points_error = points_total - points_gained) %>%
  # How much does it cost, on average, to record a point?
  # Cost of gain + cost of error
  
  # points gained per dollar spent
  # Gained this many points given this cost
  mutate(valued_added = points_gained / (cells_total*price),
         valued_lost = points_error / (cells_total*price),
         
         net_valued_added = valued_added - valued_lost)
```

So, the Places API will give us 20 results per query. It is possible to obtain up to 60 total sites from each initial query (although each time you ask for the next 20, eg. 21-40, 41-60, that they will charge that as a query as well, and it's computationally a little difficult).

However, you get a lot of information from each request. There are two types of requests, roughly - "basic search" and "details". Basic search still delivers a bunch of information, including the *place ID,* *address*, *status of the establishment (open/closed/etc.), type of place* (eg. park, point of interest, restaurant, hotel), the *opening hours*, *etc., a rating,* and *longitude and latitude,* store in the geometry.


Really, the best way to do this is to query on a grid. We *will* get overlap, because we will have to use circles that overlap slightly with a square fishnet grid, but we can use the place_IDs as a way to eliminate duplicates. This is much worth the cost, because otherwise, we would almost certainly leave places out.

## 2.5. API Real Searches

Having run this initial test, we will now gather social infrastructure sites using our 2 km (most efficient) grid, and tally the results up using our 1 km grid.

This project uses the Google Maps Places API to collect geocoded point and polygon data of social infrastructure sites in the United States. For ~$0.03 per longitude-latitude coordinates queries, the API provides a near-complete record of the locations and meta-data of up to 20 social infrastructure sites within a 1 square-kilometer area, the average size of an American’s neighborhood (Donaldson 2013). 

The first goal is to identify the best, most cost effective strategy for mapping social infrastructure.

So, that means identifying the best keywords.

Ideally, for each 2 km<sup>2</sup> grid cell, we want to run 19 searches, for:

(1) "libraries", community center, places of worship, or city hall," 

(2) "parks, squares, or fountains," 

(3) "cafes, coffeeshops, bookstores, barbershops, or beauty salons," and 

(4) "sports field, recreation center, museums, art galleries, zoos, aquariums, gardens." 


### Figure A1: Search Methods

There are three ways to search for data. First, we could use a search term, which is a query of any kind you want. This tends to return popular results, meaning it undercounts social infrastructure substantially, instead returning the most popular churches in the city, for example. Second, you could use keywords. This is highly effective, but you can only use one.

```{r,eval = FALSE}
# Let's write a function to format results
convert_points = function(mydata){
  mydata$results %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
    st_transform(crs = aea)  %>%
    # Grab just the distinct results
    group_by(place_id) %>%
    summarize_at(vars(name, business_status, 
                      #plus_code.compound_code, 
                      rating, types, user_ratings_total, #vicinity, 
                      #permanently_closed,
                      geometry),
                 funs(unique(.))) %>%
    ungroup() %>%
    # Classify the type
    mutate(types = .$types %>%
             map(~paste(., collapse = "; ")) %>%
             unlist()) %>%
    return()
  
}

# Get projections
meta <- dget("meta.txt")
# In case you've deleted it (I had)
# re-summon your Google Places API key from the environment
mykey <- Sys.getenv("GOOGLEWAY_KEY")

# Get University Location
neu <- data.frame(lon = c(-71.0945748), lat = c(42.3403499)) %>%
  st_as_sf(crs = meta$wgs$proj, coords = c("lon", "lat")) %>%
  st_transform(crs = meta$aea$proj)
# Located in grid cell 124

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 1000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat) %>%
  filter(cell_id == 124)

# Let's do this for all of Boston
run_string <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  search_string = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_keyword <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_type <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  place_type = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

dat <- bind_rows(
  run_string %>%
    convert_points() %>% mutate(id = "String Search"), 
  run_keyword %>% 
    convert_points() %>% mutate(id = "Keyword Search"), 
  run_type %>% 
    convert_points() %>% mutate(id = "Type Search"),
  neu %>%
    mutate(id = "Northeastern University")) 

g1 <- ggplot() +
  geom_sf(data = points, color = "darkgrey", fill = "white",
          size = 31, shape = 21, alpha = 0.5) +
  geom_sf(data = fish, color = "black", fill = NA) +
  geom_sf(data = fish %>% filter(id == 124), fill = "darkgrey", color = "black") +
  geom_sf(data = dat, mapping = aes(color = id), size = 3, alpha = 0.5) +
  geom_sf(data = neu, size = 3, color = "black") +
  coord_sf(xlim = c(1904498, 1911007),
           ylim = c(523391, 531900)) +
  theme(panel.background = element_rect(fill = "tan"),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  labs(color = "Results by Type\nof Search",
       caption = "Note: Black point and grey cell represents 1 square kilometer range\naround Northeastern University, the center of each 770 meter radius search\nper square kilometer.")
# Save!
ggsave(g1, filename = "viz/figure_A1.png", dpi = 500)

# Clear data
remove(mykey, mypoints, neu, g1, run_type, run_keyword, run_string, convert_points)
```

## 2.6 Searches

Based on this, we're going to deploy 19 search terms, saved in the ```sites``` vector.

```{r, eval = FALSE}
# Public Facilities
sites <- c("library",
           "community center",
           "place of worship",
           "city hall", 
           
           # Parks & Green Space 
           "park",
           "fountain",
           "square",
           
           # Commercial
           "bookstore",
           "cafe","coffeeshop",
           "beauty salon",
           "barbershop",
           
           # Recreation
           "sports field",
           "recreation center",
           "aquarium",
           "art gallery",
           "zoo",
           "museum", 
           "garden")
# 81*0.03*18 = ~ $40
#270 * 0.03 * 18

# Area
#sqrt(2000^2 / (pi))
#1125^2*pi
# 1125 is roughly the radius of a 2 square kilometer circle

# Let's import our grid cells in Boston
mypoints <- read_sf("shapes/fishnet_points.geojson") %>%
  filter(size == 2000) %>%
  as_tibble() %>%
  select(cell_id, lng, lat)

# Make a folder to hold results!
dir.create("query/actual")

# Gather each of these sites 1 per 2 square kilometers
for(i in 1:length(sites) ){

  print(sites[i])  
  
  # Get places of worship
  mypoints %>%
    split(.$cell_id) %>%
    # Run the query in a loop!
    map_dfr(~google_places(
      location = c(.$lat[1], .$lng[1]),
      keyword = sites[i],
      radius = 1125, # circumference of a circle that extends that far
      key = mykey)$result, .id = "grid") %>%
    # Save Query!
    saveRDS(paste("query/actual/", "search", sites[i], ".rds", sep = ""))
}

# Bind all searches together
data.frame(file = dir("query/actual", full.names = TRUE)) %>%
  split(.$file) %>%
  map_dfr(~read_rds(.$file), .id = "file") %>%
  saveRDS("query/sites.rds")

```

## 2.7 Process Points

Next, let's convert our search query results into nice `sf`-formatted points, which we can save as a `.geojson` file for easy access.

```{r}
# Let's identify which search each site appeared in MOST;
# these will be the final sites we attribute them to.
# Format
meta <- dget("meta.txt")

# Import fishnet grid
fish <- read_sf("shapes/fishnet.geojson") %>%
  filter(size == 1000) %>%
  st_transform(crs = meta$aea$proj)


mysites <- read_rds("query/sites.rds") %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
  st_transform(crs = meta$aea$proj) %>%
  # For each place id,
  # Consolidate any duplicates
  group_by(place_id) %>%
  summarize(
    file = table(file) %>% sort(decreasing = TRUE) %>% names() %>% .[1],
    name = unique(name),
    business_status = unique(business_status),
    rating = unique(rating),
    types = unique(types),
    user_ratings_total = unique(user_ratings_total),
    permanently_closed = unique(permanently_closed),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
           map(~paste(., collapse = "; ")) %>%
           unlist()) %>%
  mutate(file = file %>% str_remove("query/actual/search") %>% str_remove(".rds")) %>%
  mutate(group = file %>% recode_factor(
    "library" = "Community Space",
    "community center" = "Community Space",
    "city hall" = "Community Space", 
    
  
    
    "place of worship" = "Places of Worship",
    
    # Parks & Green Space 
    "park" = "Parks",
    "fountain" = "Parks",
    "square" = "Parks",
    "garden" = "Parks",
    
    # Commercial
    "bookstore" = "Social Businesses",
    "cafe" = "Social Businesses",
    "coffeeshop" = "Social Businesses",
    "beauty salon" = "Social Businesses",
    "barbershop" = "Social Businesses",
    
    # Recreation
    "sports field" = "Recreation & Education",
    "recreation center" = "Recreation & Education",
    "aquarium" = "Recreation & Education",
    "art gallery" = "Recreation & Education",
    "zoo" = "Recreation & Education",
    "museum" = "Recreation & Education")) %>%
  # Exclude any points that aren't directly in a cell
  # (sometimes Google returns a fews points nearby but not within the cell;
  # we're not interested in any points that are OUTSIDE the overall Boston grid.
  st_join(fish, left = FALSE) %>%
  # Switch back to wgs for portability
  st_transform(crs = meta$wgs$proj) %>%
  # Save to file
  st_write("query/sites_points.geojson", delete_dsn = TRUE)

# Clear data
rm(list = ls())
```

## 2.8 Diagnostics

### Figure Z3: Duplicates Barchart

```{r}
# Format
meta <- dget("meta.txt")
mysites <- read_rds("query/sites.rds") %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
    st_transform(crs = meta$aea$proj)
  
# Count and visualize how often duplicates or triplicates, etc. occur
viz <- mysites %>%
  as_tibble() %>%
  # Per place,
  # How many rows are there?
  group_by(place_id) %>%
  count() %>%
  ungroup() %>%
  # Arrange from highest to lowest 
  arrange(desc(n)) %>%
  # Per level (duplicates, triplicates, etc.)
  # How many times does this occur?
  group_by(n) %>%
  count() %>%
  # Plot it!
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site\n(1 = One time, 2 = Duplicate, 3 = Triplicate, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5))

ggsave(viz, filename = "figures/other/figure_Z3_duplicates_barchart.png", dpi = 500, width = 6, height = 3)

rm(list = ls())
```

### Figure Z4: Duplicates by Search

```{r}
# Format
meta <- dget("meta.txt")
mysites <- read_rds("query/sites.rds") %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = meta$wgs$proj) %>%
    st_transform(crs = meta$aea$proj)

# How often did different searches record the same places
viz <- mysites %>%
  tibble() %>%
  # Per file,
  # How many distinct places are the?
  group_by(file) %>%
  select(place_id) %>%
  distinct() %>%
  ungroup() %>%
  # Per place,
  # How many DISTINCT rows are there?
  group_by(place_id) %>%
  count() %>%
  # Sort
  arrange(desc(n)) %>%
  # Per level (duplicate/triplicate/etc.)
  # How often do they occur?
  group_by(n) %>%
  count() %>%
  # Visualize!
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site in Multiple Searches\n(1 = 1 Search, 2 = 2 Searches, 3 = 3 Searches, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5))

ggsave(viz, filename = "figures/other/figure_Z4_duplicates_by_search_barchart.png", dpi = 500, width = 6, height = 4)

rm(list = ls())
```

That's surprisingly good. Fewer duplicates than before, it seems.
This means that just 42 sites appeared across multiple categories.

### Figure A3: Map Rates by Grid

We're going to do a cool descriptive sensitivity test. First, we need to tally rates for the two separate grids. Does the size of collection grid used (1km vs 2km) affect the eventual rates, and if so, how much?

```{r, eval = FALSE}
# Get projections
meta <- dget("meta.txt")

# Let's run a test using our Places of Worship query results,
# which we used as a test-run with 1km collection grid and an actual-run on the 2 km collection grid

# gathered from a 1km2 collection grid
query1 <- read_rds("query/test/test_points.rds") %>%
    st_transform(crs = meta$aea$proj) %>%
    select(-any_of("cell_id"))


# gathered from a 2km2 collection grid
query2 <- read_sf("query/sites_points.geojson") %>%
  filter(group == "Places of Worship") %>%
  st_transform(crs = meta$aea$proj) %>%
  select(-any_of("cell_id"))


# Get 1 km2 tallying grid
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  st_transform(crs = meta$aea$proj)

# Write a quick function to tally up a set of points-per-grid cell
tallyup = function(data){
  data %>%
    # Count up total points per cell
    group_by(cell_id) %>%
    summarize(
      sites = sum(!is.na(place_id)),
      # Use interpolated population density
      across(.cols = c(pop_density_int, geometry), .fns = ~unique(.x))) %>%
    ungroup() %>%
    # Calculate rate of sites per 1000 persons in the grid cell
    mutate(rate = sites / pop_density_int * 1000) %>%
    return()
}

# Tally up points gathered from the 1-km^2 collection grid
tally1 <- grid %>%
  st_join(query1) %>%
  tallyup()

# Tally up points gathered from the 2-km^2 collection grid
tally2 <- grid %>%
  st_join(query2) %>%
  tallyup()



# Bind them together
bind_rows(tally1, tally2, .id = "size") %>%
  group_by(size) %>%
    # Log, but deal with zeros
  mutate(y = case_when(
    # Replace zeros with half the value of the smallest non-zero value
    rate == 0 ~ rate %>% unique() %>% sort(FALSE) %>% .[2] / 2,
    # otherwise, keep the same
    TRUE ~ rate),
    # Log transform
    ylogged = log(y)) %>%
  ungroup() %>%
  # Join in covariates 
  left_join(by = "cell_id",
            y = read_sf("shapes/grid_covariates.geojson") %>%
              as_tibble() %>%
              select(cell_id, type, milestone, neighborhood, 
                     pop_women:pop_age_65_plus,
                     social_capital:linking)) %>%
  st_write("shapes/rates_by_collection_grids.geojson", delete_dsn = TRUE)

rm(list = ls())
```

Now, make the corresponding visual for the grid.

```{r}
library(tidyverse)
library(sf)
library(ggtext)

tally <- read_sf("shapes/rates_by_collection_grids.geojson") %>%
  filter(!type %in% c("excluded_validation", "excluded_boston", "outside_of_boston"))

stats <- tally %>%
  as_tibble() %>%
  group_by(size) %>%
  summarize(mean = mean(rate, na.rm = TRUE) %>% round(2))

# Looks like there is a very minor, not especially significant difference between the two.
test <- tally %>%
  as_tibble() %>%
  with(t.test(ylogged ~ size)) %>%
  broom::tidy() %>% 
  mutate_at(vars(estimate1, estimate2), list(~exp(.)))


# Calculate their relative ranking
cor1 <- tally %>%
  as_tibble() %>%
  group_by(size) %>%
  arrange(rate) %>%
  mutate(rank = 1:n()) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(cell_id), names_from = size, values_from = rank) %>%
  summarize(cor = cor(`1`, `2`))

cor2 <- tally %>%
  as_tibble() %>%
  pivot_wider(id_cols = c(cell_id), names_from = size, values_from = rate) %>%
  summarize(cor = cor(`1`, `2`))


mylabels <- as_labeller(c(
  "1" = paste("<b>1 km<sup>2</sup> Collection Grid<br>1 km<sup>2</sup> Tally Grid</b><br>Mean Rate: ", stats$mean[1]),
  "2" = paste("<b>2 km<sup>2</sup> Collection Grid<br>1 km<sup>2</sup> Tally Grid</b><br>Mean Rate: ", stats$mean[2])))

# How much do results change when using a 1 km collection grid versus a 2 km tally grid?
g1 <- ggplot() +
   geom_sf(data = tally, mapping = aes(fill = rate, group = size),
          fill = NA, color = "#373737", size = 1) +
  geom_sf(data = tally, mapping = aes(fill = rate, group = size),
          color = "white") +
  facet_wrap(~size, labeller = mylabels) +
  theme_void(base_size = 14) +
  theme(
    plot.caption = element_markdown(face = "italic", size = 12, hjust = 0),
    strip.text.x = element_markdown(size = 12, hjust = 0.5, 
                                    colour = "#373737"),
    plot.margin = margin(0,0,0,0, "cm"),
legend.title = element_markdown(face = "italic", size = 13, hjust = 0)) +
  viridis::scale_fill_viridis(option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(fill = "<b>Rates of<br>Places<br>of Worship</b><br><br><i>per 1000<br>residents<br>per km<sup>2</sup></i>")


# Analyze the difference of means
ggsave(g1, filename = "figures/figure_A3.png", dpi = 300, width = 5, height = 3.5)

rm(list = ls())
```


### Figure A4: Correlations by Grid

Are there any major demographic differences? Do they correlate as expected with covariates?

```{r, eval = FALSE}

mycor <- read_sf("shapes/rates_by_collection_grids.geojson") %>%
  filter(type %in% c("validation", "boston")) %>%
  as_tibble() %>%
  pivot_longer(cols = c(
    pop_density_int, pop_women, pop_age_65_plus, 
    pop_black, pop_white, pop_asian, pop_hisplat,
    pop_some_college, median_income, 
    pop_unemployed, median_monthly_housing_cost, 
    social_capital:linking), names_to = "covariate", values_to = "x") %>%
  group_by(size, covariate) %>%
  summarize(cor = cor(y, x, use = "pairwise.complete.obs")) %>%
  ungroup() %>%
  # Relabel terms
  mutate(covariate = covariate %>% recode_factor(
    "pop_density_int" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost",
    "linking" = "Linking\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "bonding" = "Bonding\nSocial Capital",
    "social_capital" = "Overall\nSocial Capital"),
    size = size %>% recode_factor(
      "1" = "1 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid",
      "2" = "2 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid"))


g1 <- mycor %>%
  ggplot(mapping = aes(x = size, y = covariate, fill = cor, label = round(cor, 2))) +
  geom_tile(color = "#373737", size = 0.1) +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", high = "#648FFF", mid = "white", 
                       midpoint = 0, limits = c(-1, 1),
                       breaks = c(-1, -0.75, -0.5, -0.25, 0, .25, .5, .75, 1))  +
  theme_classic() +
  theme(
    plot.title = element_markdown(hjust = 0.5, size = 14),
    plot.subtitle = element_markdown(hjust = 0.5, size = 12),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_text(size = 10),
        axis.text.x = ggtext::element_markdown(size = 10, hjust = 0.5),
    legend.title = ggtext::element_markdown(size = 10, hjust= 0),
    legend.text = ggtext::element_markdown(size = 12, hjust= 0),
        panel.border = element_blank(),
        legend.position = "right") +
  guides(fill = guide_colorsteps(barheight = 15, barwidth = 2,
                                 tick.colour = "#373737", frame.colour = "#373737", 
                                 show.limits = TRUE)) +
  labs(x = NULL, y = NULL, fill = "<b>Correlation</b><br>(<i>Pearson's R</i>)",
       title = "Demographic Correlations Unchanged<br>by Grid Collection Size",
       subtitle = "Correlations between Logged Rates<br>of Places of Worship vs. Demographics",
       caption = "Note: Demographics represent averages of tract level covariates.")


ggsave(g1, filename = "figures/figure_A4.png", width = 6, height = 6.5, dpi = 300)
# Get a simple estimate of population density within a fishnet grid cells using the mean of the rate of tracts
# Import tracts

rm(list = ls())
```

# 3. Tally

Next, we uploaded our data to Google MyMaps, created a human coded map, and finally returned the data from that map to R, saving it in our ```validated``` folder.

```{r}
# Load projections
meta <- dget("meta.txt")


#Get MyMaps data
read_sf("query/sites_validated.geojson") %>%
  # Get Original Google Sites; 
  filter(!is.na(place_id)) %>%
  # Turns out, our point-grid joins are unaffected by projection given the scale we're at.
  # so fine so join in WGS projection
  st_join(read_sf("shapes/grid_covariates.geojson") %>% 
  select(cell_id, milestone_id, type, geometry)) %>%
  # to get study area, just filter to milestone_id %in% 1:3
  # Save
  st_write("shapes/mygoogle.geojson", delete_dsn = TRUE)
# thus, shapes/mygoogle.geojson can double as the old mygoogle.kml and mygoogle_boston.kml

read_sf("query/sites_points.geojson") %>%
  head()

read_sf("query/sites_validated.geojson")$group %>% table()
  
read_sf("query/sites_validated.geojson") 

read_sf("query/sites_validated.geojson") 

sum(s1$cell_id == s2$cell_id)

```


### Get Online-Checked Sites

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Get projections
meta <- dget("meta.txt")
# Import grid!
grid <- read_sf("shapes/grid_covariates.geojson") %>%
  select(cell_id, neighborhood, milestone) %>%
  # convert to equal area conic projection
  st_transform(crs = meta$aea$proj)

# Import grid cells sampled for ground truthing
blocks <- read_sf("shapes/grid_sampled.geojson") %>%
  filter(coders == "masters") %>%
  # Transform to Equal Area conic projection
  st_transform(crs = meta$aea$proj)
```


```{r}
read_sf("query/sites_validated.geojson") %>%
  group_by(status) %>%
  count()

read_sf("query/sites_validated.geojson") %>%
  st_join(read_sf("shapes/grid_covariates.geojson") %>% 
            select(cell_id, milestone_id, type, geometry)) %>%
  filter(milestone_id %in% 1:4) %>%
  as_tibble() %>%
  group_by(status) %>%
  count()

sites <- read_sf("query/sites_validated.geojson") %>%
  filter(is.na(status))

grid <- read_sf("shapes/grid_covariates.geojson")

boston <- read_sf("shapes/boston.geojson")

ggplot() +
  geom_sf(data = boston, fill = "white", color = "black") +
  geom_sf(data = grid, fill = NA, color = "lightgrey") +
  geom_sf(data = sites, color = "red")
```
```{r}
read_sf("query/sites_validated.geojson") %>% head()

# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds")  %>%
  mutate(source = "groundtruth_masters") %>%
  select(name = title, type = group, status = groundtruth, cell_id, source, geometry)  %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])  %>%
  as_tibble() %>%
  select(-geometry)


```

```{r}
  # st_write("query/sites_validated.geojson", delete_dsn = TRUE)
  mutate(
    status = case_when(
    # Was in original google results
    !is.na(place_id) & status == "checked" ~ "checked",
    !is.na(place_id) & status == "checked, new" ~ "checked",
    !is.na(place_id) & status == "checked, new, visited" ~ "checked, visited",
    !is.na(place_id) & status == "checked, visited" ~ "checked, visited",
    !is.na(place_id) & status == "visited" ~ "checked, visited",
    
    # Was not in original google results
    is.na(place_id) & status == "checked" ~ "new",
    is.na(place_id) & status == "checked, new" ~ "new",
    is.na(place_id) & status == "checked, new, visited" ~ "new, visited",
    is.na(place_id) & status == "new, visited" ~ "new, visited",
    is.na(place_id) & status == "visited" ~ "new, visited",
    
    TRUE ~ status),
  source = case_when(
    str_detect(source, "googleapi") ~ "google_api",
    TRUE ~ "humancoding_fall")) %>%
```

Next, let's get the final list of points that were checked.

```{r}
# Import online-checked points
mychecked <- read_sf("mysites_all_boston.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  # Filter out parking lots
  mutate(source = "googlemymaps") %>%
  select(name = Name, type = group, status, cell_id, 
         google_id, source, geometry)  %>%


#789 = have an id
mychecked %>%
  filter(!is.na(google_id)) %>%
  count()

# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds")  %>%
  mutate(source = "groundtruth_masters") %>%
  select(name = title, type = group, status = groundtruth, cell_id, source, geometry)  %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])  %>%
  as_tibble() %>%
  select(-geometry)

bind_rows(mygoogle, mychecked) %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  as_tibble() %>%
  select(-geometry) %>%
  # Join in ground-truthed points
  bind_rows(myground) %>%
  # Round to nearest 30 feet
  mutate(xish = round(x, 4), yish = round(y, 4)) %>%
  # Group by name and general 30-feet area; there's not going to be 
  # 2 caffe neros within the same 30 feet.
  group_by(name, xish, yish) %>%
  summarize(type = unique(type) %>% paste(collapse = ", "),
            cell_id = unique(cell_id) %>% paste(collapse = ", "),
            google_id = unique(google_id) %>% paste(collapse = ", "),
            status = unique(status) %>% paste(collapse = ", "),
            search = unique(search) %>% paste(collapse = ", "),
            source = unique(source) %>% sort() %>% paste(collapse = ", "),
            x = unique(x) %>% paste(collapse = ", "),
            y  = unique(y) %>% paste(collapse = ", ")) %>%
  ungroup() %>%
  mutate(
    google_id = str_remove(google_id, ", NA") %>% na_if("NA") %>% na_if(""),
    search = str_remove(search, ", NA") %>% na_if("NA") %>% na_if(""),
    type = case_when(
      name == "Allan Crite Community Garden" ~ "Parks",
      name == "Fenway Park" ~ "Parks",
      name == "Garden of Peace" ~ "Parks",
      name == "Iglesia Comunitaria Casa De Gloria" ~ "Places of Worship",
      name == "Islamic Society of Boston Cultural Center" ~ "Places of Worship",
      name == "St Leonard-Port Maurice Parish" ~ "Places of Worship",
      TRUE ~ type),
    status = case_when(
      # Was in original google results
      !is.na(google_id) & status == "checked" ~ "checked",
      !is.na(google_id) & status == "checked, new" ~ "checked",
      !is.na(google_id) & status == "checked, new, visited" ~ "checked, visited",
      !is.na(google_id) & status == "checked, visited" ~ "checked, visited",
      !is.na(google_id) & status == "visited" ~ "checked, visited",
      
      # Was not in original google results
      is.na(google_id) & status == "checked" ~ "new",
      is.na(google_id) & status == "checked, new" ~ "new",
      is.na(google_id) & status == "checked, new, visited" ~ "new, visited",
      is.na(google_id) & status == "new, visited" ~ "new, visited",
      is.na(google_id) & status == "visited" ~ "new, visited",
      
      TRUE ~ status),
    source = case_when(
      str_detect(source, "googleapi") ~ "google_api",
      TRUE ~ "humancoding_fall")) %>%
  ungroup() %>%
  select(-xish, -yish) %>%
  # Finally, filters just to points within cells
  filter(cell_id != "NA") %>%
  # A couple of places accidentally got a bunch of coordinates of different lengths, even though they show the same place. Just pick the first.  
    mutate_at(vars(x, y), list(~word(., 1) %>% str_remove(pattern = ",") %>% str_trim() %>% as.numeric())) %>%
  # if it has a google_id, it's not new - that means it was retuned in the original query
  mutate(status = if_else(!is.na(google_id) & status == "NA, new", 
                          true = "checked", false = status)) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)  %>%
  # We're going to cut any places located outside of Boston
  st_join(read_rds("neighborhoods.rds") %>% 
            st_transform(crs = 4326) %>% 
            select(neighborhood = name)) %>%
  filter(!is.na(neighborhood)) %>%
  mutate(
    status = if_else(
    condition = name %in% c(
      "African Studies Library",
      "Baker Square Condominiums",
      "Boston City",
      "Lofts At Lower Mills and Baker Square Condominums"), 
    true = "not social infrastructure", false = status),
    
    status = if_else(
      condition = name %in% c(
        "Archdale Community Center", 
        "Dorchester Park",
        "Lower Mills Branch of the Boston Public Library",
        "Flat Black Coffee",
        "Wesley United Methodist Church",
        "Ventura Park",
        "Sweet Life",
        "William G. Walsh Playground",
        "Top of the Line Barbershop"),
      true = "checked", false = status),
    
    status = if_else(
      google_id %in% c(
        "ChIJ3Wc0GUp744kRPWwNW7JpKSU",	
        "ChIJqb2b43N844kR8prF3geo2os"),
      true = "checked", false = status)) %>%
  mutate(x = st_coordinates(.$geometry)[,1],
         y = st_coordinates(.$geometry)[,2]) %>%
  as_tibble() %>%
  mutate(id = 1:n()) %>%
  write_csv("export/boston_sites_2022_03_03.csv")
```

```{r}
# Use boston neighborhood boundaries to exclude places on the other side of the River Charles

read_csv("export/boston_sites_2022_03_03.csv") %>%
  filter(!status %in% c("duplicated", "not found", "not social infrastructure")) %>%
  write_csv("export/boston_social_infra_2022_03_03.csv")
```


## Next

```{r}  
  
  # Filter to just places that are 'checked', 'new sites', or 'been there'
  filter(status %in% c("checked", "new sites", "been there")) %>%
  # add unique id for each point
  mutate(id = 1:n()) %>%
  # condense number of columsn
  select(Name, description, id, group, status, google_id = place_id, geometry) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Filter points to just those located within milestones 1, 2, and 3.
  #filter(str_detect(milestone, pattern = "M1|M2|M3|M4")) %>%
  select(Name, description, id, group, status, google_id, 
         cell_id, neighborhood, milestone, geometry) %>%
  mutate(milestone = na_if(milestone, "")) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other")) %>%
  write_sf("mysites_all_boston.kml", driver = "kml", delete_layer = TRUE)


# Filter points to just those located within milestones 1, 2, and 3.
read_sf("mysites_all_boston.kml") %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  write_sf("mysites_core_boston.kml", driver = "kml", delete_layer = TRUE)


#read_sf("validated/sites.kml")$status %>% unique() %>% sort()
#read_sf("sites.kml")$status %>% unique() %>% sort()
```




# 4. Validation




## 6.1 Import our data from Google MyMaps

```{r}
# Get sites
sites <- read_sf("validated/sites.kml")

# Get fishnet grid
grid <- read_sf("validated/milestones.kmz")

# Get neighborhood polygons
neighbor <- read_sf("validated/neighborhoods.kmz")


aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

```


We're going to select our study area for validation.

```{r}
# Import our grid cells 
fish <- read_sf("grid_covariates_tracts.kml") %>%
  select(-Name, -c(timestamp:icon)) %>%
  st_transform(crs = aea) %>%
  # Filter to just the inner-neighborhoods of Boston, which are most comparable
  filter(str_detect(milestone, "M1|M2|M3") ) %>%
  # remove cells which should be ineligible, as in 
  # they don't contain entire cells because they are coastal peninsulas
  filter(!cell_id %in% 
    paste("Cell", c(81,72,62, 160, 182, 176, 166, 165, 149,
                    89, 100, 80, 50, 62, 166, 89, 112, 129))) %>%
  # split up each variable into thirds
  mutate(pop_density_cat = ntile(pop_density, 3),
         median_income_cat = ntile(median_income, 3),
         pop_white_cat = ntile(pop_white, 3),
         pop_some_college_cat = ntile(pop_some_college, 3)) %>%
  # Grouping by neighborhood and each demographic category,
  # get the distribution of residents in the city
  group_by(neighborhood, pop_density_cat, 
           median_income_cat, pop_white_cat, 
           pop_some_college_cat) %>%
  mutate(count = n()) %>%
  ungroup() %>%
  mutate(prop = count / sum(count, na.rm = TRUE))

# Visualize these cells
ggplot() +
  geom_sf(data = fish, mapping = aes(fill = neighborhood))
```

## 6.2 Sampling Blocks

We're going to randomly sample grid cells in blocks...

```{r, eval = FALSE}

fish %>%
  as.data.frame() %>%
  select(-geometry) %>%
  infer::rep_slice_sample(n = 20, reps = 1000, replace = FALSE, weight_by = .$prop) %>%
  saveRDS("validated/grid_reps.rds")

# Please give me 20 samples, sampled based on their frequency in terms of neighborhood breakdown, population density, median income, white/non-white population, and education.

# We already did one cell - cell 124, Northeastern, 
# so it's important that our final sample include it.
# Let's identify the possible samples from our range of 1000 that include cell 124
valid <- read_rds("validated/grid_reps.rds") %>%
  filter(Name == "Cell 124") %>%
  select(replicate) %>%
  distinct()
# 454 of them did. Great. Let's investigate which of them are representative of the population.


# Get summary statistics for population
sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))


mysamples <- read_rds("validated/grid_reps.rds") %>%
  # Filter to just those which contained Northeastern.
  filter(replicate %in% valid$replicate) %>%
  # Pivot all their traits into a long vector
  select(replicate, Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 


get_test = function(var){
  print(var)
  
  mysamples %>%
    # Create a joined label
    filter(variable == var) %>%
    split(.$replicate) %>%
    map_dfr(~t_test(., response = value, mu = sumstats[, var]), .id = "replicate") %>%
    return()
}

data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", "pop_natam", "pop_asian", 
               "pop_pacific", "pop_hisplat", "pop_some_college",
               "employees_muni", "median_income",
               "income_inequality", "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  saveRDS("validated/grid_reps_t_test.rds")
```

## 6.3 Validation Area Similarity

```{r, eval = FALSE}
# Find me a sample that passed its t-test for every covariate
read_rds("validated/grid_reps_t_test.rds") %>% 
  select(replicate, variable, p_value) %>%
  pivot_wider(id_cols = replicate,
              names_from = variable,
              values_from = p_value) %>%
  filter(
    pop_density > 0.35,
    median_income > 0.35,
    pop_some_college > 0.35,
    pop_white > 0.35,
    
    pop_women > 0.35, 
    pop_age_65_plus > 0.35,
    pop_pacific > 0.35,
    pop_natam > 0.35,
    pop_hisplat > 0.35,
    pop_black > 0.35,
    pop_asian > 0.35,
    pop_unemployed > 0.35,
    income_inequality > 0.35,
    median_monthly_housing_cost > 0.35,
    employees_muni > 0.35) %>%
  saveRDS("validated/mybestsample.rds")


read_rds("validated/grid_reps.rds") %>%
  filter(replicate %in% read_rds("validated/mybestsample.rds")$replicate) %>%
  select(replicate, Name) %>%
  left_join(by = "Name", y = fish) %>%
  saveRDS("validated/sample_possibilities.rds")

test <- bind_rows(
  # Make sure it includes either Beacon Hill or Back Bay
  read_rds("validated/sample_possibilities.rds") %>%
    filter(Name == "Cell 173" | Name == "Cell 161") %>%
    select(replicate)) %>%
  distinct() %>% unlist() %>%
  sample(size = 9)

pos <- read_rds("validated/sample_possibilities.rds") %>%
  filter(replicate %in% test) %>%
  st_as_sf()
#95, 802,396, 578


g2 <- ggplot() +
  geom_sf(data = fish, fill = "darkgrey", color = "white") +
  geom_sf(data = pos, 
          mapping = aes(fill = neighborhood, 
                                     linetype = "Sampled", color = "Sampled"), size = 1.15)  +
  geom_sf_text(data = pos,
          mapping = aes(label = Name %>% str_remove("Cell ")), color = "black", size = 3) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5)) +
  scale_fill_viridis(option = "plasma", discrete = TRUE) +
  scale_color_manual(values = "black") +

  guides(color = "none") +
  labs(linetype = "Ground-truthed by:",
       fill = "Neighborhood",
       caption = "Numbers indicate unique ID numbers of each grid cell.",
       subtitle = "1 sq.km. Blocks of Core Boston (n = 73 / 168)") +
  facet_wrap(~replicate) 
  
ggsave(g2, filename = "validated/best_options.png", dpi = 500, 
       width = 9, height = 8)


# 366
# Filter to the final sample
read_rds("validated/grid_reps.rds") %>%
  filter(replicate == 366) %>%
  saveRDS("validated/sample20.rds")

remove(mysamples, mystats, valid)
```

## 6.4 Divide Masters & Undergrad Samples

Next, we collected a random stratified sample of 20 grid cells, and conducted ground truthing in each grid cell.

To do so, we split up the population of grid cells geographically by X neighborhoods, and then in terms of four additional key demographic traits, including population density, median income, the white/non-white population, and % residents with some college education. For each variable, we binned grid cells into 3 equally sized quantiles. Then, we calculated the proportion of grid cells which fit into each strata. (An example strata might be Jamaica Plains, upper 3rd share in terms of population density, lower 3rd share of median income, upper 3rd share of non-white residents, middle 3rd share of college educated residents.) Finally, we used these proportions as sampling probabilities, and took 1000 different samples of 20 grid cells using these probabilities, to ensure that in each of these 1000 different possible samples, our samples would reflect the frequency of these categories in the population at large.

The final sample of 20 grid cells was representative of the core Boston area in terms of 15 key demographic traits (one sample t-tests found p > 0.50 for each, which means they were extremely close to the population mean). When we zoomed out to compare against all neighborhoods of Boston, they were representative in terms of 14 key demographic traits (just education was slightly different, p < 0.05, but not by much). When we zoomed out to compare against every single cell, even including islands, they were representative in terms of 13 key demographic traits (just education and income inequality were slightly different, p < 0.05, but not by much).

Finally, I randomly assigned 10 of these grid cells to masters students and 10 grid cells to undergraduates.

```{r, eval = FALSE}
mine <- read_rds("validated/grid_covariates.rds") %>%
  mutate(sampled = if_else(Name %in% read_rds("validated/sample20.rds")$Name,
                           true = "yes", false = "no")) %>%
  group_by(sampled) %>%
  mutate(type = sample(x = c("masters", "undergrad"), size = n(), 
                       prob = c(0.5, 0.5), replace = TRUE),
         type = if_else(sampled == "no", true = NA_character_, false = type))

mine %>%
  filter(sampled == "yes") %>%
  select(Name, neighborhood, type) %>%
  group_by(type) %>% count()

mine %>%
  saveRDS("validated/grid_final.rds")

remove(mine)
```



# 4. Tallying


Repeat, but this time get the sites for the entire Boston area.

```{r}
grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(description, timestamp:icon)) %>%
    # convert to equal area conic projection
  st_transform(crs = aea) %>%
  select(-Name) %>%
  # Zoom into inner boston
  filter(str_detect(milestone, "M1|M2|M3|M4"))


read_sf("validated/sites.kml") %>% 
  # Zoom into just our Google search results
  mutate(place_id = place_id %>% na_if("")) %>%
  filter(!is.na(place_id)) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Zoom into just sites within the grid
  filter(str_detect(milestone, "M1|M2|M3|M4")) %>%
  select(-c(timestamp:icon)) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "Community Space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other",
    "Not social infrastructure" = "Other")) %>%
  
  write_sf("mygoogle_boston.kml", driver = "kml", delete_layer = TRUE)
```



## 3.3 Get Ground-Truthed Points, from Masters Students

```{r, message = FALSE, warning = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


###OLD 
# Import ground-truthed sites from masters students
#read_csv("groundtruth/map_marker_export_2021_10_10-12h47m15.csv") %>%
#  select(lat = Latitude, lon = Longitude,
#         title = Title, 
#         groundtruth = "MultiChoiceSelection: groundtruth/visited/not yet visited",
#         group = "MultiChoiceSelection: group/Not yet classified/Community Spaces/Places of Worship/Social Businesses/Parks/Other",
#         notes = "FreeText: Notes") %>%
#  mutate(group = case_when(
#    title %in% c("Curry Center Square","McConnell Park","Highland Avenue Community Garden",
#                 "Whitey McGra Memorial") ~ "Parks",
#    title %in% c("Science and Engineering Library", "Fineman & Pappas Law Libraries",
#    "Frederick S. Pardee Management Library","Michael D. Papagiannis Astronomy Library",
#    "Pickering Educational Resources Library", "Mugar Memorial Library",
#    "Fenway Library Organization, Inc.", "School of Theology Library",
#    "Boston University Science Library","Stone Science Library") ~ "Not social infrastructure",
#    title == "Freedom Christ-Ministry" ~ "Places of Worship",
#    title == "Starbucks?" ~ "Social Businesses",
#    TRUE ~ group)) %>%
  # Remove duplicated locations
#  filter(!notes %in% c("Double")) %>%
  # Remove any sites that are not social infrastructure, were not found, or were not completed
#  filter(!notes %in% c("Not social infrastructure", "Not found",
#                       "Not found/ not a park anymore",
#                       "not found", "Not found\n", "Under construction") ) %>%
#  filter(!group %in% c("Not social infrastructure")) %>%
#  # Remove any that weren't checked off either. These were not found.
#  mutate(groundtruth = groundtruth %>% na_if("")) %>%
#  filter(!is.na(groundtruth)) %>%
#  
#  filter(groundtruth == "visited") %>%
#  select(-notes) %>%
#  
#  # Get just distinct rows and sites
#  distinct() %>%
#  # Convert to sf format
#  mutate(geometry = paste("POINT(", lon, " ", lat, ")", sep = "")) %>%
#  st_as_sf(crs = 4326, wkt = "geometry") %>%
#  st_transform(crs = aea) %>%
#  select(-lat, -lon) %>%
#  # Filter out any points outside our blocks
#  st_join(blocks %>% select(cell_id = Name, geometry)) %>%
#  filter(!is.na(cell_id)) %>%
#  write_sf("mygroundtruthed.kml", driver = "kml", delete_layer = TRUE)



# Import grid cells sampled for ground truthing
blocks <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  select(Name, description, type, sampled, 
         neighborhood:pop_age_65_plus, geometry) %>%
  # Transform to Equal Area conic projection
  st_transform(crs = aea)


# Import ground-truthed sites from masters students
read_csv("groundtruth/map_marker_export_2021_10_10-12h47m15.csv") %>%
  select(lat = Latitude, lon = Longitude,
         title = Title, 
         fieldworkers = "FreeText: fieldworkers",
         description = "Description",
         color = "Color",
         groundtruth = "MultiChoiceSelection: groundtruth/visited/not yet visited",
         group = "MultiChoiceSelection: group/Not yet classified/Community Spaces/Places of Worship/Social Businesses/Parks/Other",
         notes = "FreeText: Notes") %>%
  mutate(group = case_when(
    title %in% c("Curry Center Square","McConnell Park","Highland Avenue Community Garden",
                 "Whitey McGra Memorial") ~ "Parks",
    title %in% c("Science and Engineering Library", 
                 "Fineman & Pappas Law Libraries",
    "Frederick S. Pardee Management Library",
    "Michael D. Papagiannis Astronomy Library",
    "Pickering Educational Resources Library",
    "Mugar Memorial Library",
    "Fenway Library Organization, Inc.",
    "School of Theology Library",
    "Boston University Science Library",
    "Stone Science Library") ~ "Not social infrastructure",
    title == "Freedom Christ-Ministry" ~ "Places of Worship",
    title == "Starbucks?" ~ "Social Businesses",
    TRUE ~ group)) %>%
  # Remove duplicated locations
      
  # indicate any that weren't checked off either. These were not found.
  mutate(groundtruth = groundtruth %>% na_if("")) %>%
  mutate(exclude = case_when(
    notes == "Double" ~ "yes",
    # Remove any sites that are not social infrastructure, were not found, or were not completed
    notes %in% c("Not social infrastructure", 
                 "Not found",
                 "Not found/ not a park anymore",
                 "not found", 
                 "Not found\n", 
                 "Under construction") ~ "yes",
    group %in% c("Not social infrastructure") ~ "yes",
    is.na(groundtruth) ~ "yes",
    groundtruth != "visited" ~ "yes",
    TRUE ~ "no")) %>%
  #filter(!is.na(groundtruth)) %>%
  #filter(groundtruth == "visited") %>%
  #select(-notes) %>%
  # Get just distinct rows and sites
  distinct() %>%
  rowwise() %>%
  mutate(fieldworkers = paste(unique(c(fieldworkers, description)), 
                              collapse = ", ") %>% 
           str_remove("[,] NA|NA[,] ")) %>%
  ungroup() %>%
  # Filter out any points outside our blocks
  # Convert to sf format
  mutate(geometry = paste("POINT(", lon, " ", lat, ")", sep = "")) %>%
  st_as_sf(crs = 4326, wkt = "geometry") %>%
  st_transform(crs = aea) %>%
  st_join(blocks %>% select(cell_id = Name, type, geometry)) %>%
  # If outside of block, then exclude, otherwise, keep regular value
  mutate(exclude = if_else(is.na(cell_id), "yes", exclude))  %>%
  as_tibble() %>%
  write_csv("export/ground_truthed_points_processed.csv")

library(tidyverse)
read_csv("groundtruthedpoints_editing_final.csv") %>%
  filter(exclude == "no")  %>%
  select(-exclude) %>%
  # Fix a few where people forgot to label theme
  mutate(group = case_when(
    title == "Highland Park Garden" ~ "Parks",
    title == "Ell Hall Walkway" ~ "Parks",
    title == "Symphony Road Community Garden" ~ "Parks",
    title == "Don Negro Barbershop" ~ "Social Businesses",
    title == "Malibu Beach Boardwalk" ~ "Parks",
    TRUE ~ group)) %>%
  #filter(title == "Malibu Beach Boardwalk") %>%
  st_as_sf(coords = c("lon", "lat"), dim = "XY", crs = wgs) %>%
  saveRDS("mygroundtruthed.rds")
```




# 5. Statistical Analysis

```{r}

grid <- read_rds("tally.rds") %>%
  select(cell_id, group, sites = googled, pop_density = pop_density_int,
         pop_women:pop_age_65_plus) %>%
  mutate(sites = sites / pop_density * 1000) %>%
  left_join(by = "cell_id", y = myvotes) %>%
  filter(pop_density > 0)  


mydat <- grid %>%
  as.data.frame() %>%
  select(-geometry) %>%
   # Calculate sites per capita
  mutate_at(vars(sites),
            funs(. / pop_density * 1000)) %>%
  #select(cell_id, group, year, social_capital:linking, 
  #       googled:ground, pop_density) %>%
  filter(!is.na(sites)) %>%
  filter(group != "Other") %>%
  # Impute small but reasonable value for zero cases,
  # so we can log-transform them
   ungroup() %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.)))
```

### Correlation

```{r}
mydat %>% 
  filter(type != "(%) Voted Republican") %>%
  group_by(type, group) %>%
  summarize(cor = cor(percent, sites)) %>%
  ggplot(mapping = aes(x = group, y = cor)) +
  geom_col() +
  facet_grid(cols = vars(type))
```

### Model

```{r}
mytheme <- theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0))
```
```{r}
myvif <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
            car::vif() %>%
            max() %>%
            data.frame(vif = .),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type"))
# myvif
# Pretty solid! ~4.5

mysum <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate_at(vars(percent, sites, income_inequality,
                 median_income, pop_nonwhite, pop_some_college,
                 pop_density), funs(scale(.) %>% as.numeric())) %>%
  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
              moderndive::get_regression_summaries(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type"))



mystats <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
    mutate_at(vars(percent, sites, income_inequality,
                 median_income, pop_nonwhite, pop_some_college,
                 pop_density), funs(scale(.) %>% as.numeric())) %>%

  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
              moderndive::get_regression_table(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type")) %>%
    filter(term == "sites") %>%
  filter(type != "(%) Voted Republican") %>%
  mutate(label = paste(
    round(estimate, 4) %>% format(scientific = FALSE),
    gtools::stars.pval(p_value), sep = ""))

g1 <- mystats %>% 
  mutate(group = factor(group, levels = c(
    "Total", "Community\nSpaces", "Places of\nWorship", 
    "Social\nBusinesses", "Parks") %>% rev())) %>%
  ggplot(mapping = aes(x = group, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci,
                       fill = if_else(estimate > 0, "Positive", "Negative"),
                       label = label)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_col(width = 0.5) +
  geom_linerange() +
  geom_point() +
  geom_text(nudge_x = 0.4) +
  facet_grid(cols = vars(type),
             scales = "free") +
  coord_flip() +
  scale_fill_manual(values = c("#DC267F", "#648FFF")) +
  theme_bw(base_size = 14) +
  #scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  mytheme +
  theme(panel.spacing.x = unit(0.5, "cm"),
        plot.subtitle = ggtext::element_markdown(size = 15),
        panel.border = element_rect(color = "grey", fill = NA)) +
  labs(
    subtitle = "Association between Social Infrastructure and Voting",
    y = "Standardized Effect of Site Rates (Z-score) on Voting Outcomes (Z-score)\n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Direction of Effect (Beta)",
    x = "Type of Social Infrastructure")

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}

combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("#0D0887", 
                            "#D35171")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_C2.png", dpi = 500, width = 12, height = 5)


combo2 <- g1 + 
labs(caption = "<b>Models: </b>Generated from OLS Models, which predict Percentage Voted (or Voted Democrat) using
    the Rate of Social Infrastructure (shown above), Median Income,<br>% Some College, 
    Income Inequality (Gini Coefficient), % Non-White Population, and Population Density.<br>All models explained between 43-86% of variation in outcome. 
    <b>Colinearity</b>: VIF below 10 for Google Sites (n = 71, Max = 4.5)
    <br>
    <b>Standardized Effects</b>: All numeric variables were rescaled into Z-scores so that beta coefficients reflect the projected increase in Voting Outcomes in standard deviations<br>as the rate of Social Infrastructure increases by 1 standard deviation.
    <b>Statistical Significance</b>: *** p < 0.001; ** p < 0.01; * p < 0.05; . p < 0.10.") %>%
  scale_panel_fill(side = "t", 
                   fill = c("#0D0887", 
                            "#D35171")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo2, filename = "covariates/beta_vote.png", dpi = 500, width = 12, height = 6)
```




# Z. Working Notes

## Education

```{r}
d <- read_csv("data/covariates/tracts_data.csv") %>%
  select(pop_some_college, pop, contains("edu")) %>%
  mutate_at(vars(contains("edu")), list(~. / pop)) %>%
  summarize_at(vars(contains("edu")), 
               list(~cor(pop_some_college, ., use = "pairwise.complete.obs")))
# Looks like I should just double check later analyses to make sure nothing changes.

# Clear data
rm(list = ls())
```

## Size of Neighborhood


Before we estimate the amount of social infrastructure in each neighborhood in Boston, we need to conduct several preparatory exercises. How much would this approach cost if we extend it to the entire United States? We need to estimate this to see if this is even feasible before we run the Boston pilot study.

To know how much it will cost to estimate social infrastructure in every community, we need to know three main things:

- ```area```: total area of populated, applicable communities in the US

- ```size```: Size of a Community

- ```n```: Number of those Communities in the US

- ```cost```: Cost of mapping a single community

Such that:

$$ n_{communities} = area_{total} / size_{1-community} $$

and therefore:

$$ cost_{total} =  cost_{per-community} \times n_{communities} $$


Recently, several communities have tried to estimate the average size of a community in the US, based on resident perceptions. The results have been varied. [Kwame	Donaldson at the US Census Bureau](https://www.census.gov/content/dam/Census/programs-surveys/ahs/working-papers/how_big_is_your_neighborhood.pdf
) relied on census questions, such as those which ask residents whether there is a beach in their community, and then estimated the distance from the resident's home to the nearest beach. His Census Bureau working paper estimated the average size of a community, as perceived by respondents, at a radisu of between ***520 and 1060 meters.*** This number varies somewhat by region.

```{r}
# Borrowing from Donaldson's report on page 33:
community <- tribble(
  ~place,       ~estimate,   ~lower,  ~upper,
  "Northeast",  6.0771,      4.3404,  7.8137,
  "Midwest",    8.6661,      5.4781,  8.6661,
  "South",      4.9010,      1.5795,  8.2225,
  "West",       6.8835,      4.9697,  8.7973
)
```

Let's view his tallies below. These numbers reflect **derived community area**, which is the value of the exponent with a base of 10. For example, $6.4862 = 10^{6.4862} = 3,063,374$ meters.

```{r}
community
```

That is obviously not easily interpretable, so let's adjust these to the correct units. So let's exponentiate it, but divide by 1 million square meters to translate to square kilometers.

Well, their estimates for the Midwest seem extreme, but the other ones look plausible. Accounting for these regional differences, it makes sense to conclude, as they did, that the average neighborhood is between 520 and 1060 meters. 1 square kilometer makes for a good estimate, given the wide range in the Northeast (1.19), South (0.08), and West (7.65).

```{r}
community %>%
  mutate_at(vars(estimate, lower, upper),
            funs((10^. *1e-6)))
```

## Cost of US

First, we're going to get some background data to assist us, like the polygons of all states and counties in the US.

```{r}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
```

Let's download the polygons for all US tracts.

```{r, eval =  FALSE}
data.frame(state = state.abb) %>%
  # For each state,
  split(.$state) %>%
  # Get the tracts
  map(~tigris::tracts(state = .$state, cb = TRUE, year = 2019) %>%
        st_as_sf() %>%
        # Set all column names to lowercase
        magrittr::set_colnames(value = names(.) %>% tolower()) %>%
        # Set a basic WGS projection, just in case
        st_transform(crs = wgs), .id = "state") %>%
  dplyr::bind_rows() %>%
  # Keep only a subset of variables
  select(geoid, area_land = aland, geometry) %>%
  saveRDS("tracts_us.rds")
```

Next, let's eliminate [areas outside the continental US](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/?cid=nrcs143_013696) (mostly because Alaska, Hawaii, Guam, and other such states and territories have very different geographic traits). We will save this as ```tracts_us.rds```.

```{r, eval = FALSE}
read_rds("tracts_us.rds") %>%
  filter(as.numeric(str_sub(geoid, 1,2)) %in% 1:56) %>%
  # Transform to equal area conic
  st_transform(crs = aea) %>%
  # Join in their population
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv") %>% 
              mutate(geoid = str_pad(geoid, width = 11, side = "left", pad = "0")) %>%
  select(geoid, pop)) %>%
  # Filter to populated tracts
  filter(pop > 0) %>%
  # Get population density in 1000s per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) %>%
  saveRDS("tracts_pop.rds")
```

```{r, eval = FALSE}
# Now let's calculate the total area of populated census tracts in the US
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  summarize(area = sum(area_land, na.rm = TRUE) / 1000000)
```

The total area of populated census tracts in the continental US is 9,114,979 square kilometers.

In fact, using the histogram below, we can explore the distribution of area in populated census tracts. It looks like a small percentage of census tracts (~5%) have an area near or at 1,000 square kilometers. That's enormous! We can hardly call a census tract like that, likely found in communities in Montana, comparable to census tracts in Boston or other urban census tracts. Best to exclude these from the current tally, since we will need a different system to measure social infrastructure for such different, sparsely populated communities.

```{r, eval = FALSE}
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  ggplot(mapping = aes(x = area_land / 1000000)) +
  geom_histogram() +
  scale_x_log10()
```
But at what point should we exclude such communities? We examined the distribution of census tract land area at the 50th, 75th, 90th, 95th, 97.5th and 99th percentiles below.

```{r, eval = FALSE}
# Get percentiles
quantile(read_rds("tracts_pop.rds")$area_land / 1000000, 
         probs = c(0, 0.50, .75, .90, .95, .975, .99), na.rm = TRUE)
```

If your census tract is above the 95th percentile in area, you have at least 422 square kilometers of land area. Which is twice the size of ALL of Boston. That likely means most of this land is not inhabited. To map every square kilometer would use a tremendous amount of money, but capture very few communities. So let's shave off all communities like this - the top 5% largest census tracts in terms of area.

By excluding those with an area above 214.15 square kilometers (the 95th percentile), let's calculate the total area of 'populated' (read: not extremely sparesely populated) census tracts in the US.

```{r, eval = FALSE}
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  # Zooming into just the bottom 95% 
  filter((area_land / 1000000) < 214.157569) %>%
  summarize(area = sum(area_land, na.rm = TRUE) / 1000000)
```

That leaves a total of 1,389,431 square kilometers of land area in the US to examine. Given the 10 types of social infrastructure we identify below as recordable through the Google Maps API, how much would it cost to map these?

At the 1 square kilometer level:

$ 1,389,431_{km^{2}} * 10_{types-of-social-infrastructure} \times 0.03_{USD-per-km^{2}} = 41,829_{USD}$

That is a lot, but doable!

To do just Boston would cost:

$ 125.4_{km^{2}} \times 10_{types-of-social-infrastructure} \times 0.03_{USD-per-km^{2}} = 37.62_{USD}$

This is super doable. We just need to start small with individual cities to ensure our approach is valid.

How many queries will we need to run?

Well, for Boston, we should get approximately 125.4 grid cells, each 1 square kilometer, plus or minus a few cells. Using 1000 square meter squares, we get 1030 squares. Using 1000 square meter hexagons, we get 1174 squares. It seems more efficient to use cells for now, rather than hexagons.

In order to fully encapsulate a 1000 square meter square, a circular radius needs to be greater than 1000 meters. We can use the Pythagorean theorem to get the correct size raidus for a circle that perfectly captures that square. The radius should be 1414.214 meters.

```{r{}
sqrt(1000^2 + 1000^2)
```

We can find out just how extreme it will be to query them all right now.

For example, let's say we need to query every one of them, and we cap each search at 20 results. Even given a conservative estimate, we would need to make at least 10-15 searches of each grid cell.

Here's our starter list of possible social infrastructure sites, based on the literature. We use a more refined list below.

1. Parks
2. squares
3. fountains
4. sports grounds
5. public schools
7. community centers
8. libraries
9. museums
10. art galleries
11. zoos
12. botanical gardnes
13. corner stores
14. barbershops
15. places of worship


## Sample Code for Multi-Round Queries


```{r, eval = FALSE}
# For example, this is how you would run MULTI-part queries

# Get all data
super_query = function(mysearch){
  # Run initial query
  # To get first 20 results
  myquery <- google_places(
    search_string = mysearch,
    key = mykey)
  
  # Supply token from first query
  # to ask for next 20 results
  myquery2 <- google_places(
    search_string = mysearch,
    page_token = myquery$next_page_token,
    key = mykey)

  # Supply token from second query
  # to ask for final 20 results
  myquery3 <- google_places(
    search_string = mysearch,
    page_token = myquery2$next_page_token,
    key = mykey)
  
  # Bind results as a list and return
  list(myquery, myquery2, myquery3) %>%
    return()
}
  
# Test it out
result <- super_query(mysearch = "Parks in Boston, MA")

result

# Extract latitude and longitude
data.frame(
  lng = myquery$results$geometry$location$lng,
  lat = myquery$resultsgeometry$location$lat,
)

# Investigate results
myquery$results

# Map them all together
myquery$results$types %>%
  map(~paste(., collapse = "; ")) %>% 
  unlist()

# In summary, it's a lot of work.
# We focused on the first 20 results,
# since extra results means extra expenses,
# and the potential for expenses due to failed queries with no reward.
```



