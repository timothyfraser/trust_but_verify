---
title: "Mapping Social Infrastructure in Boston"
author: "Timothy Fraser"
date: "November 10, 2021"
output:
  html_document:
    df_print: paged
subtitle: "Replication Code for Data Output"
---


# Background

This document summarizes the creation and formatting of data for the project so far. Please see the codebook below, which details the different files available:

<br>

## Main data

- ```"mygoogle.kml"```: sample of original Google Maps Places API sites which lie within the study area of Milestones 1-3. Points have been classified into five groups, including (1) *community spaces*, (2) *places of worship*, (3) *social businesses*, (4) *parks*, or (5) *other*.

- ```"mysites_core_boston.kml"```: refined sample of social infrastructure sites in 'core Boston neighborhoods.' All social infrastructure sites were identified either by the Google Maps Places API, and then checked by Northeastern University masters students, or found independently by those students. Points have been classified into five groups, including (1) *community spaces*, (2) *places of worship*, (3) *social businesses*, (4) *parks*, or (5) *other*.

- ```"mygroundtruthed.kml"```: refined sample of social infrastructure sites in 10 grid cells within core Boston neighborhoods, which were confirmed *on the ground* by Northeastern University masters students, or found independently by those students. Points have been classified into five groups, including (1) *community spaces*, (2) *places of worship*, (3) *social businesses*, (4) *parks*, or (5) *other*.

- ```"grid_covariates_tracts.kml"```: dataset of 226 grid cell polygons, each 1 square kilometer in area, covering the entirety of Boston. Meant to approximate the size of an individual's city block. Contains neighborhood that each grid cell falls into. Each grid cell contains the average of overlapping census tract variables, recorded in the American Community Survey's 5 year estimates from 2019. Split up into 4 milestone categories. *Note: Our analysis should only focus on grid cells from milestones 1, 2, and 3 (core Boston). We have not yet collected social infrastructure data for milestone 4 grid cells (outer Boston).*

- ```"grid_covariates_block_groups.kml"```: same dataset as above, except that census variables reflect average of overlapping census block group variables, recorded using American Community Survey 5-year estimates from 2019. It was not possible to get 2020 data (not yet fully released or verified), or census block data, because census block data dates back to 2010 - a little too long ago to feel really accurate anymore. Instead, I used census block groups because these match the size of our grid cells pretty well. Note: not all variables are reliable, due to missing data. Please be cautious when using block group data.

- ```"grid_samples_for_ground_truthing.kml"```: 20 grid cells, randomly sampled from the population of Milestone 1-3 populated, non-ocean grid cells, and then randomly divided into two groups: undergrads and masters students. Our students are walking the streets of Boston currently and mapping every type of social infrastructure that they can find, to verify our online tallies. These random samples match the population in the traits included.

<br>

## Other Data

- ```county.kml```: county polygon for Boston (Suffolk County) (2019).

- ```tracts_census.csv```: raw census track data, from 5-year 2019 ACS census results.

- ```tracts.kml```: census tract polygons in Boston (2019).

- ```block_groups.kml```: census tract block groups in Boston (2019).

- ```block_groups_census.csv```: raw census block group data, from 5-year 2019 ACS census results.

- ```blocks.kml```: census block polygons in Boston (says 2020, but might actually be 2010).

- ```neighborhoods.kmz```: neighborhood polygons, in case helpful.

<br>

For those interested, please see below for example code on how these data outputs were created.



# 0. Load Packages

```{r, message = FALSE, warning = FALSE,  message = FALSE, warning = FALSE}
# Data Wrangling Packages
library(tidyverse) # for data manipulation
library(viridis) # for color palettes
library(ggpubr) # for nice visuals
library(ggtext) # for html code in visuals

# Mapping Packages
library(sf) # for tidy spatial data
library(rgdal)  # for spatial operations
library(ggspatial) # for scales & north arrows

# Census Data Packages
library(tigris) # for US polygons
library(tidycensus)
library(censusapi)

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
```

<br>

# 1. Get Raw Data

## County Polygon

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Download the polygons for Suffolk County, MA
tigris::counties(cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = wgs) %>%
  filter(statefp == "25", name %in% c("Suffolk")) %>%
  select(name, geoid, area_land = aland, geometry) %>%
  write_sf("county.kml", driver = "kml", delete_layer = TRUE)
```

## Census Tract Polygons

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Download the polygons for Suffolk County census tracts
tigris::tracts(state = "MA", county = "025", cb = TRUE, year = 2019) %>%
  st_as_sf() %>%
  # Set all column names to lowercase
  magrittr::set_colnames(value = names(.) %>% tolower()) %>%
  # Set a basic WGS projection, just in case
  st_transform(crs = wgs) %>%
  dplyr::bind_rows() %>%
  # Keep only a subset of variables
  select(geoid, area_land = aland, geometry) %>%
  write_sf("tracts.kml", driver = "kml", delete_layer = TRUE)
```

## Census Tract Data

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
library(tidycensus)
# Load your Census API key here
census_api_key("My_API_key_goes_here")
```

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
library(tidycensus)
library(censusapi)

# Explore variables available here
# vars <- load_variables(year = 2018, dataset = "acs5", cache = TRUE) %>%
#     mutate_at(vars(label, concept), funs(tolower(.)))
# vars %>% head()

# Repeat for tracts
get_acs(
  year = 2019, survey = "acs5",
  state = "MA", geography = "tract",
  variables = c(
    "B01003_001E", # Total Population
    # Age
    "B01001_020", "B01001_021",
    "B01001_022", "B01001_023",
    "B01001_024", "B01001_025",
    "B01001_044", "B01001_045",
    "B01001_046", "B01001_047",
    "B01001_048", "B01001_049",
    "B01001_026", # Gender
    # Race/Ethnicity
    "B02001_002E", "B02001_003E", 
    "B02001_004E", "B02001_005E", #
    "B02001_006E", "B03001_003E", 
    # Socioeconomics
    "B19083_001E", "B06009_004", 
    "B23025_003E", "B23025_005E", 
    "B19013_001", "B25105_001", 
    "B08128_006E", "B08128_007E",
    "B08128_008E")) %>%
  select(geoid = GEOID, variable, estimate) %>%
  # Now recode each of these variables with names
  mutate(variable = variable %>% dplyr::recode(
    "B01003_001" = "pop", # Total Population
    # Age
    "B01001_020" = "pop_age_65_66_male",
    "B01001_021" = "pop_age_67_69_male",
    "B01001_022" = "pop_age_70_74_male",
    "B01001_023" = "pop_age_75_79_male",
    "B01001_024" = "pop_age_80_84_male",
    "B01001_025" = "pop_age_85_over_male",
    
    "B01001_044" = "pop_age_65_66_female",
    "B01001_045" = "pop_age_67_69_female",
    "B01001_046" = "pop_age_70_74_female",
    "B01001_047" = "pop_age_75_79_female",
    "B01001_048" = "pop_age_80_84_female",
    "B01001_049" = "pop_age_85_over_female",
    
    "B01001_026" = "pop_women", # Gender
    "B02001_002" = "pop_white",
    "B02001_003" = "pop_black", # Estimate!!Total!!Black or African American alone
    "B02001_004" = "pop_natam", #Estimate!!Total!!American Indian andAlaska Native alone
    "B02001_005" = "pop_asian", #Estimate!!Total!!Asian alone
    "B02001_006" = "pop_pacific", #Estimate!!Total!!Native Hawaiian and Other Pacific Islander alone
    "B03001_003" = "pop_hisplat", # Hispanic or Latino
    "B06009_004" = "pop_some_college",
    "B19083_001" = "income_inequality", #Income inequality: Estimate!!Gini Index)
    
    "B23025_003" = "pop_labor_force", # Estimate!!Total!!In labor force!!Civilian labor force
    "B23025_005" = "pop_unemployed", #Estimate!!Total!!In labor f orce!!Civilian labor force!!Unemployed
    
    "B19013_001" = "median_income", #Estimate!!Median Household income (dollars)!
    "B25105_001" = "median_monthly_housing_cost",
    "B08128_006" = "employees_muni", #Estimate!!Total!!Local government workers
    "B08128_007" = "employees_state", #Estimate!!Total!!State government workers
    "B08128_008" = "employees_fed", #Estimate!!Total!!Federal government workers
  )) %>%
  # Pivot into a wide matrix, where each variable is a column
  pivot_wider(id_cols = geoid,
              names_from = variable,
              values_from = estimate) %>%
  # Tally up population in this age category
  mutate(pop_age_65_plus = pop_age_65_66_male + pop_age_67_69_male +
           pop_age_70_74_male + pop_age_75_79_male +
           pop_age_80_84_male + pop_age_85_over_male +
           pop_age_65_66_female + pop_age_67_69_female +
           pop_age_70_74_female + pop_age_75_79_female +
           pop_age_80_84_female + pop_age_85_over_female) %>%
  # % percentage of population
  mutate_at(
    vars(
      pop_age_65_plus, pop_women,
      pop_white, pop_black, pop_natam,
      pop_asian, pop_pacific, pop_hisplat,
      pop_some_college,
      employees_muni, employees_state, employees_fed),
    # Normalize as a percent
    funs(. / pop)) %>%
  # Calculate percentage of residents unemployed
  mutate(pop_unemployed = pop_unemployed / pop_labor_force) %>%
  # Get age groups
  select(-c(pop_age_65_66_male, pop_age_67_69_male,
            pop_age_70_74_male, pop_age_75_79_male,
            pop_age_80_84_male, pop_age_85_over_male,
            pop_age_65_66_female, pop_age_67_69_female,
            pop_age_70_74_female, pop_age_75_79_female,
            pop_age_80_84_female, pop_age_85_over_female)) %>%
  # Filter into just Suffolk County
  filter(str_sub(geoid, 1, 5) == "25025") %>%
  # and save
  write_csv("tracts_census.csv")
```

## Tracts in Boston

```{r, eval = FALSE}
# Load in census tract level demographics from 2019
read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv", 
                         col_types = list(geoid = col_character(),
                                          col_double()))) %>%
  st_transform(crs = aea) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk county
  filter(area_land > 0) %>%
  filter(!str_sub(geoid, 6,7) %in% c(15:18)) %>%
  # Save these tracts simply so you don't need to go get them again
  saveRDS("tracts_boston.rds")
```


## Census Block/Block Group Data

Census block data is currently only available through tidycensus from the 2010 decennial census. Further, this data is not complete, since the Census bureau is still double checking numbers for many variables. So, I gathered data from the block group level instead.

Download block boundaries.

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
tigris::blocks(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = wgs) %>%
  select(geoid = GEOID10, area_land = ALAND10, geometry) %>%
  write_sf("blocks.kml", driver = "kml", delete_layer = TRUE)
```

Download block group boundaries!

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
tigris::block_groups(state = "MA", county = "025", year = 2020) %>%
  st_as_sf() %>%
  st_transform(crs = wgs) %>%
  select(geoid = GEOID, area_land = ALAND, geometry) %>%
  write_sf("block_groups.kml", driver = "kml", delete_layer = TRUE)

```

Download block group data!

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
get_acs(
  geography = "block group",
  year = 2019, state = "MA", county = "025",
  variables = c(
    "B01003_001E", # Total Population
    # Age
    "B01001_020", "B01001_021",
    "B01001_022", "B01001_023",
    "B01001_024", "B01001_025",
    "B01001_044", "B01001_045",
    "B01001_046", "B01001_047",
    "B01001_048", "B01001_049",
    "B01001_026", # Gender
    # Race/Ethnicity
    "B02001_002E", "B02001_003E", 
    "B02001_004E", "B02001_005E", #
    "B02001_006E", "B03001_003E", 
    # Socioeconomics
    "B19083_001E", "B06009_004", 
    "B23025_003E", "B23025_005E", 
    "B19013_001", "B25105_001", 
    "B08128_006E", "B08128_007E",
    "B08128_008E")) %>%
  select(geoid = GEOID, variable, estimate) %>%
  # Now recode each of these variables with names
  mutate(variable = variable %>% dplyr::recode(
    "B01003_001" = "pop", # Total Population
    # Age
    "B01001_020" = "pop_age_65_66_male",
    "B01001_021" = "pop_age_67_69_male",
    "B01001_022" = "pop_age_70_74_male",
    "B01001_023" = "pop_age_75_79_male",
    "B01001_024" = "pop_age_80_84_male",
    "B01001_025" = "pop_age_85_over_male",
    
    "B01001_044" = "pop_age_65_66_female",
    "B01001_045" = "pop_age_67_69_female",
    "B01001_046" = "pop_age_70_74_female",
    "B01001_047" = "pop_age_75_79_female",
    "B01001_048" = "pop_age_80_84_female",
    "B01001_049" = "pop_age_85_over_female",
    
    "B01001_026" = "pop_women", # Gender
    "B02001_002" = "pop_white",
    "B02001_003" = "pop_black", # Estimate!!Total!!Black or African American alone
    "B02001_004" = "pop_natam", #Estimate!!Total!!American Indian andAlaska Native alone
    "B02001_005" = "pop_asian", #Estimate!!Total!!Asian alone
    "B02001_006" = "pop_pacific", #Estimate!!Total!!Native Hawaiian and Other Pacific Islander alone
    "B03001_003" = "pop_hisplat", # Hispanic or Latino
    "B06009_004" = "pop_some_college",
    "B19083_001" = "income_inequality", #Income inequality: Estimate!!Gini Index)
    
    "B23025_003" = "pop_labor_force", # Estimate!!Total!!In labor force!!Civilian labor force
    "B23025_005" = "pop_unemployed", #Estimate!!Total!!In labor f orce!!Civilian labor force!!Unemployed
    
    "B19013_001" = "median_income", #Estimate!!Median Household income (dollars)!
    "B25105_001" = "median_monthly_housing_cost",
    "B08128_006" = "employees_muni", #Estimate!!Total!!Local government workers
    "B08128_007" = "employees_state", #Estimate!!Total!!State government workers
    "B08128_008" = "employees_fed", #Estimate!!Total!!Federal government workers
  )) %>%
  # Pivot into a wide matrix, where each variable is a column
  pivot_wider(id_cols = geoid,
              names_from = variable,
              values_from = estimate) %>%
  # Tally up population in this age category
  mutate(pop_age_65_plus = pop_age_65_66_male + pop_age_67_69_male +
           pop_age_70_74_male + pop_age_75_79_male +
           pop_age_80_84_male + pop_age_85_over_male +
           pop_age_65_66_female + pop_age_67_69_female +
           pop_age_70_74_female + pop_age_75_79_female +
           pop_age_80_84_female + pop_age_85_over_female) %>%
  # % percentage of population
  mutate_at(
    vars(
      pop_age_65_plus, pop_women,
      pop_white, pop_black, pop_natam,
      pop_asian, pop_pacific, pop_hisplat,
      pop_some_college,
      employees_muni, employees_state, employees_fed),
    # Normalize as a percent
    funs(. / pop)) %>%
  # Calculate percentage of residents unemployed
  mutate(pop_unemployed = pop_unemployed / pop_labor_force) %>%
  # Get age groups
  select(-c(pop_age_65_66_male, pop_age_67_69_male,
            pop_age_70_74_male, pop_age_75_79_male,
            pop_age_80_84_male, pop_age_85_over_male,
            pop_age_65_66_female, pop_age_67_69_female,
            pop_age_70_74_female, pop_age_75_79_female,
            pop_age_80_84_female, pop_age_85_over_female)) %>%
  # Filter into just Suffolk County
  filter(str_sub(geoid, 1, 5) == "25025") %>%
  # and save
  write_csv("block_groups_census.csv")


```


# 2. Fishnet Grid


## Fishnet

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# How to make the original fishnet
#########################
# 1 square kilometer
#########################
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Import tracts
tracts <- read_sf("tracts.kml") %>%
  # remove extraneous kml fields
  select(-c(Name, description, timestamp:icon)) %>%
  # Transform to equal area conic projection
  st_transform(crs = aea)  


tracts %>%
  # Let's try 1000 meters (1 city block is 100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = 1000, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(tracts) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(id = 1:n()) %>%
  write_sf("fishnet.kml", driver = "kml", delete_layer = TRUE)

remove(tracts)
```


## Fishnet with Census Tract Data

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Import census tracts
tracts <- read_sf("tracts.kml") %>%
  # remove extraneous kml fields
  select(-c(Name, description, timestamp:icon)) %>%
  # Transform to equal area conic projection
  st_transform(crs = aea)  %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv",
                         col_types = list(geoid = col_character()))) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) 

# We're going to get the mean census traits for every grid cell
read_sf("milestones.kmz")  %>%
  select(Name, description, neighborhood, milestone, geometry) %>%
  # Transform to Albers equal area conic project
  st_transform(crs= aea) %>%
  # Join in census tracts
  st_join(tracts) %>%
  # now reduce to just grid cells,
  group_by(Name, description, neighborhood, milestone) %>%
  # Calculating average traits for each
  summarize(pop_density = mean(pop_density, na.rm = TRUE),
            pop_women = mean(pop_women, na.rm = TRUE),
            pop_white = mean(pop_white, na.rm = TRUE),
            pop_black = mean(pop_black, na.rm = TRUE),
            pop_natam = mean(pop_natam, na.rm = TRUE),
            pop_asian = mean(pop_asian, na.rm = TRUE),
            pop_pacific = mean(pop_pacific, na.rm = TRUE),
            pop_hisplat = mean(pop_hisplat, na.rm = TRUE),
            pop_some_college = mean(pop_some_college, na.rm = TRUE),
            employees_muni = mean(employees_muni, na.rm = TRUE),
            employees_state = mean(employees_state, na.rm = TRUE),
            employees_fed = mean(employees_fed, na.rm = TRUE),
            median_income = mean(median_income, na.rm = TRUE),
            income_inequality = mean(income_inequality, na.rm = TRUE),
            pop_unemployed = mean(pop_unemployed, na.rm = TRUE),
            median_monthly_housing_cost = mean(median_monthly_housing_cost, na.rm = TRUE),
            pop_age_65_plus = mean(pop_age_65_plus, na.rm = TRUE),
            geometry = unique(geometry)) %>%
  ungroup() %>%
  # In a few cases, the blocks had no residents, so no population density.
  mutate(pop_density = if_else(is.infinite(pop_density), 0, pop_density)) %>%
  rename(cell_id = Name) %>%
  st_write("grid_covariates_tracts.kml", driver = "kml", delete_layer = TRUE) 


remove(tracts)
```


## Fishnet with Census Block Group Data

```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Import census tracts
bg <- read_sf("block_groups.kml") %>%
  # remove extraneous kml fields
  select(-c(Name, description, timestamp:icon)) %>%
  # Transform to Albers equal area conic project
  st_transform(crs = aea) %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("block_groups_census.csv",
                         col_types = list(geoid = col_character()))) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) 


# We're going to get the mean census traits for every grid cell
read_sf("milestones.kmz")  %>%
  select(Name, description, neighborhood, milestone, geometry) %>%
  # Transform to Albers equal area conic project
  st_transform(crs= aea) %>%
  # Join in census block groups
  st_join(bg) %>%
  # now reduce to just grid cells,
  group_by(Name, description, neighborhood, milestone) %>%
  # Calculating average traits for each
  summarize(pop_density = mean(pop_density, na.rm = TRUE),
            pop_women = mean(pop_women, na.rm = TRUE),
            pop_white = mean(pop_white, na.rm = TRUE),
            pop_black = mean(pop_black, na.rm = TRUE),
            pop_natam = mean(pop_natam, na.rm = TRUE),
            pop_asian = mean(pop_asian, na.rm = TRUE),
            pop_pacific = mean(pop_pacific, na.rm = TRUE),
            pop_hisplat = mean(pop_hisplat, na.rm = TRUE),
            pop_some_college = mean(pop_some_college, na.rm = TRUE),
            employees_muni = mean(employees_muni, na.rm = TRUE),
            employees_state = mean(employees_state, na.rm = TRUE),
            employees_fed = mean(employees_fed, na.rm = TRUE),
            median_income = mean(median_income, na.rm = TRUE),
            income_inequality = mean(income_inequality, na.rm = TRUE),
            pop_unemployed = mean(pop_unemployed, na.rm = TRUE),
            median_monthly_housing_cost = mean(median_monthly_housing_cost, na.rm = TRUE),
            pop_age_65_plus = mean(pop_age_65_plus, na.rm = TRUE),
            geometry = unique(geometry)) %>%
  ungroup() %>%
  # In a few cases, the blocks had no residents, so no population density.
  mutate(pop_density = if_else(is.infinite(pop_density), 0, pop_density)) %>%
  st_write("grid_covariates_block_groups.kml", 
           driver = "kml", delete_layer = TRUE) 

remove(bg)
```


## Interpolate

Some areas of Boston *are* populated, by the census doesn't pick them up. We'll use spatial interpolation to estimate their population density based on surrounding grid cells.

```{r}
library(gstat)
library(sf)
library(rgdal)
library(tidyverse)

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

shapes <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  mutate(pop_density = if_else(pop_density == 0, NA_real_, pop_density)) %>%
  st_transform(crs = aea) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  select(cell_id, pop_density, geometry) 

m <- gstat::gstat(
    id = "cell_id", 
    formula = pop_density ~ 1, 
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(shapes) %>%
  mutate(cell_id = shapes$cell_id)

m <- m %>% 
  as.data.frame() %>%
  select(cell_id, pop_density_int = cell_id.pred)

read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  mutate(pop_density = if_else(pop_density == 0, NA_real_, pop_density)) %>%
  st_transform(crs = aea) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  # Join in our interpolated population density
  left_join(by = "cell_id", y = m) %>%
  # Overwrite
  write_sf("grid_covariates_tracts.kml", delete_layer = TRUE)
```

## Repeat for 2 km

Let's briefly repeat this now for the 2 km-squared grid, for sensitivity testing.

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Import census tracts
tracts <- read_sf("tracts.kml") %>%
  # remove extraneous kml fields
  select(-c(Name, description, timestamp:icon)) %>%
  # Transform to equal area conic projection
  st_transform(crs = aea)  %>%
  # Load in census tract level demographics from 2019
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv",
                         col_types = list(geoid = col_character()))) %>%
  # calculate population density, in people per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) 

grid <- read_rds("grid/fishnet_2.rds") %>%
  # Transform to Albers equal area conic project
  st_transform(crs= aea) %>%
  # Join in census tracts
  st_join(tracts) %>%
  # now reduce to just grid cells,
  group_by(id) %>%
  # Calculating average traits for each
  summarize(pop_density = mean(pop_density, na.rm = TRUE),
            pop_women = mean(pop_women, na.rm = TRUE),
            pop_white = mean(pop_white, na.rm = TRUE),
            pop_black = mean(pop_black, na.rm = TRUE),
            pop_natam = mean(pop_natam, na.rm = TRUE),
            pop_asian = mean(pop_asian, na.rm = TRUE),
            pop_pacific = mean(pop_pacific, na.rm = TRUE),
            pop_hisplat = mean(pop_hisplat, na.rm = TRUE),
            pop_some_college = mean(pop_some_college, na.rm = TRUE),
            employees_muni = mean(employees_muni, na.rm = TRUE),
            employees_state = mean(employees_state, na.rm = TRUE),
            employees_fed = mean(employees_fed, na.rm = TRUE),
            median_income = mean(median_income, na.rm = TRUE),
            income_inequality = mean(income_inequality, na.rm = TRUE),
            pop_unemployed = mean(pop_unemployed, na.rm = TRUE),
            median_monthly_housing_cost = mean(median_monthly_housing_cost, na.rm = TRUE),
            pop_age_65_plus = mean(pop_age_65_plus, na.rm = TRUE),
            geometry = unique(geometry)) %>%
  ungroup() %>%
  # In a few cases, the blocks had no residents, so no population density.
  mutate(pop_density = if_else(is.infinite(pop_density), 0, pop_density))  %>%
  rename(cell_id = id)

remove(tracts)

library(gstat)
library(sf)
library(rgdal)
library(tidyverse)

shapes <- grid %>%
  mutate(pop_density = if_else(pop_density == 0, NA_real_, pop_density)) %>%
  st_transform(crs = aea) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  select(cell_id, pop_density, geometry) 

m <- gstat::gstat(
    id = "cell_id", 
    formula = pop_density ~ 1, 
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(shapes) %>%
  mutate(cell_id = shapes$cell_id)

m <- m %>% 
  as.data.frame() %>%
  select(cell_id, pop_density_int = cell_id.pred)

# Join back in the interpolated population density rates
# for 2-km-2 grid cells
grid %>%
  mutate(pop_density = if_else(pop_density == 0, NA_real_, pop_density)) %>%
  st_transform(crs = aea) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  # Join in our interpolated population density
  left_join(by = "cell_id", y = m) %>%
  # Overwrite
  write_sf("grid_covariates_tracts_2.kml", delete_layer = TRUE)
```



# 3. Preparations

Before we estimate the amount of social infrastructure in each neighborhood in Boston, we need to conduct several preparatory exercises. How much would this approach cost if we extend it to the entire United States? We need to estimate this to see if this is even feasible before we run the Boston pilot study.

To know how much it will cost to estimate social infrastructure in every community, we need to know three main things:

- ```area```: total area of populated, applicable communities in the US

- ```size```: Size of a Community

- ```n```: Number of those Communities in the US

- ```cost```: Cost of mapping a single community

Such that:

$$ n_{communities} = area_{total} / size_{1-community} $$

and therefore:

$$ cost_{total} =  cost_{per-community} \times n_{communities} $$

## 3.1 Past Estimates of Community Size

Recently, several communities have tried to estimate the average size of a community in the US, based on resident perceptions. The results have been varied. [Kwame	Donaldson at the US Census Bureau](https://www.census.gov/content/dam/Census/programs-surveys/ahs/working-papers/how_big_is_your_neighborhood.pdf
) relied on census questions, such as those which ask residents whether there is a beach in their community, and then estimated the distance from the resident's home to the nearest beach. His Census Bureau working paper estimated the average size of a community, as perceived by respondents, at a radisu of between ***520 and 1060 meters.*** This number varies somewhat by region.

```{r}
# Borrowing from Donaldson's report on page 33:
community <- bind_rows(
  data.frame(
    place = "Northeast",
    estimate = 6.0771,
    lower = 4.3404,
    upper = 7.8137),
  data.frame(
    place = "Midwest",
    estimate = 8.6661,
    lower = 5.4781,
    upper = 11.8542),
  data.frame(
    place = "South",
    estimate = 4.9010,
    lower = 1.5795,
    upper = 8.2225),
  data.frame(
    place = "West",
    estimate = 6.8835,
    lower = 4.9697,
    upper = 8.7973))
```

Let's view his tallies below. These numbers reflect **derived community area**, which is the value of the exponent with a base of 10. For example, $6.4862 = 10^{6.4862} = 3,063,374$ meters.

```{r}
community
```

That is obviously not easily interpretable, so let's adjust these to the correct units. So let's exponentiate it, but divide by 1 million square meters to translate to square kilometers.


Well, their estimates for the Midwest seem extreme, but the other ones look plausible. Accounting for these regional differences, it makes sense to conclude, as they did, that the average neighborhood is between 520 and 1060 meters. 1 square kilometer makes for a good estimate, given the wide range in the Northeast (1.19), South (0.08), and West (7.65).

```{r}
community %>%
  mutate_at(vars(estimate, lower, upper),
            funs((10^. *1e-6)))
```

## 3.2 Estimating Cost

First, we're going to get some background data to assist us, like the polygons of all states and counties in the US.

```{r}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
```

Let's download the polygons for all US tracts.

```{r, eval =  FALSE}
data.frame(state = state.abb) %>%
  # For each state,
  split(.$state) %>%
  # Get the tracts
  map(~tigris::tracts(state = .$state, cb = TRUE, year = 2019) %>%
        st_as_sf() %>%
        # Set all column names to lowercase
        magrittr::set_colnames(value = names(.) %>% tolower()) %>%
        # Set a basic WGS projection, just in case
        st_transform(crs = wgs), .id = "state") %>%
  dplyr::bind_rows() %>%
  # Keep only a subset of variables
  select(geoid, area_land = aland, geometry) %>%
  saveRDS("tracts_us.rds")
```

Next, let's eliminate [areas outside the continental US](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/?cid=nrcs143_013696) (mostly because Alaska, Hawaii, Guam, and other such states and territories have very different geographic traits). We will save this as ```tracts_us.rds```.

```{r, eval = FALSE}
read_rds("tracts_us.rds") %>%
  filter(as.numeric(str_sub(geoid, 1,2)) %in% 1:56) %>%
  # Transform to equal area conic
  st_transform(crs = aea) %>%
  # Join in their population
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv") %>% 
              mutate(geoid = str_pad(geoid, width = 11, side = "left", pad = "0")) %>%
  select(geoid, pop)) %>%
  # Filter to populated tracts
  filter(pop > 0) %>%
  # Get population density in 1000s per square kilometer
  mutate(pop_density = pop / (area_land / 1000000)) %>%
  saveRDS("tracts_pop.rds")
```

```{r, eval = FALSE}
# Now let's calculate the total area of populated census tracts in the US
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  summarize(area = sum(area_land, na.rm = TRUE) / 1000000)
```

The total area of populated census tracts in the continental US is 9,114,979 square kilometers.

In fact, using the histogram below, we can explore the distribution of area in populated census tracts. It looks like a small percentage of census tracts (~5%) have an area near or at 1,000 square kilometers. That's enormous! We can hardly call a census tract like that, likely found in communities in Montana, comparable to census tracts in Boston or other urban census tracts. Best to exclude these from the current tally, since we will need a different system to measure social infrastructure for such different, sparsely populated communities.

```{r, eval = FALSE}
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  ggplot(mapping = aes(x = area_land / 1000000)) +
  geom_histogram() +
  scale_x_log10()
```
But at what point should we exclude such communities? We examined the distribution of census tract land area at the 50th, 75th, 90th, 95th, 97.5th and 99th percentiles below.

```{r, eval = FALSE}
# Get percentiles
quantile(read_rds("tracts_pop.rds")$area_land / 1000000, 
         probs = c(0, 0.50, .75, .90, .95, .975, .99), na.rm = TRUE)
```

If your census tract is above the 95th percentile in area, you have at least 422 square kilometers of land area. Which is twice the size of ALL of Boston. That likely means most of this land is not inhabited. To map every square kilometer would use a tremendous amount of money, but capture very few communities. So let's shave off all communities like this - the top 5% largest census tracts in terms of area.

By excluding those with an area above 214.15 square kilometers (the 95th percentile), let's calculate the total area of 'populated' (read: not extremely sparesely populated) census tracts in the US.

```{r, eval = FALSE}
read_rds("tracts_pop.rds") %>%
  as.data.frame() %>%
  # Zooming into just the bottom 95% 
  filter((area_land / 1000000) < 214.157569) %>%
  summarize(area = sum(area_land, na.rm = TRUE) / 1000000)
```

That leaves a total of 1,389,431 square kilometers of land area in the US to examine. Given the 10 types of social infrastructure we identify below as recordable through the Google Maps API, how much would it cost to map these?

At the 1 square kilometer level:

$ 1,389,431_{km^{2}} * 10_{types-of-social-infrastructure} \times 0.03_{USD-per-km^{2}} = 41,829_{USD}$

That is a lot, but doable!

To do just Boston would cost:

$ 125.4_{km^{2}} \times 10_{types-of-social-infrastructure} \times 0.03_{USD-per-km^{2}} = 37.62_{USD}$

This is super doable. We just need to start small with individual cities to ensure our approach is valid.

How many queries will we need to run?

Well, for Boston, we should get approximately 125.4 grid cells, each 1 square kilometer, plus or minus a few cells. Using 1000 square meter squares, we get 1030 squares. Using 1000 square meter hexagons, we get 1174 squares. It seems more efficient to use cells for now, rather than hexagons.

In order to fully encapsulate a 1000 square meter square, a circular radius needs to be greater than 1000 meters. We can use the Pythagorean theorem to get the correct size raidus for a circle that perfectly captures that square. The radius should be 1414.214 meters.

```{r{}
sqrt(1000^2 + 1000^2)
```

We can find out just how extreme it will be to query them all right now.

For example, let's say we need to query every one of them, and we cap each search at 20 results. Even given a conservative estimate, we would need to make at least 10-15 searches of each grid cell.

Here's our starter list of possible social infrastructure sites, based on the literature. We use a more refined list below.

1. Parks
2. squares
3. fountains
4. sports grounds
5. public schools
7. community centers
8. libraries
9. museums
10. art galleries
11. zoos
12. botanical gardnes
13. corner stores
14. barbershops
15. places of worship

## 3.3 Visualizing Boston

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Load in census tracts in suffolk county
tracts <-read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk county
  filter(area_land > 0) %>%
  filter(!str_sub(geoid, 6,7) %in% c(15:18)) 

# Load in suffolk county background
county <- read_sf("county.kml") %>%
  select(-c(Name:icon))

ggplot() +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "darkgrey", size = 5, fill = "white") +
  geom_sf(data = county,
            mapping = aes(size = "County"),
          color = "black", fill = "white") +
  geom_sf(data = tracts,
          mapping = aes(size = "Tracts"),
          fill = "tan", color = "black") +
  scale_size_manual(values = c(1.15, 0.1)) +
  geom_sf(data = county, color = "black", size = 1.5, fill = NA) +
  theme_void(base_size = 14) +
  labs(size = "Borders",
       subtitle = paste("Boston Census Tracts", 
                        " (n = ", nrow(tracts), ")", sep = ""))
```
## 3.4 Make grids

Based on this, let's make a few grids to compare their sizes.

```{r, eval = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Import all census tracts in suffolk county
tracts <-read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_triangulate(crs = aea)
```

### 1 km<sup>2</sup>

```{r, eval = FALSE}
tracts %>%
  # Let's try 1000 meters (1 city block is 100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = 1000, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(tracts) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(id = 1:n()) %>%
  saveRDS("grid/fishnet.rds")

# Get centroids from those points
read_rds("grid/fishnet.rds") %>%
  # Now let's extract the centroids of these cells too
  group_by(id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
      # Transform back to normal coorindate projection
  st_transform(crs = wgs) %>%
  mutate(lng = st_coordinates(geometry)[,1],
            lat = st_coordinates(geometry)[,2])  %>%
   # Transform to equal area conic
  st_transform(crs = aea) %>%
  saveRDS("grid/fishnet_points.rds")
```

### 2 km<sup>2</sup>

Let's also make several bigger fishnets.

```{r,eval = FALSE}
tracts %>%
  # Let's try 2000 meters (1 city block is 100 meters long; 20 blocks are 2000 meters long)
  st_make_grid(cellsize = 2000, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(tracts) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(id = 1:n()) %>%
  saveRDS("grid/fishnet_2.rds")


# Get centroids from those points
read_rds("grid/fishnet_2.rds") %>%
  # Now let's extract the centroids of these cells too
  group_by(id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
      # Transform back to normal coorindate projection
  st_transform(crs = wgs) %>%
  mutate(lng = st_coordinates(geometry)[,1],
            lat = st_coordinates(geometry)[,2])  %>%
   # Transform to equal area conic
  st_transform(crs = aea) %>%
  saveRDS("grid/fishnet_2_points.rds")
```

### 5 km<sup>2</sup>

```{r, eval = FALSE}
tracts %>%
  # Let's try 5000 meters (1 city block is 100 meters long; 50 blocks are 5000 meters long)
  st_make_grid(cellsize = 5000, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(tracts) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(id = 1:n()) %>%
  saveRDS("grid/fishnet_5.rds")


# Get centroids from those points
read_rds("grid/fishnet_5.rds") %>%
  # Now let's extract the centroids of these cells too
  group_by(id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
      # Transform back to normal coorindate projection
  st_transform(crs = wgs) %>%
  mutate(lng = st_coordinates(geometry)[,1],
            lat = st_coordinates(geometry)[,2])  %>%
   # Transform to equal area conic
  st_transform(crs = aea) %>%
  saveRDS("grid/fishnet_5_points.rds")
```

### 10 km<sup>2</sup>

```{r,eval = FALSE}
tracts %>%
  # Let's try 10,000 meters (1 city block is 100 meters long; 100 blocks are 10,000 meters long)
  st_make_grid(cellsize = 10000, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  # Join in original counties
  st_join(tracts) %>%
  # Zoom into just grid cells overlapping those counties
  filter(!is.na(geoid)) %>%
  # Get just distinct grid cells
  select(geometry) %>%
  distinct() %>%
  # and let's give each an ID
  mutate(id = 1:n()) %>%
  saveRDS("grid/fishnet_10.rds")


# Get centroids from those points
read_rds("grid/fishnet_10.rds") %>%
  # Now let's extract the centroids of these cells too
  group_by(id) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
      # Transform back to normal coorindate projection
  st_transform(crs = wgs) %>%
  mutate(lng = st_coordinates(geometry)[,1],
            lat = st_coordinates(geometry)[,2])  %>%
   # Transform to equal area conic
  st_transform(crs = aea) %>%
  saveRDS("grid/fishnet_10_points.rds")
```


## 3.5 Visualize Grid

Using a dataset that we make later, let's just grab the cell IDs of the cells we use in our study range, so we can highlight them in the visual below.

```{r}
mycells <- read_sf("milestones.kmz") %>%
  select(cell_id = Name, milestone, geometry) %>%
  # Filter to just the inner-neighborhoods of Boston, which are most comparable
  filter(str_detect(milestone, "M1|M2|M3") ) %>%
  # remove cells which should be ineligible, as in 
  # they don't contain entire cells because they are coastal peninsulas
  filter(!cell_id %in% 
    paste("Cell", c(81,72,62, 160, 182, 176, 166, 165, 149,
                    89, 100, 80, 50, 62, 166, 89, 112, 129))) %>%
  as.data.frame() %>%
  select(cell_id) %>%
  unlist() %>%
  str_remove("Cell ") %>%
  as.integer()

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Load in census tracts in suffolk county
tracts <-read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk county
  filter(area_land > 0) %>%
  filter(!str_sub(geoid, 6,7) %in% c(15:18))  %>%
  st_transform(crs= aea)

# Create one big polygon outline of tracts within Boston
boston <- tracts %>% 
  summarize(geoid = "Boston", geometry = st_union(geometry))

# Load in Suffolk county background
county <- read_sf("county.kml") %>%
  select(-c(Name:icon)) %>%
    st_transform(crs= aea)


# Get fishnet grid
fish <- read_rds("grid/fishnet.rds") %>%
  # Joining in Boston label for cells within Boston
  st_join(boston) %>%
  mutate(type = case_when(
    !is.na(geoid) & id %in% mycells ~ "validation",
    !is.na(geoid) ~ "boston",
    TRUE ~ "outside")) %>%
  mutate(type = type %>% dplyr::recode_factor(
    "validation" = "<b>Validation Cells</b><br>(n = 58)<br>",
    "boston" = "<b>Boston Cells</b><br>(n = 159)<br>(217 including<br>validation)<br>",
    "outside" = "<b>Outside Boston</b><br>(n = 43)<br>(Excluded)"))

# Save the ids of cells within Boston for later use
fish %>%
  filter(type != "<b>Outside Boston</b><br>(n = 43)<br>(Excluded)") %>%
  as.data.frame() %>%
  select(id) %>%
  unlist() %>%
  unname() %>%
  saveRDS("boston_grid.rds")




# Create buffers
#points <- fish %>%  
#  mutate(geometry = st_buffer(st_centroid(geometry), dist = 1410/2)) %>% 
#  st_as_sf() 

library(ggtext)
g1 <- ggplot() +
   geom_sf(data = tracts, mapping = aes(size = "Tracts"), 
          color = "black",  fill = "grey") +
  geom_sf(data = fish, mapping = aes(fill = type), 
          color = "white", size = 0.25, alpha = 0.25) +
  #geom_sf(data = points, mapping = aes(color = type), fill = NA, 
  #        alpha = 0.5, size = 0.05) +
  scale_color_manual(values = c("#FFB000", "#DC267F", "#785EF0")) +
  scale_fill_manual(values = c("#FFB000", "#DC267F", "#785EF0")) +
  geom_sf(data = county, mapping = aes(size = "County"), 
          color = "black", fill = NA) +
  scale_size_manual(values = c(1, 0.1)) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_markdown(size = 14, hjust = 0),
        legend.text = element_markdown(size = 12),
        legend.position = "right") +
  labs(
    fill = "Cells (n = 260)\n in Suffolk County",
    size = "Borders",
    subtitle = "<b>1 km<sup>2</sup> grid cells for tallying sites</b><br><i>(n = 217 cells in Boston)</i>") +
  guides(color = "none", size = guide_legend(override.aes = list(fill = c("white", "grey"))))  +
  ggspatial::annotation_north_arrow(height = unit(0.5, "cm"),
                                    width = unit(0.5, "cm")) +
  ggspatial::annotation_scale(pad_x = unit(3.5, "cm"))

ggsave(g1, filename = "viz/figure_A5.png", dpi = 500, height = 4, width = 4.5)
# There are 260 cells of 1 square kilometer in Suffolk County

rm(list = ls())
```



# 4. API Test Run 

First, we conduct a test-run using the Google Maps Places API, accessed via the ```googleway``` package.

This demonstration will use D. Cooley's helpful *googleway* vignette from November 2020, available [here](https://cran.r-project.org/web/packages/googleway/vignettes/googleway-vignette.html#google-places-api).

First, we have to enable the Google Maps Places API. Second, we need to get an API Key. I made it restricted to work only from RStudio Cloud and only for the Places API. Let's set it below.

```{r, echo = FALSE}
# Let's load in your API key here
mykey <- "AIzaSyBCYmbJeyWhXzt0QL6haFsiW4D-r3dHcQQ"
```
```{r, eval = FALSE}
# Let's load in your API key here
mykey <- "INSERT_MY_API_KEY_HERE"
```

```{r, eval = FALSE}
# Now set the API KEY
set_key(key = mykey, api = "places")
# View which API Keys we currently have registered here
google_keys()
# If you need to use it, clear keys will clear those keys too.
#clear_keys() ## clear any previously set keys
```

You can check out pricing [here](https://cloud.google.com/maps-platform/pricing). The whole point of this demonstration is to figure out how much it will cost to run this.


## 4.1 Places of Worship Test Query

Third, we need to test out the API!

```{r, eval = FALSE}
dir.create("query")

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


# Let's import our grid cells in Boston
mypoints <- read_rds("grid/fishnet_points.rds") %>%
  as.data.frame() %>%
  select(id, lng, lat)

# Let's do this for all of Boston
testquery <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

# Get places of worship
mypoints %>%
  split(.$id) %>%
  map_dfr(~google_places(
    location = c(.$lat[1], .$lng[1]),
    keyword = "place of worship",
    radius = 770, # circumference of a circle that extends that far
    key = mykey)$result, .id = "grid") %>%
  saveRDS("grid/places_of_worship.rds")
```

```{r}
sqrt(1000^2 + 1000^2)
```

## 4.2 Duplicates

```{r}
# How many unique addresses are there?
# Does every vicinity have an ID

myquery <- read_rds("prior_queries/places_of_worship.rds")

# How often do we duplicate?
myquery %>%
  group_by(vicinity) %>%
  count() %>%
  ggplot(mapping = aes(x = reorder(vicinity, -n), y = n)) +
  geom_col(color = "white", size = 0.1, fill = "steelblue") +
  theme_classic(base_size = 14) +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank()) +
  labs(x = "378 unique place addresses found (n = 1209 search results)", 
       y = "# of Duplicate Results")

# So, this suggests that *certain kinds of infrastructure should be farmed in larger blocks than others.
```

## 4.3 Mapping

```{r}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


myquery <- read_rds("prior_queries/places_of_worship.rds") %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
  st_transform(crs = aea)  %>%
  # Grab just the distinct results
  select(-grid) %>%
  group_by(place_id) %>%
  summarize_at(vars(name, business_status, 
                 #plus_code.compound_code, 
                 rating, types, user_ratings_total, vicinity, 
                 permanently_closed, geometry),
            funs(unique(.))) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
  map(~paste(., collapse = "; ")) %>%
  unlist())

# So, this approach gathered 3 times as many results as there actually were - a really hefty amount of duplication, due to the small grid cell size

fish <- read_rds("grid/fishnet.rds")
counties <- read_sf("county.kml") %>%
  select(-c(Name:icon))

# Create buffers
points <- read_rds("grid/fishnet.rds") %>%
  mutate(geometry = st_buffer(st_centroid(geometry), dist = 1410/2)) %>% 
  st_as_sf() 

ggplot() +
  geom_sf(data = points %>% 
            st_buffer(dist = 770) %>% 
            st_as_sf(), color = "black", alpha = 0.1, size = 0.05) +
  geom_sf(data = fish, fill = "firebrick", color = "white", size = 0.1, alpha = 0.1) +
  geom_sf(data = myquery, color = "firebrick", size = 1.5, alpha = 0.5) + 
  #geom_sf(data = tracts, color = "black", size = 0.1, fill = NA) +
  geom_sf(data = counties, color = "black",size = 1, fill = NA) +
  
  #geom_sf_label(data = counties, mapping = aes(label = name)) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  labs(subtitle = "Places of Worship (red, n = 422)\n per 1 kilometer grid cells (n = 260)\nacross Populated Census Tracts\nin Suffolk County, Massachusetts")
```


## 4.4 Compare grid cell sizes

```{r, eval = FALSE}


# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

myquery <- read_rds("prior_queries/places_of_worship.rds") %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
  st_transform(crs = aea)  %>%
  # Grab just the distinct results
  select(-grid) %>%
  group_by(place_id) %>%
  summarize_at(vars(name, business_status, 
                    #plus_code.compound_code, 
                    rating, types, user_ratings_total, vicinity, 
                    permanently_closed, geometry),
               funs(unique(.))) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
           map(~paste(., collapse = "; ")) %>%
           unlist())


# number of points lost vs. dollars lost





# Import all different fishnet grids
myfish <- dplyr::bind_rows(
  read_rds("grid/fishnet.rds") %>%
    mutate(type = "1 km"),
  read_rds("grid/fishnet_2.rds") %>%
    mutate(type = "2 km"),
  read_rds("grid/fishnet_5.rds") %>%
    mutate(type = "5 km"),
  read_rds("grid/fishnet_10.rds") %>%
    mutate(type = "10 km")) %>%
  mutate(type = type %>% recode_factor(
    "1 km" = "<b>1 km<sup>2</sup></b>
    <br> <br>
    Error:<br>
    <b>Cells</b><sup>1</sup>:   0 / 260<br>
    <b>Points</b><sup>2</sup>: 0 (0%)<br>
    <b>Net gain</b><sup>3</sup>: 54.1",
    
    "2 km" = "<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3",
    "5 km" = "<b>5 km<sup>2</sup></b><br><br>Error:<br>7 / 17<br>206 (48%)<br>19.6",
    "10 km" = "<b>10 km<sup>2</sup></b><br><br>Error:<br>3 / 6<br>321 (76%)<br>-1222")) %>%
  # Join in places of worship
  st_join(myquery) %>%
  # Tally how many there are per grid cell
  group_by(id, type) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  mutate(level = case_when(
    count == 0 ~ "0",
    count > 0 & count <= 5 ~ "0-5",
    count > 5 & count <= 10 ~ "6-10",
    count > 10 & count <= 15 ~ "11-15",
    count > 15 & count <= 20 ~ "16-20",
    count > 20 & count <= 30 ~ "21-30",
    count > 30 ~ "31+") %>%
      factor(levels = c("0", "0-5", "6-10","11-15","16-20","21-30", "31+"))) %>%
  mutate(indicator = if_else(count > 20, "Problematic", "Fine"))


# Number of purple cells vs. total cells
myfish %>% 
  as.data.frame() %>%
  mutate(count = if_else(count > 20, count - 20, 0)) %>%
  group_by(type, indicator) %>%
  summarize(sum = sum(count))
# Points missed

#Total points recorded / Total points missed
#Cost of points recorded / cost of points missed
#Payoff Ratio: 


data.frame(
  label = c("1 km", "2 km", "5 km", "10 km"),
  cells_gained = c(260, 77, 10, 3),
  cells_error = c(0,   4, 7, 3),
  price = 0.03,
  points_total = 422,
  points_gained = c(422, 413, 216, 101)) %>%
  mutate(cells_total = cells_gained + cells_error,
         points_error = points_total - points_gained) %>%
  # How much does it cost, on average, to record a point?
  # Cost of gain + cost of error
  
  # points gained per dollar spent
  # Gained this many points given this cost
  mutate(valued_added = points_gained / (cells_total*price),
         valued_lost = points_error / (cells_total*price),
         
         net_valued_added = valued_added - valued_lost)

# Create a box for annotations
mybox <- data.frame(
  xmin = 1900828.7,
  ymin = 514765.5,
  xmax = 1920828.7,
  ymax = 544765.5,
  label = "Most Efficient",
  type = factor("<b>2 km<sup>2</sup></b><br><br>Error:<br>4 / 81<br>9 (2%)<br>166.3"))

g1 <- ggplot() +
  geom_sf(data = myfish, mapping = aes(fill = level,
                                       color = indicator, size = indicator)) +
  geom_sf(data = myquery, color = "#648FFF", size = 1.5, alpha = 0.5) + 
  scale_color_manual(values = c("white", "black")) +
  scale_size_manual(values = c(0.75, 1.25)) +
  geom_sf(data = myfish %>%
            filter(count > 20), 
          mapping = aes(color = indicator, size = indicator),
          fill = NA) +
  geom_sf(data = counties, color = "black",size = 0.75, fill = NA) +
  facet_wrap(~type, ncol = 4) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = ggtext::element_markdown(size = 14, hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        
        strip.text.x = ggtext::element_markdown(size = 12, hjust = 0.5),
        plot.margin = margin(0,0,0,0, "cm")) +
  viridis::scale_fill_viridis(discrete = TRUE, 
                              option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(
    fill = "# of Places\nof Worship\nper cell",
    subtitle = "<b>Places of Worship</b> (<span style='color:#648FFF'><b>blue</b></span>, n = 422) by Size of Grid Cell<br>across Populated Census Tracts in Suffolk County") +
    
    #caption = "<sup>1</sup><b> Cells:</b> counts number of cells (out of total) which registered above 20 sites per cell, leading to points missed.
    #<br>
    #<sup>2</sup><b> Points:</b> counts total places of worship (and percentage) <i>missed</i> due to exceeding the 20 point limit per grid cell search.
    #<br>
    #<sup>3</sup><b> Net gain:</b> measures total points captured (per USD spent), <i>minus</i> total points missed (per USD spent).<br><i>Highest</i> number indicates most cost-effective grid. <b>Black dashed box</b> indicates most efficient grid.") +
  guides(fill = guide_legend(override.aes = list(
    color = c("white", "white", "white", "white", "black", "black"),
    size = c(0.75,0.75,0.75,0.75,1.5, 1.5))),
         color = "none", size = "none") +
 geom_rect(
    data = mybox,
    mapping = aes(xmin = xmin - 2000,
                  ymin = ymin,
                  xmax = xmax + 1000,
                  ymax = ymax),
    fill = NA, color = "black",linetype = "dashed", size = 1) +
   geom_text(
    data = mybox,
    mapping = aes(x = (xmin + xmax)/2,
                  y = ymin - 2000,
                  label = label),
    fill = NA, color = "black",linetype = "dashed", size = 5) 
  

ggsave(g1, filename = "viz/figure_A2.png", dpi = 500, 
         width = 9.5, height = 4.5)

```


### Other

So, it looks like Places API will give me 20 results per query. It is possible to obtain up to 60 total sites from each initial query (although it's safe to assume that each time you ask for the next 20, eg. 21-40, 41-60, that they will charge that as a query as well). So, if we're looking for pricing, it's going to be 20 results per query, each of which costs... \$0.03 dollars. (\$3 per 100 queries).

However, you get a lot of information from each request. There are two types of requests, roughly - "basic search" and "details". Basic search still delivers a bunch of information, including the *place ID,* *address*, *status of the establishment (open/closed/etc.), type of place* (eg. park, point of interest, restaurant, hotel), the *opening hours*, *etc., a rating,* and *longitude and latitude,* store in the geometry.

```{r, eval = FALSE}
# Get all data
super_query = function(mysearch){
  myquery <- google_places(
    search_string = mysearch,
    key = mykey)
  
  myquery2 <- google_places(
    search_string = mysearch,
    page_token = myquery$next_page_token,
    key = mykey)
  
  myquery3 <- google_places(
    search_string = mysearch,
    page_token = myquery2$next_page_token,
    key = mykey)
  
  # Bind results as a list and return
  list(myquery, myquery2, myquery3) %>%
    return()
}
  
result <- super_query(mysearch = "Parks in Boston, MA")

result
data.frame(
  
  # Extract latitude and longitude
  lng = myquery$results$geometry$location$lng,
  lat = myquery$resultsgeometry$location$lat,
  
)

myquery$results
myquery$results$types %>%
  map(~paste(., collapse = "; ")) %>% 
  unlist()

```


Really, the best way to do this would be to query on a grid. We *will* get overlap, because we will have to use circles that overlap slightly with a square fishnet grid, but we can use the place_IDs as a way to eliminate duplicates. And I think this is much worth the cost, because otherwise, we would almost certainly leave places out.


# 5. API Searches - Boston

Having run this initial test, we will now gather social infrastructure sites using our 2 km (most efficient) grid, and tally the results up using our 1 km grid.

This demonstration will use D. Cooley's helpful *googleway* vignette from November 2020, available [here](https://cran.r-project.org/web/packages/googleway/vignettes/googleway-vignette.html#google-places-api).

First, we have to enable the Google Maps Places API. Second, we need to get an API Key. I made it restricted to work only from RStudio Cloud and only for the Places API. Let's set it below.

## 5.1 Set Key


```{r, echo = FALSE}
# Let's load in your API key here
mykey <- "AIzaSyBCYmbJeyWhXzt0QL6haFsiW4D-r3dHcQQ"
```
```{r, eval = FALSE}
# Let's load in your API key here
mykey <- "INSERT_MY_API_KEY_HERE"
```

```{r, eval = FALSE}
# Now set the API KEY
set_key(key = mykey, api = "places")
# View which API Keys we currently have registered here
google_keys()
# If you need to use it, clear keys will clear those keys too.
#clear_keys() ## clear any previously set keys
```

This project will use the Google Maps Places API to collect geocoded point and polygon data of social infrastructure sites in the United States. For $0.03 per longitude-latitude coordinates queries, the API provides a near-complete record of the locations and meta-data of up to 20 social infrastructure sites within a 1 square-kilometer area, the average size of an Americans neighborhood (Donaldson 2013). 

The first goal is to identify the best, most cost effective strategy for mapping social infrastructure.

So, that means identifying the best keywords.

Ideally, for each 2 km<sup>2</sup> grid cell, we want to run 19 searches, for:

(1) "libraries", community center, places of worship, or city hall," 

(2) "parks, squares, or fountains," 

(3) "cafes, coffeeshops, bookstores, barbershops, or beauty salons," and 

(4) "sports field, recreation center, museums, art galleries, zoos, aquariums, gardens." 


There are three ways to search for data. First, we could use a search term, which is a query of any kind you want. This tends to return popular results, meaning it undercounts social infrastructure substantially, instead returning the most popular churches in the city, for example. Second, you could use keywords. This is highly effective, but you can only use one.

## 5.2 Compare Searches

```{r,eval = FALSE}
convert_points = function(mydata){
  mydata$results %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
    st_transform(crs = aea)  %>%
    # Grab just the distinct results
    group_by(place_id) %>%
    summarize_at(vars(name, business_status, 
                      #plus_code.compound_code, 
                      rating, types, user_ratings_total, #vicinity, 
                      #permanently_closed,
                      geometry),
                 funs(unique(.))) %>%
    ungroup() %>%
    # Classify the type
    mutate(types = .$types %>%
             map(~paste(., collapse = "; ")) %>%
             unlist()) %>%
    return()
  
}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


# Get University
neu <- data.frame(lon = c(-71.0945748), lat = c(42.3403499)) %>%
  st_as_sf(crs = wgs, coords = c("lon", "lat")) %>%
  st_transform(crs = aea)
# Located in grid cell 124



# Let's import our grid cells in Boston
mypoints <- read_rds("grid/fishnet_points.rds") %>%
  as.data.frame() %>%
  select(id, lng, lat) %>%
  filter(id == 124)

# Let's do this for all of Boston
run_string <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  search_string = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_keyword <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  keyword = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

run_type <- google_places(
  location = c(mypoints$lat[1], mypoints$lng[1]),
  place_type = "place of worship",
  radius = 770, # circumference of a circle that extends that far
  key = mykey)

dat <- bind_rows(
  run_string %>%
    convert_points() %>% mutate(id = "String Search"), 
  run_keyword %>% 
    convert_points() %>% mutate(id = "Keyword Search"), 
  run_type %>% 
    convert_points() %>% mutate(id = "Type Search"),
  neu %>%
    mutate(id = "Northeastern University")) 

ggplot() +
  geom_sf(data = points, color = "darkgrey", fill = "white",
          size = 31, shape = 21, alpha = 0.5) +
  geom_sf(data = fish, color = "black", fill = NA) +
  geom_sf(data = fish %>% filter(id == 124), fill = "darkgrey", color = "black") +
  geom_sf(data = dat, mapping = aes(color = id), size = 3, alpha = 0.5) +
  geom_sf(data = neu, size = 3, color = "black") +
  coord_sf(xlim = c(1904498, 1911007),
           ylim = c(523391, 531900)) +
  theme(panel.background = element_rect(fill = "tan"),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  labs(color = "Results by Type\nof Search",
       caption = "Note: Black point and grey cell represents 1 square kilometer range\naround Northeastern University, the center of each 770 meter radius search\nper square kilometer.") +
  ggsave("viz/figure_A1.png", dpi = 500)
```


## 5.3 Searches

Based on this, we're going to deploy 19 search terms, saved in the ```sites``` vector.

```{r, eval = FALSE}
# Public Facilities
sites <- c("library",
           "community center",
           "place of worship",
           "city hall", 
           
           # Parks & Green Space 
           "park",
           "fountain",
           "square",
           
           # Commercial
           "bookstore",
           "cafe","coffeeshop",
           "beauty salon",
           "barbershop",
           
           # Recreation
           "sports field",
           "recreation center",
           "aquarium",
           "art gallery",
           "zoo",
           "museum", 
           "garden")
# 81*0.03*18 = ~ $40
#270 * 0.03 * 18

# Area
#sqrt(2000^2 / (pi))
#1125^2*pi
# 1125 is roughly the radius of a 2 square kilometer circle

# Let's import our grid cells in Boston
mypoints <- read_rds("grid/fishnet_2_points.rds") %>%
  as.data.frame() %>%
  select(id, lng, lat)

# Gather each of these sites 1 per 2 square kilometers
for(i in 1:length(sites) ){

  print(sites[i])  
  
  # Get places of worship
  mypoints %>%
    split(.$id) %>%
    map_dfr(~google_places(
      location = c(.$lat[1], .$lng[1]),
      keyword = sites[i],
      radius = 1125, # circumference of a circle that extends that far
      key = mykey)$result, .id = "grid") %>%
    saveRDS(paste("query/", "search", sites[i], ".rds", sep = ""))
}
```

## 5.4 Process Data

```{r, eval = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


# Get University
#neu <- data.frame(lon = c(-71.0945748), lat = c(42.3403499)) %>%
#  st_as_sf(crs = wgs, coords = c("lon", "lat")) %>%
#  st_transform(crs = aea)
# Located in grid cell 124


fish <- read_rds("grid/fishnet_2.rds")
points <- read_rds("grid/fishnet_2_points.rds")


# Make a fishnet grid of counties
counties <- read_sf("county.kml") %>%
  select(-c(Name:icon)) %>%
  # Transform to equal area conic
  st_transform(crs = aea) 

# Get tracts in Boston (Suffolk County)
tracts <- read_rds("tracts_pop.rds") %>%
   # Transform to equal area conic
  st_transform(crs = aea) %>%
  filter(str_sub(geoid, 1,2) == "25") %>%
  st_join(counties %>% select(name, geometry), join = st_within) %>%
  filter(!is.na(name)) 

# Bind all searches together
data.frame(file = dir("query", full.names = TRUE)) %>%
  split(.$file) %>%
  map_dfr(~read_rds(.$file), .id = "file") %>%
  saveRDS("processed_queries/boston_sites.rds")

# Read in as an sf file
mysites <- read_rds("processed_queries/boston_sites.rds") %>%
    mutate(lat = geometry$location$lat,
           lng = geometry$location$lng) %>%
    st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
    st_transform(crs = aea)

# Count and visualize how often duplicates or triplicates, etc. occur
mysites %>%
  as.data.frame() %>%
  group_by(place_id) %>%
  count() %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  group_by(n) %>%
  count() %>%
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site\n(1 = One time, 2 = Duplicate, 3 = Triplicate, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5)) +
  ggsave("processed_queries/duplicates_barchart.png", dpi = 500, width = 6, height = 3)
 
# How often did different searches record the same places
mysites %>%
  as.data.frame() %>%
  group_by(file) %>%
  select(place_id) %>%
  distinct() %>%
  ungroup() %>%
  group_by(place_id) %>%
  count() %>%
  arrange(desc(n)) %>%
  group_by(n) %>%
  count() %>%
  ggplot(mapping = aes(x = factor(n), y = nn, label = nn)) +
  geom_col() +
  geom_text(nudge_y = 50) +
  labs(x = "Appearances of a Unique Site in Multiple Searches\n(1 = 1 Search, 2 = 2 Searches, 3 = 3 Searches, ...)", y = "Frequency (#)",
       subtitle = "Boston Google Places API Diagnostics") +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.subtitle = element_text(hjust = 0.5)) +
  ggsave("processed_queries/duplicates_by_search_barchart.png", dpi = 500, width = 6, height = 4)
# That's surprisingly good. Fewer duplicates than before, it seems.
# This means that just 42 sites appeared across multiple categories.


# Let's identify which search each site appeared in MOST;
# these will be the final sites we attribute them to.
mysites %>%
  group_by(place_id) %>%
  summarize(
    file = table(file) %>% sort(decreasing = TRUE) %>% names() %>% .[1],
    name = unique(name),
    business_status = unique(business_status),
    rating = unique(rating),
    types = unique(types),
    user_ratings_total = unique(user_ratings_total),
    permanently_closed = unique(permanently_closed),
    geometry = unique(geometry)) %>%
    ungroup() %>%
    # Classify the type
    mutate(types = .$types %>%
             map(~paste(., collapse = "; ")) %>%
             unlist()) %>%
  mutate(file = file %>% str_remove("query/search") %>% str_remove(".rds")) %>%
   mutate(group = file %>% recode_factor(
    "library" = "Community Space",
    "community center" = "Community Space",
    "city hall" = "Community Space", 
    
    
    "place of worship" = "Places of Worship",
    
    # Parks & Green Space 
    "park" = "Parks",
    "fountain" = "Parks",
    "square" = "Parks",
    "garden" = "Parks",
    
    # Commercial
    "bookstore" = "Social Businesses",
    "cafe" = "Social Businesses",
    "coffeeshop" = "Social Businesses",
    "beauty salon" = "Social Businesses",
    "barbershop" = "Social Businesses",
    
    # Recreation
    "sports field" = "Recreation & Education",
    "recreation center" = "Recreation & Education",
    "aquarium" = "Recreation & Education",
    "art gallery" = "Recreation & Education",
    "zoo" = "Recreation & Education",
    "museum" = "Recreation & Education")) %>%
  saveRDS("processed_queries/boston_points.rds")
  
```


## 5.5. Export

```{r, eval = FALSE}

# Grab the 1-square kilometer fishnet grid
fish <- read_rds("grid/fishnet.rds") %>%
  ungroup() 

# Export to KML, for use in Google Maps, Google Earth, etc.
fish %>%
  mutate(Name = paste("Cell ", id, sep = "")) %>%
  select(Name, Description = id) %>%
  st_write("grid/fishnet.kml", driver = "kml", delete_dsn = TRUE)

# Test the kml grid
read_sf("grid/fishnet.kml")  %>%
  head()

# Import the points
mypoints <- read_rds("processed_queries/boston_points.rds") %>%
  # Exclude any points that aren't directly in a cell
  st_join(fish) %>%
  filter(!is.na(id)) %>%
  select(-id)

mypoints %>%
  ungroup() %>%
  rename(Name = name) %>%
  st_write("processed_queries/boston_sites.kml", driver = "kml", delete_dsn = TRUE)
```

```{r}
# remove any extra data lying around
rm(list= ls())
```


<br>
<br>

# 6. Human Coding Study

```{r, message = FALSE, warning = FALSE}
library(tidyverse) # for data manipulation
library(tidymodels)
library(sf) # for tidy spatial data
library(rgdal)  # for spatial operations
library(tigris) # for US polygons
library(tidycensus)
library(censusapi)
# Finally, let's also get the newest version of dplyr, 
# which allows for binding sf objects
#install.packages("remotes")
#remotes::install_github("hadley/dplyr")
library(dplyr) # for data.frame manipulation, plus sf bind_rows()
library(viridis)
library(ggpubr)
library(ggspatial)
library(ggtext)
```

Next, we uploaded our data to Google MyMaps, created a human coded map, and finally returned the data from that map to R, saving it in our ```validated``` folder.

## 6.1 Import our data from Google MyMaps

```{r}
# Get sites
sites <- read_sf("validated/sites.kml")

# Get fishnet grid
grid <- read_sf("validated/milestones.kmz")

# Get neighborhood polygons
neighbor <- read_sf("validated/neighborhoods.kmz")


aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

```


We're going to select our study area for validation.

```{r}
# Import our grid cells 
fish <- read_sf("grid_covariates_tracts.kml") %>%
  select(-Name, -c(timestamp:icon)) %>%
  st_transform(crs = aea) %>%
  # Filter to just the inner-neighborhoods of Boston, which are most comparable
  filter(str_detect(milestone, "M1|M2|M3") ) %>%
  # remove cells which should be ineligible, as in 
  # they don't contain entire cells because they are coastal peninsulas
  filter(!cell_id %in% 
    paste("Cell", c(81,72,62, 160, 182, 176, 166, 165, 149,
                    89, 100, 80, 50, 62, 166, 89, 112, 129))) %>%
  # split up each variable into thirds
  mutate(pop_density_cat = ntile(pop_density, 3),
         median_income_cat = ntile(median_income, 3),
         pop_white_cat = ntile(pop_white, 3),
         pop_some_college_cat = ntile(pop_some_college, 3)) %>%
  # Grouping by neighborhood and each demographic category,
  # get the distribution of residents in the city
  group_by(neighborhood, pop_density_cat, 
           median_income_cat, pop_white_cat, 
           pop_some_college_cat) %>%
  mutate(count = n()) %>%
  ungroup() %>%
  mutate(prop = count / sum(count, na.rm = TRUE))

# Visualize these cells
ggplot() +
  geom_sf(data = fish, mapping = aes(fill = neighborhood))
```

## 6.2 Sampling Blocks

We're going to randomly sample grid cells in blocks...

```{r, eval = FALSE}

fish %>%
  as.data.frame() %>%
  select(-geometry) %>%
  infer::rep_slice_sample(n = 20, reps = 1000, replace = FALSE, weight_by = .$prop) %>%
  saveRDS("validated/grid_reps.rds")

# Please give me 20 samples, sampled based on their frequency in terms of neighborhood breakdown, population density, median income, white/non-white population, and education.

# We already did one cell - cell 124, Northeastern, 
# so it's important that our final sample include it.
# Let's identify the possible samples from our range of 1000 that include cell 124
valid <- read_rds("validated/grid_reps.rds") %>%
  filter(Name == "Cell 124") %>%
  select(replicate) %>%
  distinct()
# 454 of them did. Great. Let's investigate which of them are representative of the population.


# Get summary statistics for population
sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))


mysamples <- read_rds("validated/grid_reps.rds") %>%
  # Filter to just those which contained Northeastern.
  filter(replicate %in% valid$replicate) %>%
  # Pivot all their traits into a long vector
  select(replicate, Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 


get_test = function(var){
  print(var)
  
  mysamples %>%
    # Create a joined label
    filter(variable == var) %>%
    split(.$replicate) %>%
    map_dfr(~t_test(., response = value, mu = sumstats[, var]), .id = "replicate") %>%
    return()
}

data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", "pop_natam", "pop_asian", 
               "pop_pacific", "pop_hisplat", "pop_some_college",
               "employees_muni", "median_income",
               "income_inequality", "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  saveRDS("validated/grid_reps_t_test.rds")
```

## 6.3 Validation Area Similarity

```{r, eval = FALSE}
# Find me a sample that passed its t-test for every covariate
read_rds("validated/grid_reps_t_test.rds") %>% 
  select(replicate, variable, p_value) %>%
  pivot_wider(id_cols = replicate,
              names_from = variable,
              values_from = p_value) %>%
  filter(
    pop_density > 0.35,
    median_income > 0.35,
    pop_some_college > 0.35,
    pop_white > 0.35,
    
    pop_women > 0.35, 
    pop_age_65_plus > 0.35,
    pop_pacific > 0.35,
    pop_natam > 0.35,
    pop_hisplat > 0.35,
    pop_black > 0.35,
    pop_asian > 0.35,
    pop_unemployed > 0.35,
    income_inequality > 0.35,
    median_monthly_housing_cost > 0.35,
    employees_muni > 0.35) %>%
  saveRDS("validated/mybestsample.rds")


read_rds("validated/grid_reps.rds") %>%
  filter(replicate %in% read_rds("validated/mybestsample.rds")$replicate) %>%
  select(replicate, Name) %>%
  left_join(by = "Name", y = fish) %>%
  saveRDS("validated/sample_possibilities.rds")

test <- bind_rows(
  # Make sure it includes either Beacon Hill or Back Bay
  read_rds("validated/sample_possibilities.rds") %>%
    filter(Name == "Cell 173" | Name == "Cell 161") %>%
    select(replicate)) %>%
  distinct() %>% unlist() %>%
  sample(size = 9)

pos <- read_rds("validated/sample_possibilities.rds") %>%
  filter(replicate %in% test) %>%
  st_as_sf()
#95, 802,396, 578


g2 <- ggplot() +
  geom_sf(data = fish, fill = "darkgrey", color = "white") +
  geom_sf(data = pos, 
          mapping = aes(fill = neighborhood, 
                                     linetype = "Sampled", color = "Sampled"), size = 1.15)  +
  geom_sf_text(data = pos,
          mapping = aes(label = Name %>% str_remove("Cell ")), color = "black", size = 3) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5)) +
  scale_fill_viridis(option = "plasma", discrete = TRUE) +
  scale_color_manual(values = "black") +

  guides(color = "none") +
  labs(linetype = "Ground-truthed by:",
       fill = "Neighborhood",
       caption = "Numbers indicate unique ID numbers of each grid cell.",
       subtitle = "1 sq.km. Blocks of Core Boston (n = 73 / 168)") +
  facet_wrap(~replicate) 
  
ggsave(g2, filename = "validated/best_options.png", dpi = 500, 
       width = 9, height = 8)


# 366
# Filter to the final sample
read_rds("validated/grid_reps.rds") %>%
  filter(replicate == 366) %>%
  saveRDS("validated/sample20.rds")

remove(mysamples, mystats, valid)
```

## 6.4 Divide Masters & Undergrad Samples

Next, we collected a random stratified sample of 20 grid cells, and conducted ground truthing in each grid cell.

To do so, we split up the population of grid cells geographically by X neighborhoods, and then in terms of four additional key demographic traits, including population density, median income, the white/non-white population, and % residents with some college education. For each variable, we binned grid cells into 3 equally sized quantiles. Then, we calculated the proportion of grid cells which fit into each strata. (An example strata might be Jamaica Plains, upper 3rd share in terms of population density, lower 3rd share of median income, upper 3rd share of non-white residents, middle 3rd share of college educated residents.) Finally, we used these proportions as sampling probabilities, and took 1000 different samples of 20 grid cells using these probabilities, to ensure that in each of these 1000 different possible samples, our samples would reflect the frequency of these categories in the population at large.

The final sample of 20 grid cells was representative of the core Boston area in terms of 15 key demographic traits (one sample t-tests found p > 0.50 for each, which means they were extremely close to the population mean). When we zoomed out to compare against all neighborhoods of Boston, they were representative in terms of 14 key demographic traits (just education was slightly different, p < 0.05, but not by much). When we zoomed out to compare against every single cell, even including islands, they were representative in terms of 13 key demographic traits (just education and income inequality were slightly different, p < 0.05, but not by much).

Finally, I randomly assigned 10 of these grid cells to masters students and 10 grid cells to undergraduates.

```{r, eval = FALSE}
mine <- read_rds("validated/grid_covariates.rds") %>%
  mutate(sampled = if_else(Name %in% read_rds("validated/sample20.rds")$Name,
                           true = "yes", false = "no")) %>%
  group_by(sampled) %>%
  mutate(type = sample(x = c("masters", "undergrad"), size = n(), 
                       prob = c(0.5, 0.5), replace = TRUE),
         type = if_else(sampled == "no", true = NA_character_, false = type))

mine %>%
  filter(sampled == "yes") %>%
  select(Name, neighborhood, type) %>%
  group_by(type) %>% count()

mine %>%
  saveRDS("validated/grid_final.rds")

remove(mine)
```



## 6.5 Represenativeness

How similar is this to the population overall - spanning all grid cells?

```{r}
fish <- read_sf("grid_covariates_tracts.kml") %>% 
  select(-Name, -c(timestamp:icon)) %>%
  #filter(str_remove(cell_id, pattern = "Cell ") %in% read_rds("boston_grid.rds")) 
  # Exclude the cells which barely contain any of Boston in them
  filter(str_detect(milestone, pattern = "M"))

myvars <- c("pop_density", "pop_women", "pop_white",
            "pop_black", 
            "pop_asian", 
            "pop_hisplat", "pop_some_college",
            "median_income",
            "pop_unemployed",
            "median_monthly_housing_cost",
            "pop_age_65_plus")
```

### All Boston - Core Boston 

```{r}
mysample <- fish %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  # Pivot all their traits into a long vector
  select(cell_id, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_core <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", 
               "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```

### All Boston - 20 blocks 

```{r}
mysample <- fish %>%
  filter(cell_id %in% read_rds("validated/sample20.rds")$Name) %>%
  # Pivot all their traits into a long vector
  select(cell_id, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_all20 <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", 
               "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```

### Core Boston - 20 blocks

```{r}
sumstats <- fish %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))


results_core20 <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black",
               "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```

### All Boston - 10 Masters blocks

```{r}
mysample <- read_rds("validated/grid_final.rds") %>%
  filter(sampled == "yes", type == "masters") %>%
  # Pivot all their traits into a long vector
  select(Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_allmasters <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```


### All Boston - 10 Undergrad blocks

```{r}
mysample <- read_rds("validated/grid_final.rds") %>%
  filter(sampled == "yes", type == "undergrad") %>%
  # Pivot all their traits into a long vector
  select(Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_allundergrad <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black",  "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White", "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```




### Core Boston - 10 Masters blocks

```{r}
mysample <- read_rds("validated/grid_final.rds") %>%
  filter(sampled == "yes", type == "masters") %>%
  # Pivot all their traits into a long vector
  select(Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_coremasters <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black",  "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```


### Core Boston - 10 Undergrad blocks

```{r}
mysample <- read_rds("validated/grid_final.rds") %>%
  filter(sampled == "yes", type == "undergrad") %>%
  # Pivot all their traits into a long vector
  select(Name, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") 

sumstats <- fish %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  as.data.frame() %>%
  summarize_at(vars(myvars), funs(mean(., na.rm = TRUE)))

get_test = function(var){
  print(var)
  
  mysample %>%
    # Create a joined label
    filter(variable == var) %>%
    infer::t_test(response = value, mu = sumstats[, var]) %>%
    return()
}

results_coreundergrad <- data.frame(
  variable = c("pop_density", "pop_women", "pop_white",
               "pop_black", "pop_asian", 
               "pop_hisplat", "pop_some_college",
               "median_income",
               "pop_unemployed",
               "median_monthly_housing_cost",
               "pop_age_65_plus")) %>%
  split(.$variable) %>%
  map_dfr(~get_test(.$variable), .id = "variable") %>%
  left_join(by = "variable",
            y = sumstats %>%
              pivot_longer(cols = -c(),
                           names_to = "variable",
                           values_to = "pop")) %>%
  mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White", "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```


```{r}
mypop <- fish %>%
  as.data.frame() %>%
   # Pivot all their traits into a long vector
  select(cell_id, neighborhood, myvars) %>%
  pivot_longer(cols = all_of(myvars),
    names_to = "variable", values_to = "value") %>%
    mutate(variable = variable %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White", "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost"))
```

```{r}
bind_rows(
    results_core %>% mutate(type = "Core Boston", class = "Boston"),
  results_all20 %>% mutate(type = "20 Sampled Cells", class = "Boston"),
  results_allmasters %>% mutate(type = "10 Masters Cells", class = "Boston"),
  results_allundergrad %>% mutate(type = "10 Undergrad Cells", class = "Boston"),
  results_core20 %>% mutate(type = "20 Sampled Cells", class = "Core Boston"),
  results_coremasters%>% mutate(type = "10 Masters Cells", class = "Core Boston"),
  results_coreundergrad %>% mutate(type = "10 Undergrad Cells", class = "Core Boston")
) %>%
  mutate(sig = if_else(p_value < 0.05, "Significant\nDifference", 
                       "No Major\nDifferences")) %>%
  ggplot(mapping = aes(x = class, y = variable,
                       fill = statistic, color = sig, label = round(p_value, 3))) + 
  facet_grid(~type, scales = "free") +
  geom_tile() +
  geom_text() +
  scale_color_manual(values = c("darkgrey", "black"),
                     guide = guide_legend(order = 2, 
                                          override.aes = list(fill = "white"))) +
  scale_fill_gradient2(low = "#DC267F", high = "#648FFF", mid = "white", midpoint = 0)
```



```{r}
results <- bind_rows(
  results_core %>% mutate(type = "Validation Area", class = "Boston"),
  results_all20 %>% mutate(type = "20 Sampled Cells", class = "Boston"),
  results_allmasters %>% mutate(type = "10 Ground-Truthed Cells", class = "Boston")
) %>%
  mutate(group = paste(type, variable))

results %>%
  saveRDS("validated/representativeness.rds")

g1 <- read_rds("validated/representativeness.rds") %>%
  ggplot(mapping = aes(x = variable, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci, 
                       color = type, group = group)) +
  geom_jitter(data = mypop, 
              mapping = aes(x = variable, y = value,
                            ymin = NA_real_, ymax = NA_real_,
                            color = NA, group = NA), 
              alpha = 0.2, color = "black") +
  geom_violin(data = mypop, 
              mapping = aes(x = variable, y = value,
                            ymin = NA_real_, ymax = NA_real_,
                            color = NA, group = NA), 
              alpha = 0.75, fill = "darkgrey", color = "black") +
  
  geom_point(position = position_dodge2(width = 1), alpha = 0.75, size = 5,
             shape = 22, fill = "black", 
             mapping = aes(x = variable, y = pop, 
                                       ymin = NA_real_, ymax = NA_real_,
                           color = "Boston (Population)")) +
  geom_linerange(position = position_dodge2(width = 1), alpha = 0.95, size = 2) +
  geom_point(position = position_dodge2(width = 1), alpha = 0.95, size = 5) +
  facet_wrap(~variable, scales = "free", ncol = 2) +
  theme_bw(base_size = 14) +
  theme(strip.text = element_blank(), 
        strip.background = element_blank(),
        legend.position = c(0.8, 0.03), 
        legend.box.margin = margin(0, 0, 0, 0, "cm")) +
  coord_flip() +
  scale_color_manual(values = c("#648FFF", "#785EF0","#DC267F", "black"),
                     breaks = c("10 Ground-Truthed Cells","20 Sampled Cells", 
                                "Validation Area","Boston (Population)")) +
  scale_linetype_manual(values = c("solid")) +
  guides(color = guide_legend(override.aes = list(
    
    shape = c(21, 21, 21, 22),
    fill = c("#648FFF", "#785EF0","#DC267F","black")))) +
  theme(plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        plot.subtitle = element_text(hjust = 0.5)) +
  labs(color = NULL, linetype = NULL,
       y = "Traits of Grid Cells,   \naveraged from Census Tracts", x = NULL,
       subtitle = "Representativeness in Validation Samples\nSample Mean vs. Population Mean (95% Sample Confidence Intervals)")

ggsave(g1, filename = "viz/figure_A6.png",
       dpi = 500, width = 7.5, height = 7)

g2 <- g1 + labs(
caption = "<b>Note:</b> <i>Validation Area</i> describes 73 contiguous grid cells (1 km2) in 10 core<br>Boston neighborhoods. <i>20 cells</i> sampled from it via random stratified sample;<br><i>10 cells</i> randomly sampled from 20-cell sample for ground-truthing.<br><i>Violins and points</i> depict distribution of traits in complete grid.
       <br>
       <br>
       <b>Representativeness: </b>One-sample t-tests reveal that each sample is largely<br>representative of complete Boston grid. Minor differences detected in 3 areas: <br><i>Validation area</i> was slightly below population in terms of (1) college education.<br><i>10 ground-truthing cells</i> saw slightly below population levels of (2) college education<br>and (3) Hispanic/Latino residents (p < 0.05). Error bars show that these differences,<br>while statistically significant, are extremely minor.")

ggsave(g2, filename = "validated/representativeness.png",
       dpi = 500, width = 7.5, height = 9)
```

Very good. Just some slight differences in terms of education and ethnicity, but not much.


## 6.7 Get points for Map Marker

```{r, eval = FALSE}

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

sites <- read_sf("validated/sites_2021_10_23.kml") %>%
  st_transform(crs= aea) %>%
  # Join in sampled grid cells
  st_join(read_rds("validated/grid_final.rds") %>%
            filter(sampled == "yes") %>%
            select(-description, -Name)) %>%
  # Filter to just points in sampled grid cells
  filter(sampled == "yes") %>%
  # remove extraneous
  filter(status %in% c("Checked", "Checked ", "Been there", "New Sites", "Double")) %>%
  mutate(status = case_when(
    status == "Checked" | status == "Checked " ~ "Checked",
    
    TRUE ~ status))


sites %>%
  select(Name, type, status, group, neighborhood, place_id) %>%
  arrange(type) %>%
  write_sf("validated/sites_in_sampled_cells.kml")
```


## 6.6 Export Map Marker Files

```{r, eval = FALSE}
read_rds("validated/grid_final.rds") %>%
  filter(sampled == "yes") %>%
  mutate(description = paste(type, "\n", "neighborhood: ", neighborhood, sep = "")) %>%
  write_sf("validated/grid_samples_for_ground_truthing.kml")

#read_sf("validated/grid_samples_for_ground_truthing.kml")
```

```{r}
read_rds("validated/grid_final.rds") %>%
  filter(type == "undergrad") %>%
  mutate(Name = str_remove(Name, "Cell ") %>% as.numeric()) %>%
  arrange(Name) %>%
  select(Name, neighborhood, milestone, type)
```

```{r, eval = FALSE}

mine <- read_rds("validated/grid_final.rds") %>%
  ungroup() %>%
# Filter to just the inner-neighborhoods of Boston, which are most comparable
  filter(str_detect(milestone, "M[1-4]{1}") ) 


g1 <- ggplot() +
  geom_sf(data = mine, fill = "darkgrey", color = "white") +
  geom_sf(data = mine %>%
            filter(str_detect(milestone, "M4", negate = TRUE)), 
          mapping = aes(fill = neighborhood, 
                                     linetype = type, color = type), size = 1.15)  +
  geom_sf_text(data = mine %>%
                 filter(str_detect(milestone, "M4", negate = TRUE)),
          mapping = aes(label = Name %>% str_remove("Cell ")), color = "black") +
  theme_void(base_size = 14) +
  scale_fill_viridis(option = "plasma", discrete = TRUE) +
  scale_color_manual(
    breaks = c("masters", "undergrad", NA_character_),
    labels = c("Masters", "Undergrads", "Not Sampled"),
    values = c("black", "black", "black")) +
  scale_linetype_manual(
    breaks = c("masters", "undergrad", NA_character_),
    labels = c("Masters", "Undergrads", "Not Sampled"),
    values = c("solid", "dotted", "blank")) +
  guides(color = "none") +
  labs(linetype = "Ground-truthed by:",
       fill = "Neighborhood",
       caption = "Numbers indicate unique ID numbers of each grid cell.",
       subtitle = "1 sq.km. Blocks of Core Boston (n = 73 / 168)") +
  ggspatial::annotation_north_arrow(height = unit(0.5, "cm"), width = unit(0.5, "cm")) +
  ggspatial::annotation_scale(height = unit(0.25, "cm"), pad_x = unit(1, "cm")) +
  theme(plot.background = element_rect(fill = "white", color = NA)) +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))

ggsave(g1, filename = "validated/plan.png", dpi = 500, width = 6, height = 8)
```




# 7. Descriptives

## 7.1 Point Map

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Grab the 1-square kilometer fishnet grid
fish <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = aea) %>%
  # Filter to cells within boston
  filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid.rds"))


mypoints <- read_rds("processed_queries/boston_points.rds") %>%
  st_transform(crs = aea) %>%
  # Exclude any points that aren't directly in a cell
  st_join(fish %>% select(cell_id, geometry)) %>%
  filter(!is.na(cell_id)) 


# Make a fishnet grid of counties
counties <- read_sf("county.kml") %>%
  select(geometry) %>%
  # Transform to equal area conic
  st_transform(crs = aea) 

# Get tracts in Boston (Suffolk County)
tracts <- read_rds("tracts_boston.rds") %>%
  st_transform(crs = aea)

# Get Boston Outline
boston <- tracts %>%
  summarize(geometry = st_union(geometry))

library(tidyverse)
library(ggtext)
library(magick)
library(viridis)
library(sf)
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Grab t
# Let's import a little airport image to make 
# it really clear why we don't map that area
airport <- data.frame(x =  -71.0108, 
           y =  42.36255,
           x_aea = 1912748,
           y_aea = 532544.3,
           image = "dashdata/airport.png")
#airport %>%
#  mutate(geometry = paste("POINT(", x, " ", y, ")", sep = "")) %>%
#  st_as_sf(wkt = "geometry", crs = wgs) %>%
#  st_transform(crs = aea)

# Import labels
areas <- read_rds("dashdata/key_areas.rds") %>%
  st_transform(crs = aea)

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)


g1 <- ggplot() +
  geom_sf(data = boston, color = "darkgrey",size = 5) +
  geom_sf(data = boston, color = "black",fill = "#ffe3be", size = 0.5) +
  geom_sf(data = fish, mapping = aes(linetype = "1 km<sup>2</sup> grid cell"),
          fill = "grey", color = "white", size = 0.5, alpha = 0.5) +
  geom_sf(data = mypoints %>%
            filter(group != "Recreation & Education") %>%
            mutate(group = group %>% recode_factor(
              "Places of Worship" = "Places of Worship<br>(n = 224)",
              "Community Space" = "Community Space<br>(n = 225)",
              "Social Businesses" = "Social Businesses<br>(n = 225)",
               "Parks" = "Parks<br>(n = 534)") %>%
                factor(levels = levels(.) %>% rev())), 
          mapping = aes(fill = group), shape = 21, color = "white", size = 3, alpha = 0.75) + 
  geom_image(data = airport, aes(x = x_aea, y = y_aea, image= image), 
             size = 0.05) +
  #geom_sf(data = boston, color = "black",size = 0.5, fill = NA) +
  scale_color_manual(values = c(mycolors)) +
  scale_linetype_manual(values = "solid") +
  scale_fill_manual(values = c(mycolors)) +
  guides(fill = guide_legend(
    order = 1, 
    override.aes = list(
      size = 5, 
      shape = 21, color = "black"),
    linetype = guide_legend(
      order = 2, 
      override.aes = list(shape = 22, alpha = 0.75, size = 5))),
    color = FALSE) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0.5),
        #panel.border = element_rect(fill = NA, color = "black"),
        plot.background = element_rect(fill = "white", color = "white"),
        legend.position  = c(0.77, 0.20),
        legend.text = element_markdown(size = 14),
        legend.background = element_rect(fill = "white", color = NA)) +
  labs(subtitle = "Social Infrastructure Sites (n = 1208) in Boston",
       fill = NULL,linetype = NULL) +
    ggspatial::annotation_north_arrow(height = unit(0.75, "cm"), width = unit(0.75, "cm")) +
  ggspatial::annotation_scale(height = unit(0.25, "cm"), pad_x = unit(2, "cm"))  +
  geom_sf_label(data = areas, mapping = aes(label = neighborhood),
                fill = "white", color = "black", alpha = 0.75, size = 5)

ggsave(g1, filename = "viz/figure_1.png", dpi = 750, width = 6, height = 7.5)


g1b <- g1 + 
       labs(caption = "All sites sourced from Google Places API searches<br>across populated census tracts, tallied by fishnet grid of 1 km<sup>2</sup> cells.
       <br>
       Several well-known neighborhoods labeled as geographic reference points.")

ggsave(g1, filename = "viz/boston_sites.png", dpi = 750, width = 6, height = 8)

```

## 7.2 Calculate Rates

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Grab the 1-square kilometer fishnet grid
fish <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = aea) %>%
  # Filter to cells within boston
  filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid.rds")) 


mypoints <- read_rds("processed_queries/boston_points.rds") %>%
  st_transform(crs = aea) %>%
  # Exclude any points that aren't directly in a cell
  st_join(fish %>% select(cell_id, geometry)) %>%
  filter(!is.na(cell_id)) %>%
  select(-cell_id)


# Make a fishnet grid of counties
counties <- read_sf("county.kml") %>%
  select(geometry) %>%
  # Transform to equal area conic
  st_transform(crs = aea) 

# Get tracts in Boston (Suffolk County)
tracts <- read_rds("tracts_boston.rds") %>%
  st_transform(crs = aea)



# How many points per cell would we expect if things were distributed equally?
fish %>%
  st_join(mypoints) %>%
  group_by(cell_id) %>%
  summarize(
#    sites = sum(!is.na(group)),
    # Since we're not going to count museums, ditch this category.
    sites = sum(!is.na(group) & file != "museum"),
    # Commmunity Space
    community = sum(group == "Community Space", na.rm = TRUE),
    library = sum(file == "library", na.rm = TRUE),
    community_center = sum(file == "community center", na.rm = TRUE),
    city_hall = sum(file == "city hall", na.rm = TRUE),
    
    place_of_worship = sum(file == "place of worship", na.rm = TRUE),
    # Parks
    parks = sum(group == "Parks", na.rm = TRUE),
    park = sum(file == "park", na.rm = TRUE),
    fountain = sum(file == "fountain", na.rm = TRUE),
    square = sum(file == "square", na.rm = TRUE),
    # Social Business
    social = sum(group == "Social Businesses", na.rm = TRUE),
    
    bookstore = sum(file == "bookstore", na.rm = TRUE),
    cafe = sum(file == "cafe", na.rm = TRUE),
    coffeeshop = sum(file == "coffeeshop", na.rm = TRUE),
    beauty_salon = sum(file == "beauty salon", na.rm = TRUE),
    barbershop = sum(file == "barbershop", na.rm = TRUE),
    # Recreation
    sports_field = sum(file == "sports field", na.rm = TRUE),
    recreation_center = sum(file == "recreation center", na.rm = TRUE),
    aquarium = sum(file == "aquarium", na.rm = TRUE),
    art_gallery = sum(file == "art gallery", na.rm = TRUE),
    zoo = sum(file == "zoo", na.rm = TRUE),
    museum = sum(file == "museum", na.rm = TRUE),
    garden = sum(file == "garden", na.rm = TRUE),
    
    rec = sum(group == "Recreation & Education", na.rm = TRUE),
    # Keep interpolated population density
    pop_density = unique(pop_density_int),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  # Calculate rate of sites per 1000 persons in the grid cell
  mutate_at(vars(sites:rec), funs(. / pop_density * 1000) ) %>%
  mutate_at(vars(sites:rec), funs(if_else(is.infinite(.) | is.nan(.),
                                          NA_real_, .))) %>%
  saveRDS("processed_queries/sites_crosstab.rds")

# Get cross-table of all sites in Boston
cross <- read_rds("processed_queries/sites_crosstab.rds")
```


## 7.3 Descriptive Table

```{r}
mypoints %>%
  as.data.frame() %>%
  group_by(file) %>%
  count()
```

```{r}
cross %>% head()

```

```{r}
library(ggtext)
library(viridis)
library(tidyverse)

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)

g1 <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(sites:rec),
               names_to = "type", values_to = "value") %>%
  filter(!type %in% c("sites", "community", "parks", "social", "rec")) %>%
  group_by(type) %>%
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            min = min(value, na.rm = TRUE),
            p25 = quantile(value, probs = 0.25, na.rm = TRUE),
            median = median(value, na.rm = TRUE),
            p75 = quantile(value, probs = 0.75, na.rm = TRUE),
            max = max(value, na.rm = TRUE),
            total = sum(value, na.rm = TRUE),
            cells = sum(value > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(mean:cells),
               names_to = "class", values_to = "stat") %>%
  mutate(class = class %>% recode_factor(
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25%", "median" = "50%", 
    "p75" = "75%", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells\n(any)")) %>%
  mutate(group = type %>% recode_factor(
    "library" = "community_space",
    "city_hall" = "community_space",
    "community_center" = "community_space",
    "place_of_worship" ="place_of_worship",
    "bookstore" = "social_business", 
    "cafe" = "social_business", 
    "coffeeshop" = "social_business", 
    "barbershop" = "social_business", 
    "beauty_salon" = "social_business", 
    "park" = "park", 
    "fountain" = "park", 
    "square" = "park", 
    "garden" = "park", 
    "recreation_center" = "park", 
    "sports_field" = "park",
    .default = "other"),
    group = group %>%
      recode_factor(
        "community_space" = "<b>Community<br>Spaces</b>",
        "place_of_worship" = "<b>Places<br>of Worship</b>", 
       "social_business"  = "<b>Social<br>Businesses</b>", 
      "park" =  "<b>Parks</b>", 
      "other" = "<b>Other</b>")) %>%
  mutate(type = type %>% recode_factor(
    #"sites" = "Total Sites\n(n = 1318)",
    "library" = "<b>Libraries</b>\n(n = 108)",
    "city_hall" = "<b>City Hall Facilities</b>\n(n = 43)",
    "community_center" = "<b>Community Centers</b>\n(n = 74)",
    "place_of_worship" = "<b>Places of Worship</b>\n(n = 224)",
    
    "bookstore" = "<b>Bookstores</b>\n(n = 62)",
    "cafe" = "<b>Cafes</b>\n(n = 163)",
    "coffeeshop" = "Coffeeshop\n(n = 0)",
    
    "barbershop" = "Barbershops\n(n = 0)",
    "beauty_salon" = "Beauty Salons\n(n = 0)",
    
    "park" = "<b>Parks</b>\n(n = 288)",
    "fountain" = "<b>Fountains</b>\n(n = 34)",
    "square" = "<b>Squares</b>\n(n = 200)",
    
    "garden" = "<b>Gardens</b>\n(n = 12)",
    "recreation_center" = "Recreation Center\n(n = 0)",
    "sports_field" = "Sports Fields\n(n = 0)", 
    "aquarium" = "Aquariums\n(n = 0)",
    "art_gallery" = "Art Galleries\n(n = 0)", 
    "museum" = "Museum\n(n = 1)",
    "zoo" = "Zoo\n(n = 0)"),
    type = factor(type, levels = levels(type) %>% rev())) %>% 
  mutate(coverage = if_else(str_detect(type, "n = 0|n = 1[)]"),
                             "low", "high")) %>%
  ggplot(mapping = aes(x = class, y = reorder(type,stat), label = round(stat, 1),
                       fill = coverage, color = coverage)) +
  geom_tile(color = "grey") +
  geom_text() +
  scale_fill_manual(values = c("darkgrey", "white"),
                    breaks = c("high", "low"),
                    labels = c("High<br>Coverage", "Minimal<br>Coverage")) +
  scale_color_manual(values = c("black", "darkgrey")) +
  guides(color = FALSE) +
  facet_grid(rows = vars(group),  scales = "free", space = "free", shrink = TRUE) +
  scale_x_discrete(position = "top") +
  theme_classic(base_size = 14) +
  theme(plot.caption = ggtext::element_markdown(size = 12, hjust = 0.5),
        legend.position = "none",
        legend.text = ggtext::element_markdown(size = 12),
        strip.text.y = ggtext::element_markdown(size = 11, angle = 0, color = "white"),
        strip.background = element_rect(fill = c("black")),
        panel.border = element_rect(fill = NA, color = "black"),
        axis.text.y = element_markdown(size = 12)) +
  labs(caption = "Note: Rates measured per 1000s persons per 1 km<sup>2</sup>.<br>Keywords which returned high rates of social infrastructure highlighted and shaded <b><span style='color:darkgrey'>grey</b></span>.<br><b>Total</b> depicts total rate of each type of social infrastructure in Boston, per 1000 residents.<br><b>Cells</b> describes total cells with any of that type found.",
       y = NULL, x = "Descriptive Statistics of API-generated Social Infrastructure Rates")

g <- ggplot_gtable(ggplot_build(g1))
strip_r <- which(grepl('strip-r', g$layout$name))
fills <- c(mycolors, "darkgrey")

k <- 1
for (i in strip_r) {
  j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
  g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
  k <- k+1
}
library(ggpubr)

ggsave(as_ggplot(g), filename = "viz/table_A4.png", dpi = 500, width = 8.5, height = 7)
```


Let's repeat that now, but just for category subtotals.

```{r}

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"
dat <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(sites, community, social, parks, place_of_worship),
               names_to = "type", values_to = "value") %>%
  group_by(type) %>%
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            min = min(value, na.rm = TRUE),
            p25 = quantile(value, probs = 0.25, na.rm = TRUE),
            median = median(value, na.rm = TRUE),
            p75 = quantile(value, probs = 0.75, na.rm = TRUE),
            max = max(value, na.rm = TRUE),
            total = sum(value, na.rm = TRUE),
            cells = sum(value > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(mean:cells),
               names_to = "group", values_to = "stat") %>%
  mutate(group = group %>% recode_factor(
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25th %", "median" = "50th %", 
    "p75" = "75th %", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells\nwith Sites")) %>%
  mutate(type = type %>% recode_factor(
    "sites" = "<b>Total Sites</b><br>(n = 1208)<br>",
    "community" = "<b>Community Spaces</b><br>(n = 225)<br>",
    "place_of_worship" = "<b>Places of Worship</b><br>(n = 224)<br>",
    "social" = "<b>Social Businesses</b><br>(n = 225)<br>",
    "parks" = "<b>Parks</b><br>(n = 534)<br>"),
    type = factor(type, levels = levels(type) %>% rev())) 

g1 <- dat %>%
  ggplot(mapping = aes(
    x = group, y = type, 
    fill = type,color = type,
    label = paste("<b>",round(stat, 1), "</b>", sep = ""))) +
  scale_fill_manual(values = c(rev(mycolors), "black")) +
  guides(fill = "none", color = "none") +
  geom_tile(color = "white") +
  geom_richtext(label.color = NA, color = "white") +
  scale_x_discrete(position = "top") +
  theme_classic(base_size = 14) +
  theme(plot.caption = ggtext::element_markdown(size = 10, hjust = 0),
        axis.text.y = ggtext::element_markdown(size = 14),
        panel.border = element_rect(fill = NA, color = "black")) +
  labs(caption = "Note: Rates measured per 1000s persons per 1 km<sup>2</sup>.<br><b>Total</b> depicts total rate of each type of social infrastructure in Boston, per 1000 residents.<br><b>Cells</b> describes total cells with any of that type found.",
       y = NULL, x = "Descriptive Statistics of API-generated Social Infrastructure Rates") 

ggsave(g1, filename = "viz/table_1.png", dpi = 500, width = 8, height = 3.5)

rm(list = ls())
```

#### Table 1

```{r}
library(knitr)
library(kableExtra)

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"

dat <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(sites, community, social, parks, place_of_worship),
               names_to = "type", values_to = "value") %>%
  group_by(type) %>%
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            min = min(value, na.rm = TRUE),
            p25 = quantile(value, probs = 0.25, na.rm = TRUE),
            median = median(value, na.rm = TRUE),
            p75 = quantile(value, probs = 0.75, na.rm = TRUE),
            max = max(value, na.rm = TRUE),
            total = sum(value, na.rm = TRUE),
            cells = sum(value > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(mean:cells),
               names_to = "group", values_to = "stat") %>%
  mutate(group = group %>% recode_factor(
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25th %", "median" = "50th %", 
    "p75" = "75th %", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells with Sites")) %>%
  mutate(type = type %>% recode_factor(
    "sites" = "<b>Total Sites</b><br>(n = 1208)<br>",
    "community" = "<b>Community Spaces</b><br>(n = 225)<br>",
    "place_of_worship" = "<b>Places of Worship</b><br>(n = 224)<br>",
    "social" = "<b>Social Businesses</b><br>(n = 225)<br>",
    "parks" = "<b>Parks</b><br>(n = 534)<br>"),
    type = factor(type, levels = levels(type) %>% rev())) 

tab <- dat %>%
  pivot_wider(id_cols = type, names_from = group, values_from = stat) %>%
  mutate_at(vars(Mean:Total), list(~round(., 1))) %>%
  # Get order of rows from factor
  mutate(order = type %>% as.numeric()) %>%
  arrange(desc(order)) %>%
  select(-order) %>%
  # Extract number of observations & title from HTML
  mutate(nobs = type %>% str_extract(pattern = "[0-9]+")) %>%
  mutate(type = type %>% str_extract(pattern = paste(c("Total Sites", "Community Spaces", "Places of Worship", "Social Businesses", "Parks"), collapse = "|"))) %>%
  mutate(` ` = "") %>%
  select(` `, Type = type, Sites = nobs, Mean:Total, "Cells with Sites")

tab %>%
  kable(format = "latex", booktabs = TRUE, 
        caption = "Descriptive Statistics of API-generated Social Infrastructure Rates") %>%
  kable_styling() %>%
  row_spec(1, bold = T, color = "white", background = "black") %>%
  row_spec(2, bold = T, color = "white", background = mycolors[1]) %>%
  row_spec(3, bold = T, color = "white", background = mycolors[2]) %>%
  row_spec(4, bold = T, color = "white", background = mycolors[3]) %>%
  row_spec(5, bold = T, color = "white", background = mycolors[4]) %>%
  column_spec(1, background = "white", image = spec_image(
    c("thumbnails/hands-helping-grey.png", "thumbnails/store-alt-blue.png",
      "thumbnails/place-of-worship-purple.png",
      "thumbnails/coffee-red.png", "thumbnails/tree-yellow.png"),
    width = 50, height = 50)) %>%
  column_spec(2, background = "white", color = "black") %>%
  cat()

#column_spec(2, image = spec_image(
#c("kableExtra_sm.png", "kableExtra_sm.png"), 50, 50))
```

### Table A4

```{r}
dat <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(sites:rec),
               names_to = "type", values_to = "value") %>%
  filter(!type %in% c("sites", "community", "parks", "social", "rec")) %>%
  group_by(type) %>%
  summarize(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            min = min(value, na.rm = TRUE),
            p25 = quantile(value, probs = 0.25, na.rm = TRUE),
            median = median(value, na.rm = TRUE),
            p75 = quantile(value, probs = 0.75, na.rm = TRUE),
            max = max(value, na.rm = TRUE),
            total = sum(value, na.rm = TRUE),
            cells = sum(value > 0, na.rm = TRUE)) %>%
  pivot_longer(cols = c(mean:cells),
               names_to = "class", values_to = "stat") %>%
  mutate(class = class %>% recode_factor(
    "mean" = "Mean",
    "sd" = "Std. Dev.",
    "min" = "Min", "p25" = "25%", "median" = "50%", 
    "p75" = "75%", "max" = "Max",
    "total" = "Total",
    "cells" = "Cells (any)")) %>%
  mutate(group = type %>% recode_factor(
    "library" = "community_space",
    "city_hall" = "community_space",
    "community_center" = "community_space",
    "place_of_worship" ="place_of_worship",
    "bookstore" = "social_business", 
    "cafe" = "social_business", 
    "coffeeshop" = "social_business", 
    "barbershop" = "social_business", 
    "beauty_salon" = "social_business", 
    "park" = "park", 
    "fountain" = "park", 
    "square" = "park", 
    "garden" = "park", 
    "recreation_center" = "park", 
    "sports_field" = "park",
    .default = "other"),
    group = group %>%
      recode_factor(
        "community_space" = "Community Spaces",
        "place_of_worship" = "Places of Worship", 
       "social_business"  = "Social Businesses", 
      "park" =  "Parks", 
      "other" = "Other")) %>%
  mutate(type = type %>% recode_factor(
    #"sites" = "Total Sites\n(n = 1318)",
    "library" = "Libraries\n(n = 108)",
    "city_hall" = "City Hall Facilities\n(n = 43)",
    "community_center" = "Community Centers\n(n = 74)",
    "place_of_worship" = "Places of Worship\n(n = 224)",
    
    "bookstore" = "Bookstores\n(n = 62)",
    "cafe" = "Cafes\n(n = 163)",
    "coffeeshop" = "Coffeeshop\n(n = 0)",
    
    "barbershop" = "Barbershops\n(n = 0)",
    "beauty_salon" = "Beauty Salons\n(n = 0)",
    
    "park" = "Parks\n(n = 288)",
    "fountain" = "Fountains\n(n = 34)",
    "square" = "Squares\n(n = 200)",
    
    "garden" = "Gardens\n(n = 12)",
    "recreation_center" = "Recreation Center\n(n = 0)",
    "sports_field" = "Sports Fields\n(n = 0)", 
    "aquarium" = "Aquariums\n(n = 0)",
    "art_gallery" = "Art Galleries\n(n = 0)", 
    "museum" = "Museum\n(n = 1)",
    "zoo" = "Zoo\n(n = 0)"),
    type = factor(type, levels = levels(type) %>% rev())) %>% 
  mutate(coverage = if_else(str_detect(type, "n = 0|n = 1[)]"),
                             "low", "high")) %>%
  pivot_wider(id_cols = c(group, type), names_from = class, values_from = stat) %>%
  mutate_at(vars(Mean:Total), list(~round(., 1))) %>%
  # Get order of rows from factor
  mutate(type_order = type %>% as.numeric(),
         group_order = group %>% as.numeric()) %>%
  # order
  arrange(desc(type_order), group_order) %>%
 # Extract number of observations & title from HTML
  mutate(nobs = type %>% str_extract(pattern = "[0-9]+"),
         type = type %>% str_extract(".*\n") %>% str_remove("\n")) %>%
  mutate(` ` = "") %>%
  select(group, ` `, Type = type, Sites = nobs, Mean, SD = `Std. Dev.`, Min:Total, Cells = "Cells (any)")
```

```{r}
options(knitr.table.format = "latex")

tab <- dat %>%
  mutate(image = group %>% recode(
    "Community Spaces" = "thumbnails/store-alt-blue.png",
    "Places of Worship" = "thumbnails/place-of-worship-purple.png",
    "Social Businesses" = "thumbnails/coffee-red.png",
    "Parks" = "thumbnails/tree-yellow.png",
    "Other" = "thumbnails/hands-helping-grey.png")) %>%
  mutate(Sites = as.numeric(Sites))

tab %>%
  select(-group, -image) %>%
  kable(format = "latex", booktabs = TRUE, 
        caption = "Descriptive Statistics of API-generated Social Infrastructure Rates (for all Searches)") %>%
  kable_styling() %>%
  pack_rows("Community Spaces", 1, 3, 
            background = mycolors[1], color = "white", hline_before = FALSE) %>%
  pack_rows("Places of Worship", 4, 4, 
            background = mycolors[2], color = "white", hline_before = FALSE) %>%
  pack_rows("Social Businesses", 5, 9, 
            background = mycolors[3], color = "white", hline_before = FALSE) %>%
  pack_rows("Parks", 10, 15, 
            background = mycolors[4], color = "black", hline_before = FALSE) %>%
  pack_rows("Other", 16, 19, 
            background = "gray", color = "black", hline_before = FALSE) %>%
  # Get shading for variables, plus bolding
  column_spec(2, bold = tab$Sites > 10, 
              background = if_else(tab$Sites > 10, "#C5C6D0", "white")) %>%
  # Get shading for all other columns  
  column_spec(3:12, 
              background = if_else(tab$Sites > 10, "#C5C6D0", "white"),
              color = if_else(tab$Sites > 10, "black", "#808080")) %>%
  column_spec(column = 1, background = "white", 
              image = spec_image(tab$image, height = 100, width = 100), 
              width = "1cm", latex_valign = "m") %>%
  collapse_rows(columns = 1, valign = "middle", latex_hline = "major") %>%
  footnote(general = "Rates measured per 1000s persons per 1 km$^{2}$. Keywords which returned high rates of social infrastructure highlighted and shaded \textbf{\textcolor{mygrey}{grey}}. \textbf{Total} depicts total rate of each type of social infrastructure in Boston, per 1000 residents. \textbf{Cells} describe total cells with any of that type found.",
           threeparttable = TRUE) %>%
  cat()



```

### Table 2

```{r}
library(tidyverse)
library(knitr)
library(kableExtra)
library(viridis)

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"

tab <- tibble(
  Variable = c("Bonding Social Capital", 
               "Bridging Social Capital", 
               "Linking Social Capital", 
               "Overall Social Capital", 
               "Voter Turnout", "Vote for Democratic Party"),
  
  `Total Sites` = c("+", "+", "", "+", "+", "+"),
  `Community Spaces` = c("+", "+", "", "+", "+", "+"),
  `Places of Worship` = c("+", "+", "", "", "+", "+"),
  `Social Businesses` = c("+", "-", "", "", "", ""),
  Parks = c("+", "+", "", "+", "+", "+")
)

# For column i 
posneg = function(i){
  case_when(
    tab[, i] == "+" ~ "#B6CAFF",
    tab[, i] == "-" ~ "#FD86C0",
    TRUE ~ "white") %>%
    return()
}



l <- tab %>%
  kable(format = "latex", booktabs = TRUE, 
        caption = "Expected Associations with Social Infrastructure Covariates", 
        label = "2", align = c("l", "c", "c", "c", "c", "c")) %>%
  kable_styling() %>%
  column_spec(2, background = posneg(2)) %>%
  column_spec(3, background = posneg(3)) %>%
  column_spec(4, background = posneg(4)) %>%
  column_spec(5, background = posneg(5)) %>%
  column_spec(6, background = posneg(6))

rm(list=ls())
```

We can also adjust that code, adding after toprule some nice images and header colors.

```{latex, eval = FALSE}


\begin{table*}

\caption{Expected Associations with Social Infrastructure Covariates}
\label{TAB:2}
\centering
\begin{tabular}[t]{l>{}c>{}c>{}c>{}c>{}c}
\toprule
%% First cell
& 
%% Second Cell
\includegraphics[width=1cm, height=1cm]{
    thumbnails/hands-helping-grey.png}
& 
%% Third Cell
\includegraphics[width=1cm, height=1cm]{
    thumbnails/store-alt-blue.png}
&
%% Fourth Cell
\includegraphics[width=1cm, height=1cm]{
    thumbnails/place-of-worship-purple.png}
& 
%% Fifth Cell
\includegraphics[width=1cm, height=1cm]{
    thumbnails/coffee-red.png}
& 
%% Sixth Cell
\includegraphics[width=1cm, height=1cm]{
    thumbnails/tree-yellow.png}
\\
\textbf{Variable}
& 
\cellcolor[HTML]{000000} \textcolor{white}{\textbf{Total Sites}} 
& 
\cellcolor[HTML]{0D0887} \textcolor{white}{\textbf{Community Spaces}} 
& 
\cellcolor[HTML]{8405A7} \textcolor{white}{\textbf{Places of Worship}} 
& 
\cellcolor[HTML]{D35171} \textcolor{white}{\textbf{Social Businesses}} 
& 
\cellcolor[HTML]{FCA636} \textcolor{black}{\textbf{Parks}}
\\
\midrule
Bonding Social Capital & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+}\\
Bridging Social Capital & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{FD86C0}{-} & \cellcolor[HTML]{B6CAFF}{+}\\
Linking Social Capital & \cellcolor{white}{} & \cellcolor{white}{} & \cellcolor{white}{} & \cellcolor{white}{} & \cellcolor{white}{}\\
Overall Social Capital & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor{white}{} & \cellcolor{white}{} & \cellcolor[HTML]{B6CAFF}{+}\\
Voter Turnout & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor{white}{} & \cellcolor[HTML]{B6CAFF}{+}\\
\addlinespace
Vote for Democratic Party & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor[HTML]{B6CAFF}{+} & \cellcolor{white}{} & \cellcolor[HTML]{B6CAFF}{+}\\
\bottomrule
\end{tabular}
\end{table*}

```

## 7.4 Matrix

```{r}
library(ggtext)

myvars <- c("library", "city_hall", "community_center", "place_of_worship", "bookstore", "cafe", "park", "fountain", "square", "garden")

g1 <- read_rds("processed_queries/sites_crosstab.rds") %>%
  as.data.frame() %>%
  select(sites:rec) %>%
  mutate_at(vars(sites:rec), 
            funs(
              if_else(condition = . == 0,
                      true = (sort(unique(.))[2] / 2) %>% log(),
                      false = log(.) ))) %>%
  select(myvars) %>%
  cor(use = "pairwise.complete.obs") %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "term") %>%
  pivot_longer(cols = -c(term), names_to = "covariate", values_to = "cor") %>%
   mutate_at(vars(term, covariate),
             funs(. %>% dplyr::recode_factor(
  #"sites" = "Total Sites\n(n = 1318)",
    "library" = "Libraries",
    "city_hall" = "City Hall Facilities",
    "community_center" = "Community Centers",
    "place_of_worship" = "Places of Worship",
    
    "bookstore" = "Bookstores",
    "cafe" = "Cafes",
    "coffeeshop" = "Coffeeshop",
    
    "barbershop" = "Barbershops",
    "beauty_salon" = "Beauty Salons",
    
    "park" = "Parks",
    "fountain" = "Fountains",
    "square" = "Squares",
    
    "garden" = "Gardens",
        "aquarium" = "Aquariums",
    "art_gallery" = "Art Galleries", 
    "museum" = "Museum",
    
    "recreation_center" = "Recreation Center",
    "sports_field" = "Sports Fields", 
    "zoo" = "Zoo"))) %>%
  filter(!is.na(cor)) %>%
  mutate_at(vars(term, covariate),
            funs(factor(., levels = levels(.) %>% rev()))) %>%
  mutate(strong = if_else(cor > 0.3, "Strong", "Weak")) %>%
  ggplot(mapping = aes(x = reorder(term, cor), y = reorder(covariate, cor), fill = cor, label = round(cor, 2))) +
  geom_tile(color = "black", size = 0.1) +
  geom_tile(data = . %>% filter(strong == "Strong"),
            color = "black", size = 0.4) +
  geom_text() +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        axis.text.y = element_text(),
        axis.text.x = element_text(hjust = 1, angle = 20)) +
  scale_fill_gradient2(low = "#DC267F", high = "#648FFF", mid = "white", midpoint = 0) +
  labs(caption = "Note: Reflects correlations of logged rates per 1000 residents, to account for right-skew.\n'City blocks' refer to every 1 square kilometer block (n = 183) within Boston city limits.\nVariables ordered by strength of correlation; thick outlines indicate correlations over 0.3",
       fill = "Correlation\n(Pearson's r)",
       x = NULL, y = NULL, subtitle = "Correlation Matrix of Social Infrastructure Rates among Boston City Blocks") +
  guides(fill = guide_colorbar(frame.colour = "black"))


ggsave(g1, filename = "processed_queries/fig_correlation_all.png", dpi = 500, width = 8, height = 6)

```


```{r}
# How correlated are tallies of social infrastructure across space?
myvars <- c("sites", "parks", "community", "social", "place_of_worship")


g1 <- read_rds("processed_queries/sites_crosstab.rds") %>%
  as.data.frame() %>%
  select(any_of(myvars)) %>%
  mutate_at(vars(myvars), 
            funs(
              if_else(condition = . == 0,
                      true = (sort(unique(.))[2] / 2) %>% log(),
                      false = log(.) ))) %>%
  cor(use = "pairwise.complete.obs") %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "term") %>%
  pivot_longer(cols = c(myvars), names_to = "covariate", values_to = "cor") %>%
   mutate_at(vars(term, covariate),
             funs(. %>% dplyr::recode_factor(
    "sites" = "Total Sites",
              "community" = "Community Space",
              "place_of_worship" = "Places of Worship",
              "social" = "Social Businesses",
              "parks" = "Parks"))) %>%
  mutate_at(vars(term, covariate),
            funs(factor(., levels = levels(.) %>% rev()))) %>%
 
   mutate(strong = if_else(cor > 0.5, "Strong", "Weak")) %>%
   ggplot(mapping = aes(x = term, y = covariate, fill = cor, label = round(cor, 2))) +
  geom_tile(color = "black", size = 0.1) +
  geom_tile(data = . %>% filter(strong == "Strong"),
            color = "black", size = 1) +
  geom_text() +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = ggtext::element_markdown(
          size = 11, hjust = 0),plot.caption.position = "plot",
        legend.title = ggtext::element_markdown(size = 14),
        plot.subtitle = element_text(hjust = 0.5),
        axis.text.x = element_text(hjust = 1, angle = 20)) +
  scale_fill_gradient2(
    limits = c(-1, 1),
    breaks = c(-1, -.75, -.5, -.25, 0, .25, .5, .75, 1),
    low = "#DC267F", high = "#648FFF", mid = "white", midpoint = 0) +
  labs(
#  caption = "<sup>1</sup> <b>Cells</b> show correlations (pearson's r) of logged rates per 1000 residents, to account for right-skew.
#  <br>
#  <b>City blocks</b> refer to every 1 square kilometer block (n = 183) within Boston city limits.
#       <br>
#       Thick outlines indicate correlations over 0.5. Positive correlations indicate internal validity.",
  fill = "Correlation<sup>1</sup>",
  x = NULL, y = NULL, subtitle = "Correlations of Social Infrastructure Rates among Boston City Blocks") +
  guides(fill = guide_colorbar(
    frame.colour = "black",
    ticks = TRUE, 
    ticks.linewidth = 2,
    ticks.colour = c("white","white","white","white",
                     "black","black","black","black","black",
                     "white","white","white","white",
                     "black","black","black","black","black"),
    barheight = 15))

g1 %>%
  ggsave(filename = "viz/figure_B1.png", 
         dpi = 500, width = 7, height = 5)

```

## 7.6 Tile Map

```{r, eval = FALSE}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

myvars <- c("sites", "parks", "community", "social", "place_of_worship")

cross_long <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(myvars), names_to = "variable", values_to = "obs") %>%
    mutate(variable = variable %>% recode_factor(
      "sites" = "<b>Total Sites</b><br>(n = 1208)",
      "community" = "<span style='color:#0D0887'><b>Community Space</b></span><br>(n = 225)",
      "place_of_worship" = "<span style='color:#8405A7'><b>Places of Worship</b></span><br>(n = 224)",
      "social" = "<span style='color:#D35171'><b>Social Businesses</b></span><br>(n = 225)",
      "parks" = "<span style='color:#d98d2b'><b>Parks</b></span><br>(n = 534)")) %>%
  mutate(sig = if_else(obs > 0, "Any", "None", missing = "None"))  %>%
  mutate(type = "map")

# Get an outline of boston grid
boston <- cross_long %>%
  summarize(geometry = st_union(geometry) %>% 
              st_cast(to = "POLYGON"),
            type = "map")

mycolors <- viridis(n = 4, option = "plasma", end = 0.8)
mycolors[4] <- "#eb9a31"

fish <- read_sf("grid_covariates_tracts.kml") %>%
  select(cell_id, pop_black, pop_hisplat, geometry) %>%
  st_transform(crs = aea) %>%
  mutate(race = if_else(pop_black > 0.50 | pop_hisplat > 0.50,
                        true = "> 50% Black/Hispanic",
                        false = "Other")) %>%
  filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid.rds")) %>%
  mutate(type = "map")
```


```{r, eval = FALSE}
g1 <- ggplot() +
  geom_sf(data = boston, mapping = aes(geometry = geometry), color = "darkgrey", size = 1.25) +
  geom_sf(data = cross_long, mapping = aes(
    fill = obs, 
    size = sig, color = sig,
    geometry = geometry)) +
  facet_wrap(~variable, ncol = 5,shrink = TRUE) +
  viridis::scale_fill_viridis(option = "plasma", na.value = "grey",
                              trans = "log",
                              labels = function(x) round(x, 2)) +
  scale_size_manual(values = c(0.2, 0.1)) +
  scale_color_manual(values = c("white", "#777777")) +
  scale_linetype_manual(values = "solid") +
  geom_sf(data = cross_long %>%
            filter(obs > 0),
          mapping = aes(geometry = geometry),
          fill = NA, color = "white", size = 0.15) +
  # Add fishnet grid for race
  geom_sf(data = fish %>%
            filter(race != "Other"), 
          mapping = aes(linetype = race),
          fill = NA, size = 0.5, color = "black") +
  theme_void(base_size = 14) +
  theme(legend.position = "bottom",
        plot.subtitle = element_text(hjust= 0.5, vjust = 2.5),
        strip.text.x = ggtext::element_markdown(size = 11),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = ggtext::element_markdown(size = 10, hjust = 0.5)) +
  # Edit legends
  guides(
    linetype = guide_legend(order = 2, override.aes = list(
      shape = 22, fill = "white", color = "black", size = 0.5)),
    size = "none",color = "none",
    fill = guide_colorsteps(order = 1, barwidth = 15, barheight = 1,
                                 show.limits = TRUE)) +
  # Add labels
  labs(fill = "Sites per 1000 residents",
       linetype = NULL) +
#       caption = "Note: Blocks with 0 sites shaded <span style='color:#777777'><b>grey.</b></span> Top row highlights rate in an average cell,<br>compared with neighborhoods of color outlined in <b>black</b> on the map.") +
  # Add north arrow and scale
  ggspatial::annotation_north_arrow(
    data = data.frame(
      type = "map",
      variable = factor(cross_long$variable[1],
                        levels = levels(cross_long$variable))),
    height = unit(0.4, "cm"),
    width = unit(0.4, "cm")) +
  ggspatial::annotation_scale(
    type = "map",
    data = data.frame(variable = factor(cross_long$variable[1],
                        levels = levels(cross_long$variable))),
    pad_x = unit(2, "cm"))
```

```{r, eval = FALSE}

htmltools::tagList(fontawesome::fa_html_dependency())

library(fontawesome)
library(rsvg)
library(magick)

get_image = function(myname){
  fa_png(name = myname,
         fill = "white", fill_opacity = 0.5, 
         file = paste("fa/", myname, ".png", sep = ""))

    data.frame(icon = paste("fa/", myname, ".png", sep = "")) %>%
      return()
}

bostonpoc <- fish %>%
  filter(race != "Other") %>%
  as.data.frame() %>%
  select(cell_id) %>% unlist()

image_tally <- cross_long %>%
  group_by(variable) %>%
  summarize(
    mean = mean(obs, na.rm = TRUE) %>% round(2),
    mean_poc = mean(
              if_else(
                condition = cell_id %in% bostonpoc,
                true = obs, false = NA_real_),
              na.rm = TRUE) %>% round(2))


myimages <- c("hands-helping",
  "store-alt",
  "place-of-worship",
  "coffee",
  "tree") %>%
  map_dfr(~get_image(.)) %>%
  mutate(variable = c("sites", "community", 
                      "place_of_worship", "parks","social"),
         image_tally,
         color = c("black", mycolors),
         type = "tally",
         geometry = st_centroid(boston$geometry[1]),
         x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])

```

```{r, eval = FALSE}
# Tell Flexdashboard to use fontawesome package for dependencies
library(tidyverse)
library(ggpubr)
library(viridis)
library(sf)
library(rgdal)
library(ggspatial)
library(fontawesome)
library(rsvg)
library(magick)

library(ggimage)


g2 <- ggplot() +
  # Add a second layer
  geom_tile(data = myimages,
            mapping = aes(x = variable, y = 3),
            fill = c("black", mycolors),
            color = c("black", mycolors),
            size = 5) +
  geom_tile(data = myimages,
            mapping = aes(x = variable, y = 2),
            fill = c("black", mycolors),
            color = c("black", mycolors),
            size = 5) +
  # Add a second layer
  geom_tile(data = myimages,
            mapping = aes(x = variable, y = 1),
            fill = c("black", mycolors),
            color = c("black", mycolors),
            size = 5) +
    # Add a second layer
  geom_tile(data = myimages,
            mapping = aes(x = variable, y = 0),
            fill = c("black", mycolors),color = c("black", mycolors),
            size = 5) +
  # Get image
  ggimage::geom_image(data = myimages, size = c(.6, .5, .5, .6, .3), 
             mapping = aes(
               image = icon, x = variable, y = 2.5)  ) +
  # Add overall label
  geom_text(data = myimages,
            mapping = aes(x = variable, y = 1.5,
                          label = "Average Rate"),
            size = 3,
            color = "white") +
  # Add overall rate
  geom_text(data = myimages,
            mapping = aes(x = variable, y = 1,
                          label = mean), size = 6,
            color = "white") +
  # Add label for communities of color
  geom_text(data = myimages,
            mapping = aes(x = variable, y = 0.5,
                          label = "Neighborhoods of Color"),
            size = 3,
            color = "white") +
  # Add rate for communities of color
  geom_text(data = myimages,
            mapping = aes(x = variable, y = 0,
                          label = paste("(", mean_poc, ")", sep = "")), 
            size = 6,
            color = "white") +
  scale_x_discrete(position = "top") +
  facet_wrap(~variable, scales = "free", ncol = 5) +
  theme_void(base_size = 14) +
  theme(
    strip.text = ggtext::element_markdown(size = 10),
    plot.subtitle = element_text(hjust = 0.5, vjust = 5),
    panel.spacing = unit(0, "cm"),
    panel.spacing.x = unit(0, "cm"),
    plot.margin = margin(t = 0.5, r = 0.0, l = 0.0, b = 0, unit = "cm"))  +
  labs(subtitle = "Social Infrastructure Rates in Boston City Blocks")

ggpubr::ggarrange(
  g2, 
  g1 + 
    theme(strip.text.x = element_blank(),
          plot.margin = margin(t = 0, r = 0,l = 0, b = 0, unit = "cm")),
  ncol = 1, heights = c(0.8, 0.8)) %>%
  ggsave(filename = "viz/figure_4.png", 
         dpi = 500, width = 7.5, height = 5)
```

```{r}
rm(list = ls())

```





# 8. Further Analysis



## Neighborhood Comparison

Download neighborhood shapefile here:
https://data.boston.gov/dataset/boston-neighborhoods1

```{r}

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Import neighborhood
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/3525b0ee6e6b427f9aab5d0a1d0a1a28_0.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(name = Name, id = Neighborhood_ID, geometry) %>%
  st_transform(crs = aea) %>%
  saveRDS("neighborhoods.rds")

myvars <- c("sites", "parks", "community", "social", "place_of_worship")



cross_long <- read_rds("processed_queries/sites_crosstab.rds") %>%
  select(cell_id, all_of(myvars), geometry) %>%
  pivot_longer(cols = c(myvars), names_to = "variable", values_to = "obs") %>%
    mutate(variable = variable %>% recode_factor(
      "sites" = "<b>Total<br>Sites</b><br>(n = 1208)",
      "community" = "<span style='color:#0D0887'><b>Community<br>Spaces</b></span><br>(n = 225)",
      "place_of_worship" = "<span style='color:#8405A7'><b>Places<br>of Worship</b></span><br>(n = 224)",
      "social" = "<span style='color:#D35171'><b>Social<br>Businesses</b></span><br>(n = 225)",
      "parks" = "<span style='color:#d98d2b'><b>Parks</b><br></span><br>(n = 534)")) %>%
  st_as_sf()

# Import and calculate average rate per neighborhood
neighbor <- read_rds("neighborhoods.rds") %>%
  st_join(cross_long) %>%
  group_by(name, variable) %>%
  summarize(rate = mean(obs, na.rm = TRUE)) %>%
  ungroup()  %>%
  as_tibble() %>%
  # Add a nice rounded label
  mutate(label = round(rate, 2)) %>%
  # Add a nice easy neighborhood name
  mutate(name = if_else(name == "South Boston Waterfront", 
                        "S.B. Waterfront", name))  %>%
  # Calculate an artificial endpoint to add to every scale
  group_by(variable) %>%
  mutate(yedge = max(rate) + mean(rate) / 1.5) %>%
  ungroup() %>%
  # Calculate the range, divided by 10, for easily nudging labels
  group_by(variable) %>%
  mutate(ynudge = (max(rate) - min(rate)) / 10)



htmltools::tagList(fontawesome::fa_html_dependency())

library(fontawesome)
library(rsvg)
library(magick)
library(ggimage)
library(ggtext)

fa_png(name = "hands-helping",
       fill = "grey", fill_opacity = 0.25, 
       file = "fa/hands-helping-grey.png")
fa_png(name = "store-alt",
       fill = "#0D0887", fill_opacity = 0.25, 
       file = "fa/store-alt-blue.png")
fa_png(name = "place-of-worship",
       fill = "#8405A7", fill_opacity = 0.25, 
       file = "fa/place-of-worship-purple.png")
fa_png(name = "coffee",
       fill = "#D35171", fill_opacity = 0.25, 
       file = "fa/coffee-red.png")
fa_png(name = "tree",
       fill = "#d98d2b", fill_opacity = 0.25, 
       file = "fa/tree-yellow.png")


mysum <- neighbor %>%
  group_by(variable) %>%
  summarize(max = max(rate, na.rm = TRUE),
            min = min(rate, na.rm = TRUE),
            range = max - min,
            y = max - range / 2,
            x = "Roxbury")

myimages <- data.frame(
  myname = c("hands-helping-grey",
             "store-alt-blue",
             "place-of-worship-purple",
             "coffee-red",
             "tree-yellow")) %>%
  mutate(icon = paste("fa/", myname, ".png", sep = "")) %>%
  mutate(variable = factor(neighbor$variable %>% levels(),
                       levels = neighbor$variable %>% levels())) %>%
  left_join(by = "variable", y = mysum)



g1 <- ggplot() +
  # Add lines
  geom_linerange(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, ymin = 0, ymax = rate),
    color = NA) +
  # Add background images
  geom_image(
    data = myimages, 
    mapping = aes(x = x, y = y, image = icon),
    size = 0.6) +
  # Add lines again over top
  geom_linerange(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, ymin = 0, ymax = rate)) +
  # Add a fake invisible point just a little bit ahead of each line
  # This is a trick that will tell ggplot to show us a little more of each x-axis,
  # so we can more clearly see the labels we're about to add
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate + ynudge*5, 
                  color = variable), size = 2.9, color = NA) +
  geom_label(
    data = neighbor,
    mapping = aes(x = reorder(name, rate),
                  y = rate + ynudge, label = label), 
    hjust = 0, label.padding = unit(0.05, "cm"), 
    size = 3.5, color = "#59515E", # smoke
    fill = NA, label.size = NA) +
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate), size = 3) +
  geom_point(
    data = neighbor,
    mapping = aes(x = reorder(name, rate), 
                  y = rate, color = variable), size = 2.9) +
  scale_color_manual(values = c("black", "#0D0887", "#8405A7", "#D35171", "#d98d2b")) +
  facet_grid(~variable, scales = "free") +
  coord_flip() +
  theme_bw(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    axis.text.x = ggtext::element_markdown(size = 11),
    axis.text.y = element_text(size = 13),
    strip.text.y = ggtext::element_markdown(size = 14),
    strip.text.x = ggtext::element_markdown(size = 14),
    strip.background = element_blank(),
    plot.subtitle = element_text(size = 15, hjust = 0.5),
    panel.background = element_rect(fill = "white", color = "white"),
    panel.border = element_blank(),
    panel.spacing = unit(0.25, "cm")) +
  labs(x = NULL,
       y = "Average Rate of Social Infrastructure per 1,000 residents (per square kilometer)",
       subtitle = "Social Infrastructure by Neighborhood") +
  guides(color = "none")


ggsave(g1, filename = "viz/figure_6.png", 
       dpi = 500, width = 10, height = 6)

neighbor
```


```{r}

#exp(c(-4,-3,-2,-1, 0)) %>% round(2) %>% as.character()

both <- dplyr::bind_rows(neighbor %>%
                   mutate(type = "Neighborhoods"), 
                 cross_long %>% 
                   mutate(type = "City Blocks") %>% 
                   rename(value = obs)) %>%
  st_as_sf()

ggplot() +
    geom_sf(data = counties, color = "black", size = 0.5) +
  geom_sf(data = counties, color = NA, fill = "lightgrey") +
  geom_sf(data = both, mapping = aes(geometry = geometry, fill = log(value)),
          color = "white", size = 0.1)  +
  facet_grid(type~variable) +
  viridis::scale_fill_viridis(option = "plasma", na.value = "black") +
  theme_void(base_size = 14) +
  theme(legend.position = "bottom",
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(barwidth = 15, barheight = 0.5)) +
  labs(fill = "Sites per 1000 residents\n(on a logged color scale)",
       caption = "Note: Areas with 0 sites shaded black. Areas outside Boston shaded grey.")  +
  ggsave("viz/fig_both.png", dpi = 500, width = 8, height = 5)



ggplot() +
  geom_sf(data = counties, color = "black", size = 0.5) +
  geom_sf(data = counties, color = NA, fill = "lightgrey") +
  geom_sf(data = both %>%
            filter(variable == "Total Sites\n(n = 1317)"), 
          mapping = aes(geometry = geometry, fill = log(value)),
          color = "white", size = 0.1)  +
  facet_grid(~type) +
  viridis::scale_fill_viridis(option = "plasma", na.value = "black") +
  theme_void(base_size = 14) +
  theme(legend.position = "bottom",
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(barwidth = 15, barheight = 0.5)) +
  labs(fill = "Sites per 1000 residents\n(on a logged color scale)",
       caption = "Note: Areas with 0 sites shaded black.")  +
  ggsave("viz/fig_compare.png", dpi = 500, width = 6, height = 5)


ggplot() +
  geom_sf(data = neighbor %>%
            filter(variable == "Total Sites\n(n = 1317)")%>%
            mutate(value = if_else(name == "Harbor Islands", NA_real_, value)),
          mapping = aes(geometry= geometry),
          color = "steelblue", size = 5, alpha = 0.25) +
  geom_sf(data = neighbor %>%
            filter(variable == "Total Sites\n(n = 1317)") %>%
            mutate(value = if_else(name == "Harbor Islands", NA_real_, value)),
          mapping = aes(fill = log(value), geometry = geometry),
          color = "black", size = 0.5) +
  geom_sf_label(data = neighbor %>%
            filter(variable == "Total Sites\n(n = 1317)") %>%
            mutate(value = if_else(name == "Harbor Islands", NA_real_, value)),
            mapping = aes(geometry = geometry, label = name), size = 2) +
  facet_wrap(~variable, ncol = 5) +
  viridis::scale_fill_viridis(option = "plasma", na.value = "black",
                              labels = c(0.37, 0.61, 1, 1.65)) +
  theme_void(base_size = 14) +
  theme(legend.position = "bottom",
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(barwidth = 15, barheight = 0.5)) +
  labs(fill = "Sites per 1000 residents\n(on a logged color scale)",
       caption = "Note: Harbor Islands excluded from color scale.") +
  ggsave("viz/boston_total_sites_neighborhoods.png", dpi = 1000, width = 6, height = 7)

```

```{r}
neighbor %>%
  as.data.frame() %>%
  group_by(variable) %>%
  arrange(desc(value)) %>%
  mutate(rank = 1:n()) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = reorder(name, value), y = value, 
                       ymin = 0, ymax = value, group = name,
                       fill = rank, color = rank, size = value)) +
  geom_linerange(size = 1) +
  geom_point(shape = 21, color = "black") +
  facet_grid(~variable, scales = "free_x") +
  coord_flip() +
  labs(y = "Average Rate of Social Infrastructure on each City Block per 1000 residents",
       subtitle = "Boston Neighborhoods by Social Infrastructure Estimate",
       caption = "Note: Calculated as average rate of sites per 1000 residents tabulated per square kilometer",
      x = NULL, fill = "Ranking") +
  viridis::scale_color_viridis(option = "plasma", end = 0.2, begin = 0.8) +
  viridis::scale_fill_viridis(option = "plasma", end = 0.2, begin = 0.8) +
  guides(fill = guide_colorbar(barwidth = 0.5, barheight = 10),
         color = FALSE, size = FALSE) +
  theme_minimal(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        panel.grid.minor = element_blank(),
        plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5),
        panel.spacing = unit(0.5, "cm")) +
  ggsave("viz/fig_lollipop.png", dpi = 500, width = 13, height = 6)


```




## Chi-Squared

How spatially concentrated are our rates?
Let's imagine that the rates we have seen were randomly scattered across these tiles; how rarely would they result in this particular level of spatial concentration?

```{r}
# We're going to make our own chi-squared test,
# which assesses how far away we are from what we would expect at random

# my chi-squared function

get_stat = function(mydata){
  mydata %>%
    select(x, y, variable, obs) %>%
    # Calculate row total
    group_by(x,  variable) %>%
    mutate(x_total = sum(obs, na.rm = TRUE)) %>%
    ungroup() %>%
    # Calculate column total
    group_by(y,  variable) %>%
    mutate(y_total = sum(obs, na.rm = TRUE)) %>%
    ungroup() %>%
    # Calculate grand total
    group_by(variable) %>%
    mutate(total = sum(obs, na.rm = TRUE)) %>%
    # Calculate expected value
    mutate(expected = (x_total * y_total) / total) %>%
    # Calculate average difference between observed and expected, squared, summed,
    # also known as chi-squared
    summarize(chisquared = sum((obs - expected)^2 / expected, na.rm = TRUE)) %>%
    ungroup() %>%
    return()
}

cross_long <- cross %>%
  as.data.frame() %>%
  pivot_longer(cols = c(sites:rec), names_to = "variable", values_to = "obs") 

stat <- cross_long %>%
  get_stat()

perm <- data.frame(reps = 1:1000) %>%
  split(.$reps) %>%
  map_dfr(~cross_long %>%
            group_by(variable) %>%
            mutate(obs = sample(obs, replace = FALSE)) %>%
            get_stat(), .id = "reps") %>%
  rename(permuted = chisquared)

stat %>%
  left_join(by = "variable", 
            y = perm) %>%
  group_by(variable) %>%
  summarize(estimate = unique(chisquared),
            p_value = sum(chisquared > permuted) / n(),
            critical = quantile(permuted, probs = 0.90)) %>%
  saveRDS("shapes/sites_cross_perm.rds")

read_rds("shapes/sites_cross_perm.rds") %>%
  mutate(sig = if_else(p_value < 0.10, "p < 0.10", "insignificant")) %>%
  ggplot(mapping = aes(x = reorder(variable, estimate), 
                       y = estimate, ymin = 0, ymax = estimate, 
                       color = sig)) +
  geom_linerange() +
  geom_point() +
  coord_flip() +
  scale_color_manual(values = c("grey",viridis(n = 3, option = "plasma")[2])) +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank())

myvars <- c("library", "city_hall", "community_center", "place_of_worship", "bookstore", "cafe", "park", "fountain", "square", "garden")

perm %>%
  filter(variable %in% myvars) %>%
  ggplot(mapping = aes(x = permuted)) +
  geom_histogram() +
  geom_vline(data = read_rds("shapes/sites_cross_perm.rds") %>%
               filter(variable %in% myvars),
             mapping = aes(xintercept = estimate), color = "firebrick") +
  facet_wrap(~variable, ncol = 5) +
  ggsave("viz/chisquared.png",dpi =500, width = 6, height = 6)
```



## Train Lines and Stations

```{r, eval = FALSE}

#########################
# Train Lines and Stations
# viz_study_area.png
#########################

library(sf)
library(rgdal)
library(lwgeom)


# Import Boston tile outline
zone <- read_rds("dashdata/zone_boston.rds")

# Get train lines within that zone
read_sf("transit/MBTA_ARC.shp") %>%
  st_transform(crs= aea) %>%
  select(line = LINE, geometry) %>%
  group_by(line) %>%
  summarize(geometry = st_union(geometry)) %>%
  # Now snap any lines to the border of boston where relevant
  mutate(geometry = st_snap(geometry, zone, tol = 1e-9)) %>%
  # For each T-line,
  group_by(line) %>%
  # splite the lines in half on these borders
  summarize(geometry = st_split(geometry, y = zone$geometry) %>%
              # Collect the results into many smaller lines
              st_collection_extract(type = "LINESTRING")) %>%
  # Join in the name "Boston" to lines overlapping that polygon
  st_join(zone) %>%
  # remove any lines not overlapping Boston polygon
  filter(!is.na(name)) %>%
  # And crop the extent of the Boston polygon to cut off any remainders
  st_crop(zone) %>%
  select(-name) %>%
  saveRDS("dashdata/train_lines_boston.rds")

# GEt train stops within that zone
read_sf("transit/MBTA_NODE.shp") %>%
  st_transform(crs= aea) %>%
  select(station = STATION, line = LINE, geometry) %>%
  # filter out any stops not within Boston polygons
  st_join(zone) %>%
  filter(!is.na(name)) %>%
  select(-name) %>%
    # Simplify names
    mutate(line = case_when(
    str_detect(line, pattern = "[/]") ~ "MULTIPLE",
    TRUE ~ line))  %>%
  saveRDS("dashdata/train_stops_boston.rds")
  

stops <- read_rds("dashdata/train_stops_boston.rds")
lines <- read_rds("dashdata/train_lines_boston.rds")
zone <- read_rds("dashdata/zone_boston.rds")

ggplot() +
  geom_sf(data = zone) +
  geom_sf(data = lines) +
  geom_sf(data = stops)

# Repeat, but just for our validation study zone

zone <- read_rds("dashdata/zone_validation.rds")

# Get train lines within that zone
read_sf("transit/MBTA_ARC.shp") %>%
  st_transform(crs= aea) %>%
  select(line = LINE, geometry) %>%
  group_by(line) %>%
  summarize(geometry = st_union(geometry)) %>%
  # Now snap any lines to the border of boston where relevant
  mutate(geometry = st_snap(geometry, zone, tol = 1e-9)) %>%
  # For each T-line,
  group_by(line) %>%
  # splite the lines in half on these borders
  summarize(geometry = st_split(geometry, y = zone$geometry) %>%
              # Collect the results into many smaller lines
              st_collection_extract(type = "LINESTRING")) %>%
  # Join in the name "Boston" to lines overlapping that polygon
  st_join(zone) %>%
  # remove any lines not overlapping validation study area polygon
  filter(!is.na(name)) %>%
  # And crop the extent of the polygon to cut off any remainders
  st_crop(zone) %>%
  select(-name) %>%
  saveRDS("dashdata/train_lines_validation.rds")

# Get train stops within that zone
read_sf("transit/MBTA_NODE.shp") %>%
  st_transform(crs= aea) %>%
  select(station = STATION, line = LINE, geometry) %>%
  # filter out any stops not within Boston polygons
  st_join(zone) %>%
  filter(!is.na(name)) %>%
  select(-name) %>%
  # Simplify names
    mutate(line = case_when(
    str_detect(line, pattern = "[/]") ~ "MULTIPLE",
    TRUE ~ line))  %>%
  saveRDS("dashdata/train_stops_validation.rds")
  

stops <- read_rds("dashdata/train_stops_validation.rds")
lines <- read_rds("dashdata/train_lines_validation.rds")
zone <- read_rds("dashdata/zone_validation.rds")

ggplot() +
  geom_sf(data = zone) +
  geom_sf(data = lines) +
  geom_sf(data = stops)
```

## Neighborhoods and Tiles

```{r, eval = FALSE}
#########################
# Neighborhoods and Tiles
#########################

# Get tiles within Boston

# Import all tiles in Suffolk county
read_sf("milestones.kmz") %>%
  select(-c(timestamp:icon)) %>%
  # Zoom into just ones within Boston (we labelled them)
  filter(str_detect(milestone, pattern = "M")) %>%
  # set to albers equal area conic projection 
  st_as_sf(crs = aea) %>%
  saveRDS("dashdata/tiles_boston.rds")
  
# Make a polygon outline of Boston tiles
read_sf("milestones.kmz") %>%
  filter(str_detect(milestone, "M")) %>%
  st_transform(crs= aea) %>%
  summarize(name = "Boston", 
            geometry = st_union(geometry) %>%
              # fix a few holes in the polygon
              nngeo::st_remove_holes()) %>%
  saveRDS("dashdata/zone_boston.rds")

# Make a polygon outline of Validation Study tiles
read_sf("milestones.kmz") %>%
  filter(str_detect(milestone, "M1|M2|M3")) %>%
  st_transform(crs= aea) %>%
  summarize(name = "Validation Study Area", 
            geometry = st_union(geometry) %>%
              # fix a few holes in the polygon
              nngeo::st_remove_holes()) %>%
  saveRDS("dashdata/zone_validation.rds")


# Import rough neighborhood boundaries
read_sf("neighborhoods.kmz") %>%
  select(-c(timestamp:icon)) %>%
  select(neighborhood = Name, geometry) %>%
  # set to albers equal area conic projection 
  st_as_sf(crs = aea) %>%
  # Get better labels
  mutate(label = case_when(
    neighborhood == "Fenway/Kenmore" ~ NA_character_, 
    neighborhood == "West End" ~ NA_character_,
    neighborhood == "Beacon Hill" ~ NA_character_,
    neighborhood == "Downtown" ~ NA_character_,
    neighborhood == "Chinatown" ~ NA_character_,
    TRUE ~ neighborhood)) %>%
  saveRDS("dashdata/neighborhoods_boston.rds")

```

## Key areas

```{r, eval = FALSE}
### Get labels for key areas!
read_rds("dashdata/neighborhoods_boston.rds") %>%
  filter(neighborhood %in% c("Allston/Brighton", "Downtown", 
                             "East Boston", "Dorchester",
                             "Roxbury", "Mattapan")) %>%
  mutate(neighborhood = if_else(
    neighborhood == "Allston/Brighton", "Allston", neighborhood)) %>%
  group_by(neighborhood) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  saveRDS("dashdata/key_areas.rds")
```

## Simple Map of Study Area

```{r, eval = FALSE}
#########################
# Simple Map of Area
# viz_study_area.png
#########################

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Demo
tiles <- read_rds("dashdata/tiles_boston.rds") %>%
  st_transform(crs = aea)
areas <- read_rds("dashdata/neighborhoods_boston.rds") %>%
  st_transform(crs = aea)
stops <- read_rds("dashdata/train_stops_boston.rds")  %>%
  st_transform(crs = aea)
lines <- read_rds("dashdata/train_lines_boston.rds") %>%
  st_transform(crs = aea)
zone <- read_rds("dashdata/zone_validation.rds") %>%
  st_transform(crs = aea)


# Let's import a little airport image to make 
# it really clear why we don't map that area
airport <- data.frame(x =  -71.0108, 
           y =  42.36255,
           image = "dashdata/airport.png") %>%
  mutate(geometry = paste("POINT(", x, " ", y, ")", sep = "")) %>%
  st_as_sf(wkt = "geometry", crs = wgs) %>%
  st_transform(crs = aea) %>%
  mutate(x_aea = st_coordinates(geometry)[,1],
         y_aea = st_coordinates(geometry)[,2])

library(ggimage)
library(ggtext)

g1 <- ggplot() +
  geom_sf(data = areas, fill = "tan", color = "white") +
  geom_sf(data = tiles, alpha = 0.5, color = "lightgrey", fill = NA) +
  geom_sf(data = lines, mapping = aes(color = line)) +
  geom_sf(data = stops, mapping = aes(color = line), size = 1) +
  geom_sf(data = zone, size = 1.5, color = "darkgrey",
          mapping = aes(fill = "Validation\nStudy")) +

  geom_sf_text(data = areas, mapping = aes(label = label),
               color = "black", size = 2.5, check_overlap = TRUE)  +
  geom_image(data = airport, aes(x = x_aea, y = y_aea, image= image), size = 0.05) + 
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white")) +
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey")) +
  scale_fill_manual(values = c(NA_character_), na.value = NA) +
  labs(color = "Train Lines",
       fill = NULL,
       title = "Boston Study Area Mapped",
       subtitle = "<i>168 City Blocks by 1 km<sup>2</sup> grid cells</i>") +
  guides(fill = guide_legend(order = 2), color = guide_legend(order = 1)) +
  theme(legend.position = c(1.15, 0.35),
        plot.title = element_text(hjust = 0, size = 14),
        plot.subtitle = element_markdown(vjust = -5, size = 12, hjust = 0), 
        plot.title.position = "plot")
ggsave(g1, filename = "dashdata/viz_study_area.png", dpi = 500, width = 5, height = 4.5)

remove(tiles, areas, stops, lines, zone, airport)
```



# 9. Get Social Infrastructure Data

## 3.1 Get Online-Checked Sites


```{r, message = FALSE, warning = FALSE,  eval = FALSE}
# Import grid!
grid <- read_sf("milestones.kmz") %>%
  # Exclude extraneous files
  select(cell_id = Name, neighborhood, milestone) %>%
    # convert to equal area conic projection
  st_transform(crs = aea)

# Import grid cells sampled for ground truthing
blocks <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  filter(type == "masters")  %>%
  select(Name, description, type, sampled, 
         neighborhood:pop_age_65_plus, geometry) %>%
  # Transform to Equal Area conic projection
  st_transform(crs = aea)



read_sf("validated/sites.kml") %>%
  # Get rid of fluff kml fields
  select(-c(timestamp:icon)) %>%
  # Zoom into just valid sites,
  # excluding any sites we identified were not social infrastrucutre,
  # which were not found, duplicated, or not yet verified ("")
  mutate(status = status %>% tolower() %>% str_trim() %>% na_if("")) %>%
  # update wording to new siteS, emphassis on s.
  mutate(status = if_else(status == "new site", "new sites", status)) %>%
  # Filter to just places that are 'checked', 'new sites', or 'been there'
  filter(status %in% c("checked", "new sites", "been there")) %>%
  # Clean initials of data coders
  mutate(description = tolower(description) %>% 
           na_if("") %>% str_trim() %>% recode(
             "checked" = NA_character_,
             "j" = "jz", "nc " = "nc")) %>%
  mutate(group = group %>% tolower() %>% str_trim() %>% na_if("") %>%
           recode(
             "community space" = "community space", 
             "community spaces" ="community space",
             "social businesses" = "social business", 
             "social business" = "social business",
             "social busiesses" = "social business",
             "parks" = "park",
             "places of worship" = "place of worship",
             "places of worhsip" = "place of worship",
             "places of worsship" = "place of worship",
             "places of worhip" = "place of worship",
             .missing = "other")) %>%
  # add unique id for each point
  mutate(id = 1:n()) %>%
  # condense number of columsn
  select(Name, description, id, group, status, google_id = place_id, geometry) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Filter points to just those located within milestones 1, 2, and 3.
  #filter(str_detect(milestone, pattern = "M1|M2|M3|M4")) %>%
  select(Name, description, id, group, status, google_id, 
         cell_id, neighborhood, milestone, geometry) %>%
  mutate(milestone = na_if(milestone, "")) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other")) %>%
  write_sf("mysites_all_boston.kml", driver = "kml", delete_layer = TRUE)


# Filter points to just those located within milestones 1, 2, and 3.
read_sf("mysites_all_boston.kml") %>%
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  write_sf("mysites_core_boston.kml", driver = "kml", delete_layer = TRUE)


#read_sf("validated/sites.kml")$status %>% unique() %>% sort()
#read_sf("sites.kml")$status %>% unique() %>% sort()
```




## 3.2 Get Original Google Sites within Study Area (M1-M3)

```{r}
grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(description, timestamp:icon)) %>%
    # convert to equal area conic projection
  st_transform(crs = aea) %>%
  select(-Name) %>%
  # Zoom into inner boston
  filter(str_detect(milestone, "M1|M2|M3"))

read_sf("validated/sites.kml") %>%
  # Zoom into just our Google search results
  mutate(place_id = place_id %>% na_if("")) %>%
  filter(!is.na(place_id)) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Zoom into just sites within the grid
  filter(str_detect(milestone, "M1|M2|M3")) %>%
  select(-c(timestamp:icon)) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "Community Space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other",
    "Not social infrastructure" = "Other")) %>%
  
  write_sf("mygoogle.kml", driver = "kml", delete_layer = TRUE)
```

Repeat, but this time get the sites for the entire Boston area.

```{r}
grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(description, timestamp:icon)) %>%
    # convert to equal area conic projection
  st_transform(crs = aea) %>%
  select(-Name) %>%
  # Zoom into inner boston
  filter(str_detect(milestone, "M1|M2|M3|M4"))


read_sf("validated/sites.kml") %>% 
  # Zoom into just our Google search results
  mutate(place_id = place_id %>% na_if("")) %>%
  filter(!is.na(place_id)) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Zoom into just sites within the grid
  filter(str_detect(milestone, "M1|M2|M3|M4")) %>%
  select(-c(timestamp:icon)) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "Community Space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other",
    "Not social infrastructure" = "Other")) %>%
  
  write_sf("mygoogle_boston.kml", driver = "kml", delete_layer = TRUE)
```



## 3.3 Get Ground-Truthed Points, from Masters Students

```{r, message = FALSE, warning = FALSE}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


###OLD 
# Import ground-truthed sites from masters students
#read_csv("groundtruth/map_marker_export_2021_10_10-12h47m15.csv") %>%
#  select(lat = Latitude, lon = Longitude,
#         title = Title, 
#         groundtruth = "MultiChoiceSelection: groundtruth/visited/not yet visited",
#         group = "MultiChoiceSelection: group/Not yet classified/Community Spaces/Places of Worship/Social Businesses/Parks/Other",
#         notes = "FreeText: Notes") %>%
#  mutate(group = case_when(
#    title %in% c("Curry Center Square","McConnell Park","Highland Avenue Community Garden",
#                 "Whitey McGra Memorial") ~ "Parks",
#    title %in% c("Science and Engineering Library", "Fineman & Pappas Law Libraries",
#    "Frederick S. Pardee Management Library","Michael D. Papagiannis Astronomy Library",
#    "Pickering Educational Resources Library", "Mugar Memorial Library",
#    "Fenway Library Organization, Inc.", "School of Theology Library",
#    "Boston University Science Library","Stone Science Library") ~ "Not social infrastructure",
#    title == "Freedom Christ-Ministry" ~ "Places of Worship",
#    title == "Starbucks?" ~ "Social Businesses",
#    TRUE ~ group)) %>%
  # Remove duplicated locations
#  filter(!notes %in% c("Double")) %>%
  # Remove any sites that are not social infrastructure, were not found, or were not completed
#  filter(!notes %in% c("Not social infrastructure", "Not found",
#                       "Not found/ not a park anymore",
#                       "not found", "Not found\n", "Under construction") ) %>%
#  filter(!group %in% c("Not social infrastructure")) %>%
#  # Remove any that weren't checked off either. These were not found.
#  mutate(groundtruth = groundtruth %>% na_if("")) %>%
#  filter(!is.na(groundtruth)) %>%
#  
#  filter(groundtruth == "visited") %>%
#  select(-notes) %>%
#  
#  # Get just distinct rows and sites
#  distinct() %>%
#  # Convert to sf format
#  mutate(geometry = paste("POINT(", lon, " ", lat, ")", sep = "")) %>%
#  st_as_sf(crs = 4326, wkt = "geometry") %>%
#  st_transform(crs = aea) %>%
#  select(-lat, -lon) %>%
#  # Filter out any points outside our blocks
#  st_join(blocks %>% select(cell_id = Name, geometry)) %>%
#  filter(!is.na(cell_id)) %>%
#  write_sf("mygroundtruthed.kml", driver = "kml", delete_layer = TRUE)



# Import grid cells sampled for ground truthing
blocks <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  select(Name, description, type, sampled, 
         neighborhood:pop_age_65_plus, geometry) %>%
  # Transform to Equal Area conic projection
  st_transform(crs = aea)


# Import ground-truthed sites from masters students
read_csv("groundtruth/map_marker_export_2021_10_10-12h47m15.csv") %>%
  select(lat = Latitude, lon = Longitude,
         title = Title, 
         fieldworkers = "FreeText: fieldworkers",
         description = "Description",
         color = "Color",
         groundtruth = "MultiChoiceSelection: groundtruth/visited/not yet visited",
         group = "MultiChoiceSelection: group/Not yet classified/Community Spaces/Places of Worship/Social Businesses/Parks/Other",
         notes = "FreeText: Notes") %>%
  mutate(group = case_when(
    title %in% c("Curry Center Square","McConnell Park","Highland Avenue Community Garden",
                 "Whitey McGra Memorial") ~ "Parks",
    title %in% c("Science and Engineering Library", 
                 "Fineman & Pappas Law Libraries",
    "Frederick S. Pardee Management Library",
    "Michael D. Papagiannis Astronomy Library",
    "Pickering Educational Resources Library",
    "Mugar Memorial Library",
    "Fenway Library Organization, Inc.",
    "School of Theology Library",
    "Boston University Science Library",
    "Stone Science Library") ~ "Not social infrastructure",
    title == "Freedom Christ-Ministry" ~ "Places of Worship",
    title == "Starbucks?" ~ "Social Businesses",
    TRUE ~ group)) %>%
  # Remove duplicated locations
      
  # indicate any that weren't checked off either. These were not found.
  mutate(groundtruth = groundtruth %>% na_if("")) %>%
  mutate(exclude = case_when(
    notes == "Double" ~ "yes",
    # Remove any sites that are not social infrastructure, were not found, or were not completed
    notes %in% c("Not social infrastructure", 
                 "Not found",
                 "Not found/ not a park anymore",
                 "not found", 
                 "Not found\n", 
                 "Under construction") ~ "yes",
    group %in% c("Not social infrastructure") ~ "yes",
    is.na(groundtruth) ~ "yes",
    groundtruth != "visited" ~ "yes",
    TRUE ~ "no")) %>%
  #filter(!is.na(groundtruth)) %>%
  #filter(groundtruth == "visited") %>%
  #select(-notes) %>%
  # Get just distinct rows and sites
  distinct() %>%
  rowwise() %>%
  mutate(fieldworkers = paste(unique(c(fieldworkers, description)), 
                              collapse = ", ") %>% 
           str_remove("[,] NA|NA[,] ")) %>%
  ungroup() %>%
  # Filter out any points outside our blocks
  # Convert to sf format
  mutate(geometry = paste("POINT(", lon, " ", lat, ")", sep = "")) %>%
  st_as_sf(crs = 4326, wkt = "geometry") %>%
  st_transform(crs = aea) %>%
  st_join(blocks %>% select(cell_id = Name, type, geometry)) %>%
  # If outside of block, then exclude, otherwise, keep regular value
  mutate(exclude = if_else(is.na(cell_id), "yes", exclude))  %>%
  as_tibble() %>%
  write_csv("export/ground_truthed_points_processed.csv")

library(tidyverse)
read_csv("groundtruthedpoints_editing_final.csv") %>%
  filter(exclude == "no")  %>%
  select(-exclude) %>%
  # Fix a few where people forgot to label theme
  mutate(group = case_when(
    title == "Highland Park Garden" ~ "Parks",
    title == "Ell Hall Walkway" ~ "Parks",
    title == "Symphony Road Community Garden" ~ "Parks",
    title == "Don Negro Barbershop" ~ "Social Businesses",
    title == "Malibu Beach Boardwalk" ~ "Parks",
    TRUE ~ group)) %>%
  #filter(title == "Malibu Beach Boardwalk") %>%
  st_as_sf(coords = c("lon", "lat"), dim = "XY", crs = wgs) %>%
  saveRDS("mygroundtruthed.rds")
```


# 10. Map Points

```{r}
library(tidyverse)
library(sf)
library(rgdal)
# Equal Area projection
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"
# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(description, timestamp:icon)) %>%
    # convert to equal area conic projection
  st_transform(crs = aea) %>%
  select(-Name) %>%
  # Zoom into inner boston
  filter(str_detect(milestone, "M1|M2|M3"))

# Import grid cells sampled for ground truthing
blocks <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  filter(type == "masters")  %>%
  select(cell_id = Name, description, neighborhood:geometry) %>%
  # Transform to Equal Area conic projection
  st_transform(crs = aea)

# Import original points
mygoogle <- read_sf("mygoogle.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  st_transform(crs = aea)

# Import online-checked points
mychecked <- read_sf("mysites_core_boston.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  st_transform(crs = aea)


#myground <- read_csv("groundtruthedpoints_editing_final.csv") %>%
#  filter(exclude == "no")  %>%
#  select(-exclude) %>%
#  mutate(x = lon, y = lat) %>%
#  st_as_sf(coords = c("lon", "lat"), dim = "XY", crs = wgs) %>%
#  st_transform(crs = aea)


# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds") %>%
  st_transform(crs = aea)

# Import census tracts within study area
shapes <- read_sf("neighborhoods.kmz") %>%
  select(-c(timestamp:icon)) %>%
  st_transform(crs = aea) 


mycolors <- viridis::plasma(n = 3, begin = 0.2, end = 0.8)

library(ggtext)

g1 <- ggplot() +
  # Add base census tracts layer
  geom_sf(data = shapes, fill = "black", 
          color = "darkgrey", size = 0.2) +
  # Add points over that
  geom_sf(data = mygoogle, mapping = aes(fill = "Google Sites <sup>1</sup>"), 
          size = 1.5, color = "black", shape = 21) +
  geom_sf(data = mychecked, mapping = aes(fill = "Checked Online <sup>2</sup>"), 
          size = 1.5, color = "black", shape = 21) +
  geom_sf(data = myground, mapping = aes(fill = "Ground Truthed <sup>3</sup>"), 
          size = 1.5, color = "white", shape = 21) +
  # Add grid overtop
  geom_sf(data = grid, mapping = aes(color = "Study Area <sup>4</sup>"),
          size = 0.5, alpha = 0.5, fill = "white") +
  # Add masters blocks sampled overtop
  geom_sf(data = blocks, size = 0.5, alpha = 0.5, fill = "#6A00A8",
         mapping = aes(color = "Sampled Cells <sup>5</sup>")) +
  theme_bw(base_size = 14) +
  scale_fill_manual(values = c("#FCA636", "#CC4678","#6A00A8"),
                    breaks = c("Google Sites <sup>1</sup>",
                               "Checked Online <sup>2</sup>",
                               "Ground Truthed <sup>3</sup>")) +
  scale_color_manual(
    values = c("black","#6A00A8"),
    breaks = c("Study Area <sup>4</sup>",
               "Sampled Cells <sup>5</sup>")) +
 coord_sf(xlim = c(1905020, 1914783),
           ylim = c(521064.4, 532540.7)) +
  theme(
    plot.caption = element_markdown(hjust = 0, size = 12),
    legend.text = element_markdown(size = 14),
    legend.key = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = "white"),
    panel.grid = element_blank(), 
    axis.text = element_blank(),
    axis.ticks = element_blank()) +
  guides(
    order = 2,
    color = guide_legend(
      override.aes = list(
        size = 2,
        shapes = 22,
        stroke = 0,
        alpha = 0.1,
        fill = c("white", "#6A00A8"),
        color = c("black", "#6A00A8"),
        linetype = "solid")),
    fill = guide_legend(
      order = 1,
      override.aes = list(
      size = c(4, 4, 4),
      shapes = c(1,1,1),
      color = c("black", "black", "white"),
      stroke = c(0.5, 0.5, 0.5),
      alpha = c(1, 1, 1),
      fill = c("#FCA636", "#CC4678","#6A00A8"),
      linetype = c("blank", "blank", "blank")))) +
  theme(plot.subtitle = element_text(hjust = 0.5), 
        plot.title.position = "plot") +
  labs(
    subtitle = "Mapping Social Infrastructure in Boston",
    fill = "Social Infrastructure",
    color = "Grid Cells")

ggsave(g1, filename = "viz/figure_A7.png", dpi = 500, width = 5.5, height = 4)

g2 <- g1 +
labs(caption = "<sup>1</sup>
    <b> Google Sites</b> = sites captured by Google Maps Places API
    <br>(<i>which were not found in searches by researchers</i>).
    <br><br>
    <sup>2</sup>
    <b> Checked Online</b> = sites confirmed by human searches online.
    <br>(<i>Includes API searches later confirmed by researchers.</i>)
    <br><br>
    <sup>3</sup>
    <b> Ground Truthed</b> = sites visually confirmed in person in 10 grid cells.
    <br><br>
    <sup>4</sup>
    <b> Study Area</b>: 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End.
    <br><br>
    <sup>5</sup>
    <b> Sampled Cells</b>: 10 cells randomly sampled for ground-truthing.<br>Representative by: population density,  median income,<br> unemployment, median monthly housing costs,<br>(%) over age 65, (%) Women, (%) Black, (%) White,<br>(%) Hispanic/Latino, (%) Asian, (%) Native American.")


ggsave(g1, filename = "viz/masters.png", dpi = 500, width = 5.5, height = 7)
```




### Validating API Sites in Core Boston Neighborhoods

```{r, eval= FALSE}
# Get a bunch of sites

newsites <- read_sf("mysites_core_boston.kml") %>%
  select(-c(timestamp:icon)) %>%
  select(Name, group, status, geometry) %>%
  filter(status == "new sites") %>%
  mutate(status = "Verified New Sites")

checkedsites <- read_sf("mysites_core_boston.kml") %>%
  select(-c(timestamp:icon)) %>%
  select(Name, group, status, geometry) %>%
  filter(status %in%  c("been there", "checked")) %>%
  mutate(status = "Verified API Sites")

mygoogle <- read_sf("mygoogle.kml") %>%
  select(-c(timestamp:icon))  %>%
  select(Name, group, status, geometry) %>%
  filter(!Name %in% checkedsites$Name) %>%
  filter(status != "Double", status != "New Sites") %>%
  mutate(status = status %>% na_if("") %>% tolower() %>% str_trim()) %>%
  mutate(status = case_when(
    status == "not social infrastructure" ~ "Not social infrastructure",
    TRUE ~ "API Sites Not Found"))

bind_rows(newsites, checkedsites, mygoogle) %>%
  saveRDS("dashdata/points_validation.rds")
```

```{r, echo = FALSE}
#input <- data.frame(infra = "Parks")

areas <- read_rds("dashdata/key_areas.rds") %>%
  filter(neighborhood %in% c("Downtown", "Roxbury", "Dorchester"))
lines <- read_rds("dashdata/train_lines_validation.rds")
zone <- read_rds("dashdata/zone_validation.rds")
tiles <- read_rds("dashdata/tiles_boston.rds") %>%
  filter(str_detect(milestone, "M1|M2|M3"))

# Equal Area projection
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

```


```{r, echo = FALSE}
# Add 5 basic colors
mycolors <- viridis::plasma(n = 5, begin = 0, end = 0.7)
points <- read_rds("dashdata/points_validation.rds") %>%
  st_transform(crs= aea)

nobs <- dim(points)[1]

## Total Plot
g1 <- ggplot() +
  geom_sf(data = zone, color = "darkgrey", size = 5) +
  geom_sf(data = zone, fill = "black") +
  geom_sf(data = tiles, alpha = 0.25, fill = "grey", color = "white", size = 0.1) +
  # 
  geom_sf(data = points, mapping = aes(fill = status), 
          shape = 21, color = "white", size = 2, alpha = 0.80) +
  # 
  geom_sf(data = lines, mapping = aes(color = line), 
          color = "white", alpha = 0.75, size = 1.7) +
  geom_sf(data = lines, mapping = aes(color = line, linetype = "Transit Lines\nby Color"), 
          alpha = 0.95, size = 1.25) +
  # Add labels
  geom_sf_label(data = areas, mapping = aes(label = neighborhood), 
                alpha = 0.8,
                nudge_x = c(1500,  # Dorchester
                            0, # Downtown
                            1000),
                nudge_y = c(000, 
                            0, 
                            500)) + # Roxbury
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white")) +
  # Color in T-lines
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey")) +
  scale_linetype_manual(values = "solid") +
  guides(
    color = "none",
    linetype = guide_legend(order = 2, override.aes = list(size = 3, color = "black"))) +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                    breaks = c("Verified API Sites",
                               "Verified New Sites",
                               "API Sites Not Found",
                               "Not social infrastructure"),
                    labels = c("Verified\nAPI Sites\n",
                               "Verified\nNew Sites\n",
                               "API Sites\nNot Found\n",
                               "Not social\ninfrastructure\n"),
                    guide = guide_legend(
                      order = 1, 
                      override.aes = list(size = 5, color = "black"))) +
  labs(linetype = NULL,color = NULL,
       fill = "Site Verified?",
       subtitle = paste("Total Social Infrastructure <sup>1</sup> ", "(n = ", nobs, ")", sep = "")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.title.position = "plot",
        plot.subtitle = ggtext::element_markdown(size = 14, hjust = 0.5))


nobs_co <- points %>%
  filter(group == "Community Spaces") %>%
  dim() %>% .[1]

## Small Plot
g2 <- ggplot() +
  geom_sf(data = zone, color = "darkgrey", size = 5) +
  geom_sf(data = zone, fill = "black") +
  geom_sf(data = tiles, alpha = 0.25, fill = "grey", color = "white", size = 0.1) +
  # 
  geom_sf(data = points %>%
            filter(group == "Community Spaces"), mapping = aes(fill = status), 
          shape = 21, color = "white", size = 2, alpha = 0.80) +
  # 
  #geom_sf(data = lines, mapping = aes(color = line), color = "white", alpha = 0.75, size = 1.5) +
  geom_sf(data = lines, mapping = aes(color = line), alpha = 0.75, size = 1) +
  # Color in T-lines
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey"),
    guide = guide_legend(order = 2, override.aes = list(size = 3)))  +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                    breaks = c("Verified API Sites",
                               "Verified New Sites",
                               "API Sites Not Found",
                               "Not social infrastructure"),
                    labels = c("Verified\nAPI Sites\n",
                               "Verified\nNew Sites\n",
                               "API Sites\nNot Found\n",
                               "Not social\ninfrastructure\n"),
                    guide = guide_legend(
                      order = 1, 
                      override.aes = list(size = 5, color = "black"))) +
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white"),
        plot.title = element_text(size = 16, hjust = 0.5),
        legend.text = element_text(size = 14, hjust = 0),
        plot.subtitle = element_text(size = 14, hjust = 0.5)) +
  geom_text(data = data.frame(
    x = 1904870.7,
    y = 521766.6, 
    label = paste("(n = ", nobs_co, ")", sep = "")), 
    mapping = aes(
      x = x, y = y, label = label), size = 5, hjust = 0) +
  labs(color = "Transit\nLine",
       fill = "Site Verified?",
       subtitle = "Community Spaces") +
  theme(legend.position = "left",
        plot.title.position = "plot",
        plot.margin = margin(0,0,0,0,"cm"))

nobs_pl <- points %>%
  filter(group == "Places of Worship") %>%
  dim() %>% .[1]

## Small Plot
g3 <- ggplot() +
  geom_sf(data = zone, color = "darkgrey", size = 5) +
  geom_sf(data = zone, fill = "black") +
  geom_sf(data = tiles, alpha = 0.25, fill = "grey", color = "white", size = 0.1) +
  # 
  geom_sf(data = points %>%
            filter(group == "Places of Worship"), mapping = aes(fill = status), 
          shape = 21, color = "white", size = 2, alpha = 0.80) +
  # 
  #geom_sf(data = lines, mapping = aes(color = line), color = "white", alpha = 0.75, size = 1.5) +
  geom_sf(data = lines, mapping = aes(color = line), alpha = 0.75, size = 1) +
  # Color in T-lines
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey"),
    guide = guide_legend(order = 2, override.aes = list(size = 3)))  +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                    breaks = c("Verified API Sites",
                               "Verified New Sites",
                               "API Sites Not Found",
                               "Not social infrastructure"),
                    labels = c("Verified\nAPI Sites\n",
                               "Verified\nNew Sites\n",
                               "API Sites\nNot Found\n",
                               "Not social\ninfrastructure\n"),
                    guide = guide_legend(
                      order = 1, 
                      override.aes = list(size = 5, color = "black"))) +
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white"),
        plot.title = element_text(size = 16, hjust = 0.5),
        legend.text = element_text(size = 14, hjust = 0),
        plot.subtitle = element_text(size = 14, hjust = 0.5)) +
  geom_text(data = data.frame(
    x = 1904870.7,
    y = 521766.6, 
    label = paste("(n = ", nobs_pl, ")", sep = "")), 
    mapping = aes(
      x = x, y = y, label = label), size = 5, hjust = 0) +
  labs(color = "Transit\nLine",
       fill = "Site Verified?",
       subtitle = "Places of Worship") +
  theme(legend.position = "left",
        plot.title.position = "plot",
        plot.margin = margin(0,0,0,0,"cm"))



nobs_sb <- points %>%
  filter(group == "Social Businesses") %>%
  dim() %>% .[1]

## Small Plot
g4 <- ggplot() +
  geom_sf(data = zone, color = "darkgrey", size = 5) +
  geom_sf(data = zone, fill = "black") +
  geom_sf(data = tiles, alpha = 0.25, fill = "grey", color = "white", size = 0.1) +
  # 
  geom_sf(data = points %>%
            filter(group == "Social Businesses"), mapping = aes(fill = status), 
          shape = 21, color = "white", size = 2, alpha = 0.80) +
  # 
  #geom_sf(data = lines, mapping = aes(color = line), color = "white", alpha = 0.75, size = 1.5) +
  geom_sf(data = lines, mapping = aes(color = line), alpha = 0.75, size = 1) +
  # Color in T-lines
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey"),
    guide = guide_legend(order = 2, override.aes = list(size = 3)))  +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                    breaks = c("Verified API Sites",
                               "Verified New Sites",
                               "API Sites Not Found",
                               "Not social infrastructure"),
                    labels = c("Verified\nAPI Sites\n",
                               "Verified\nNew Sites\n",
                               "API Sites\nNot Found\n",
                               "Not social\ninfrastructure\n"),
                    guide = guide_legend(
                      order = 1, 
                      override.aes = list(size = 5, color = "black"))) +
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white"),
        plot.title = element_text(size = 16, hjust = 0.5),
        legend.text = element_text(size = 14, hjust = 0),
        plot.subtitle = element_text(size = 14, hjust = 0.5)) +
  geom_text(data = data.frame(
    x = 1904870.7,
    y = 521766.6, 
    label = paste("(n = ", nobs_sb, ")", sep = "")), 
    mapping = aes(
      x = x, y = y, label = label), size = 5, hjust = 0) +
  labs(color = "Transit\nLine",
       fill = "Site Verified?",
       subtitle = "Social Businesses") +
  theme(legend.position = "left",
        plot.title.position = "plot",
        plot.margin = margin(0,0,0,0,"cm"))


nobs_pr <- points %>%
  filter(group == "Parks") %>%
  dim() %>% .[1]

## Small Plot
g5 <- ggplot() +
  geom_sf(data = zone, color = "darkgrey", size = 5) +
  geom_sf(data = zone, fill = "black") +
  geom_sf(data = tiles, alpha = 0.25, fill = "grey", color = "white", size = 0.1) +
  # 
  geom_sf(data = points %>%
            filter(group == "Parks"), mapping = aes(fill = status), 
          shape = 21, color = "white", size = 2, alpha = 0.80) +
  # 
  #geom_sf(data = lines, mapping = aes(color = line), color = "white", alpha = 0.75, size = 1.5) +
  geom_sf(data = lines, mapping = aes(color = line), alpha = 0.75, size = 1) +
  # Color in T-lines
  scale_color_manual(
    breaks = c("ORANGE", "RED", "GREEN","BLUE","SILVER", "MULTIPLE"),
    labels = c("Orange", "Red", "Green", "Blue", "Silver", "Multiple"),
    values = c("#FE6100", # Orange
               "#DC267F", # Red
               "#34AA6F", # Green
               "#648FFF", # Blue
               "#6F94AC", # Silver
               "darkgrey"),
    guide = guide_legend(order = 2, override.aes = list(size = 3)))  +
  scale_fill_manual(values = c(mycolors[3], mycolors[5], "grey", "black"),
                    breaks = c("Verified API Sites",
                               "Verified New Sites",
                               "API Sites Not Found",
                               "Not social infrastructure"),
                    labels = c("Verified\nAPI Sites\n",
                               "Verified\nNew Sites\n",
                               "API Sites\nNot Found\n",
                               "Not social\ninfrastructure\n"),
                    guide = guide_legend(
                      order = 1, 
                      override.aes = list(size = 5, color = "black"))) +
  theme_void(base_size = 14) +
  theme(plot.background = element_rect(fill = "white", color = "white"),
        plot.title = element_text(size = 16, hjust = 0.5),
        legend.text = element_text(size = 14, hjust = 0),
        plot.subtitle = element_text(size = 14, hjust = 0.5)) +
  geom_text(data = data.frame(
    x = 1904870.7,
    y = 521766.6, 
    label = paste("(n = ", nobs_pr, ")", sep = "")), 
    mapping = aes(
      x = x, y = y, label = label), size = 5, hjust = 0) +
  labs(color = "Transit\nLine",
       fill = "Site Verified?",
       subtitle = "Parks") +
  theme(legend.position = "left",
        plot.title.position = "plot",
        plot.margin = margin(0,0,0,0,"cm"))


```

```{r}
htmltools::tagList(fontawesome::fa_html_dependency())

library(fontawesome)
library(rsvg)
library(magick)

get_image = function(myname){
  fa_png(name = myname,
         fill = "grey", fill_opacity = 0.25, 
         file = paste("fa/", myname, ".png", sep = ""))

    data.frame(icon = paste("fa/", myname, ".png", sep = "")) %>%
      return()
}

myimages <- c("hands-helping",
  "store-alt",
  "place-of-worship",
  "coffee",
  "tree") %>%
  map_dfr(~get_image(.)) %>%
  mutate(type = c("Total", "Community Spaces", 
                      "Places of Worship", "Social Businesses","Parks")) %>%
  
  mutate(type = factor(type, levels = c("Total", "Community Spaces",
                                         "Places of Worship",
                                         "Social Businesses", "Parks")))


mybars <- bind_rows(
  points %>%
    as.data.frame() %>%
    group_by(status) %>%
    summarize(type = "Total",
              count = n()),
  points %>%
    as.data.frame() %>%
    filter(group == "Community Spaces") %>%
    group_by(status) %>%
    summarize(type = group[1],
              count = n()),
  
  points %>%
    as.data.frame() %>%
    filter(group == "Places of Worship") %>%
    group_by(status) %>%
    summarize(type = group[1],
              count = n()),
  points %>%
    as.data.frame() %>%
    filter(group == "Social Businesses") %>%
    group_by(status) %>%
    summarize(type = group[1],
              count = n()),
  points %>%
    as.data.frame() %>%
    filter(group == "Parks") %>%
    group_by(status) %>%
    summarize(type = group[1],
              count = n())) %>%
  group_by(type) %>%
  mutate(placement = max(count) / 2,
         percent = count / sum(count) * 100) %>%
  ungroup() %>%
  mutate(type = factor(type, levels = c("Total", "Community Spaces",
                                         "Places of Worship",
                                         "Social Businesses", "Parks")))

somebars <- mybars %>%
  filter(type != "Total") %>%
  mutate(status = factor(status, levels = c(
    "Verified API Sites",
    "Verified New Sites",
    "API Sites Not Found",
    "Not social infrastructure"))) %>%
  mutate(order = as.numeric(status))

someimages <- myimages  %>%
  filter(type != "Total") %>%
  mutate(status = factor(
    x = "Not social infrastructure",
    levels = somebars$status %>% levels()),
    order = as.numeric(status))

g6 <- somebars %>%
  ggplot(data = somebars,
         mapping = aes(x = reorder(status, -order), 
                       y = count, fill = status)) +
  # Plot an invisible layer of bars first
  # this will get the bar order fixed in place,
  # that geom_image doesn't overwrite it. 
  geom_col(data = somebars, 
           mapping = aes(x = reorder(status, -order), 
                         y = count,
                         fill = status),
           width = 0.5, fill = NA, color = NA) +
  geom_image(data = someimages, 
             mapping = aes(x = reorder(status, -order),
                           y = 150, image= icon),
             size = 0.5, nudge_x = 1) +
  
  geom_col(width = 0.5, color = "#333333") +
  facet_wrap(~type,  ncol = 4) +
  coord_flip() +
  labs(x = NULL,
       y = "# of Sites") +
  scale_fill_manual(
    values = c(mycolors[3], mycolors[5],  "grey", "black"),
    breaks = c("Verified API Sites",
               "Verified New Sites",
               "API Sites Not Found",
               "Not social infrastructure"),
    labels = c("Verified API Sites\n",
               "Verified New Sites\n",
               "API Sites Not Found\n",
               "Not social infrastructure\n")) +
  guides(fill = "none") +
  geom_text(data = somebars,
            mapping = aes(x = reorder(status, -order),
                          y = count, label = count), 
            nudge_y = 5, hjust = 0) +
  theme_bw(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        panel.spacing = unit(0.5, "cm"),
        strip.text = element_blank(),
        strip.background = element_blank(),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0)) +
  scale_y_continuous(breaks = c(0, 100, 200, 300),
                     limits = c(0, 315)) 
#  labs(caption = "<sup>1</sup> <b>Points</b> show social infrastructure sites (A-B). <b>Cells</b> show validation study area (n = 73 1 km<sup>2</sup> cells, A-B).<br><b>Bars</b> tally sites classified by human hand coding (A-B). <b>Small panels</b> tally social infrastructure by type (B).")


totalbars <- mybars %>%
  filter(type == "Total") %>%
  mutate(status = factor(status, levels = c(
    "Verified API Sites",
    "Verified New Sites",
    "API Sites Not Found",
    "Not social infrastructure"))) %>%
  mutate(order = as.numeric(status))

totalimages <- myimages  %>%
  filter(type == "Total") %>%
  mutate(status = factor("Not social infrastructure",
                         levels = totalbars$status %>% levels()),
         order = as.numeric(status))


g7 <- totalbars %>%
  ggplot(mapping = aes(x = reorder(status, order), 
                       y = count,
                       fill = status)) +
  # Plot an invisible layer of bars first
  # this will get the bar order fixed in place,
  # that geom_image doesn't overwrite it. 
  geom_col(data = totalbars, 
           mapping = aes(x = reorder(status, -order), 
                         y = count,
                         fill = status),
           width = 0.5, fill = NA, color = NA) +
  # Plot background image
  geom_image(data = totalimages,
             mapping = aes(x = reorder(status, -order),
                           y = 300, image= icon),
             size = 0.5, nudge_x = 1)  +
  # Plot columns overtop
  geom_col(data = totalbars, 
           mapping = aes(x = reorder(status, -order), 
                         y = count,
                         fill = status),
           width = 0.5, color = "#333333") +
  facet_wrap(~type,  ncol = 4) +
  coord_flip() +
  labs(x = NULL,
       y = "# of Sites") +
  scale_fill_manual(
    values = c(mycolors[3],mycolors[5],   "grey", "black"),
    breaks = c("Verified API Sites",
               "Verified New Sites",
               "API Sites Not Found",
               "Not social infrastructure"))  +
  guides(fill = "none") +
    geom_text(data = totalbars,
              mapping = aes(x= reorder(status, -order), 
                            y = 0, label = status),
              nudge_x = 0.4, hjust = 0, color = "#444444") + 
  ggtext::geom_richtext(
    mapping = aes(x = status, y = count, 
                  label = count),fontface = "bold",
    fill = NA, label.color = NA,
    nudge_y = 5, hjust = 0, color = "black") +
  theme_bw(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        panel.spacing = unit(0.5, "cm")) +
  scale_y_continuous(breaks = c(0, 100, 200, 300, 400, 500, 600),
                     labels = c(0, 100, 200, 300, 400, 500, 600),
                     limits = c(0,600))
```


```{r}

#ggsave(g1, filename = "viz/validating_map.png", dpi = 500, width = 5, height = 5)
first <- ggpubr::ggarrange(g1, g7,
                           ncol = 2, widths =  c(0.65, 0.35), 
                           labels = c("A", ""), hjust = c(-0.5, 0.5)) 
second <- ggpubr::ggarrange(g2, g3, g4, g5, ncol = 4, legend = "none")

total <- ggpubr::ggarrange(first, second, g6, nrow = 3, heights = c(0.45, 0.25, 0.30),
                           labels = c("", "B", ""), vjust = c(0, -0.25, 0)) +
  bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(total, filename = "viz/figure_2.png", dpi = 500, width = 8, height = 10)
```


# 11. Preliminary Validation Tests

```{r}
library(tidyverse)
library(sf)
library(rgdal)
# Equal Area projection
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"
# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(description, timestamp:icon)) %>%
    # convert to equal area conic projection
  st_transform(crs = aea) %>%
  select(-Name) %>%
  # Zoom into inner boston
  filter(str_detect(milestone, "M1|M2|M3"))

# Import grid cells sampled for ground truthing
blocks <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  filter(type == "masters")  %>%
  select(cell_id = Name, description, neighborhood:geometry) %>%
  # Transform to Equal Area conic projection
  st_transform(crs = aea)

# Import original points
mygoogle <- read_sf("mygoogle.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  st_transform(crs = aea)

# Import online-checked points
mychecked <- read_sf("mysites_core_boston.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  st_transform(crs = aea)

# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds") %>%
  st_transform(crs = aea)
```


Count number of sites recorded

```{r}
datgoogle <- bind_rows(
  # Get total sites per grid cell
  mygoogle %>% 
    as.data.frame() %>%
    group_by(cell_id) %>%
    summarize(
      group = "Total",
      count = n()),
  # Get sites by type per grid cell
  mygoogle %>% 
    as.data.frame() %>%
    group_by(cell_id, group) %>%
    summarize(count = n())
) %>%
  mutate(type = "googled")

datchecked <- bind_rows(
  # Get total sites per grid cell
  mychecked %>% 
    as.data.frame() %>%
    group_by(cell_id) %>%
    summarize(
      group = "Total",
      count = n()),
  # Get sites by type per grid cell
  mychecked %>% 
    as.data.frame() %>%
    group_by(cell_id, group) %>%
    summarize(count = n())
) %>%
  mutate(type = "checked")


datground <- bind_rows(
  # Get total sites per grid cell
  myground %>% 
    as.data.frame() %>%
    group_by(cell_id) %>%
    summarize(
      group = "Total",
      count = n()),
  # Get sites by type per grid cell
  myground %>% 
    as.data.frame() %>%
    group_by(cell_id, group) %>%
    summarize(count = n())
) %>%
  mutate(type = "ground")
```


```{r}
bind_rows(
  grid %>%
    mutate(group = "Total"),
  grid %>%
    mutate(group = "Community Spaces"),
  grid %>%
    mutate(group = "Places of Worship"),
  grid %>%
    mutate(group = "Social Businesses"),
  grid %>%
    mutate(group = "Parks"),
  grid %>%
    mutate(group = "Other")) %>%
  # Join in tally data
  left_join(by = c("cell_id", "group"), 
            y = bind_rows(datgoogle, datchecked, datground) %>%
              pivot_wider(id_cols = c(cell_id, group),
                          names_from = c(type),
                          names_sep = "_",
                          values_from = count)) %>%
  # Fill in cells that were analyzed
  mutate(
    # All cells
    googled = if_else(is.na(googled), 0, as.numeric(googled)),
    checked = if_else(is.na(checked), 0, as.numeric(checked)),
    # For ground truthed cells, this means just those in our sampled cells
    # Currently just masters-student-blocks
    ground = if_else(is.na(ground) & cell_id %in% blocks$cell_id,
                     0, as.numeric(ground) )) %>%
  # Rearrange the grid
  select(cell_id, neighborhood, milestone,
         group, googled, checked, ground,
         pop_density:pop_age_65_plus, pop_density_int) %>%
  saveRDS("tally.rds")
```

### Sheer Correlation

#### Figure 3

```{r, eval = FALSE}
library(tidyverse)
library(sf)
library(ggtext)
library(ggpubr)

tally <- read_rds("tally.rds") %>%
  mutate_at(vars(checked, googled, ground),
            funs(. / pop_density_int * 1000)) %>%
  filter(group  != "Other")

mydat <- bind_rows(
  tally %>%
    select(cell_id, x = checked, y = googled, group) %>%
    mutate(type = "<b>Online<br>Map</b><br>
           <br>
           <i>(Manual<br>Online<br>Searches)</i>
           <br>
           <br>73 cells"),
  tally %>%
    select(cell_id, x = ground, y = googled, group) %>%
    mutate(type = "<b>Ground<br>Truthed<br>Map</b><br>
           <br>
           <i>(In-Person<br>Visits)</i>
           <br>
           <br>10 cells")) %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            moderndive::get_regression_points() %>%
            select(y_hat, y, x), .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) %>%
  mutate(residual = abs(y - y_hat))

mybands <- mydat %>%
  group_by(group) %>%
  mutate(xmax = max(x)) %>%
  ungroup() %>%
  group_by(type, group) %>%
  summarize(lm(formula = y ~ x) %>%
            predict(
              se = TRUE,
              newdata = data.frame(
                x = seq(from = 0, to = max(xmax), length.out = 20))) %>%
            as.data.frame() %>%
            mutate(x = seq(from = 0, to = max(xmax), length.out = 20))) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 


totaly <- mydat %>%
  as.data.frame() %>%
  group_by(type) %>%
  summarize(yrange = range(y, na.rm = TRUE) %>% mean(),
            y100 = max(y, na.rm = TRUE) - yrange / 8,
            y75 = y100 - yrange / 2.5,
            y = min(y, na.rm = TRUE) + yrange)

myy <- mydat %>%
  as.data.frame() %>%
  filter(group != "Total\n") %>%
  group_by(type) %>%
  summarize(yrange = range(y, na.rm = TRUE) %>% mean(),
            y100 = max(y, na.rm = TRUE) - yrange / 8,
            y75 = y100 - yrange / 2.5,
            y = min(y, na.rm = TRUE) + yrange)
  
myx <- mydat %>%
  as.data.frame() %>%
  group_by(group) %>%
  summarize(xrange = range(x, na.rm = TRUE) %>% mean(),
            xmin = min(x, na.rm = TRUE) + xrange / 20,
            x = min(x, na.rm = TRUE) + xrange)

myposition <- bind_rows(
  expand_grid(myx, myy) %>%
    filter(group != "Total\n"),
  expand_grid(myx, totaly) %>%
    filter(group == "Total\n"))

mycorr <- mydat %>%
  as.data.frame() %>%
  group_by(type, group) %>%
  summarize(cor = cor(x, y, use = "pairwise.complete.obs"),
            cor = paste("r = ", round(cor, 2), sep = "")) %>%
  left_join(by = c("type", "group"), y = myposition) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) %>%
  mutate(group = factor(group, levels = levels(mydat$group)))


mystats <- mydat %>%
  as.data.frame() %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            broom::glance(), .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  select(type, group, r2 = r.squared, sigma) %>%
  mutate(r2 = paste(round(r2*100, 0), "% accuracy (R<sup>2</sup>)", sep = ""),
         sigma = paste("Avg. Error = ", round(sigma, 1), sep = "")) %>%
  left_join(by = c("type", "group"), y = myposition) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) %>%
  mutate(group = factor(group, levels = levels(mydat$group)))

mygroup <- mydat$group %>% levels()
mytype <- mydat$type%>% levels()

totaldat <- mydat %>%
  filter(group == "Total\n")
totalcorr <- mycorr %>%
  filter(group == "Total\n") 
totalstats <- mystats %>%
  filter(group == "Total\n") 
totalbands <- mybands %>%
  filter(group == "Total\n")


mygroup <- mydat$group %>% levels() %>% .[-1]

somedat <- mydat %>%
  filter(group != "Total\n") 
somecorr <- mycorr %>%
  filter(group != "Total\n")
somestats <- mystats %>%
  filter(group != "Total\n")
somebands <- mybands %>%
  filter(group != "Total\n")


g1 <- totaldat %>%
  ggplot(mapping = aes(x = x, y = y)) +
  geom_point(mapping = aes(fill = group),
             alpha = 0.75, shape = 21, color = "white", 
             size = 3) +
  scale_fill_manual(values = c("black"),
                    guide = "none") +
  geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
                 color = "grey", size = 0.5, alpha = 0.5) +
  geom_ribbon(data = totalbands, 
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group), color = NA, alpha = 0.5) +
  geom_line(data = totalbands, 
              mapping = aes(x = x, y = fit), 
            color = "black", size = 1, linetype = "dashed") +
#  geom_smooth(method = "lm", linetype = "dashed",
#              color = "black", size = 1,
#              mapping = aes(fill = group)) +
  geom_richtext(data = totalcorr, 
                mapping = aes(x = 0,
                              y = y,
                              label = cor),
                hjust = 0, label.color = NA, fill = NA) +
  geom_richtext(data = totalstats,
                mapping = aes(x = 0, y = y100,
                              label = r2),
                hjust = 0, label.color = NA, fill = NA) +
  geom_richtext(data = totalstats,
                mapping = aes(x = 0, y = y75,
                              label = sigma),
                hjust = 0, label.color = NA, fill = NA) +
  facet_grid(rows = vars(reorder(type, as.numeric(type))),
             cols = vars(reorder(group, as.numeric(group))), 
                         scales = "free") +
  theme_bw(base_size = 14) +
  labs(x = "",
       y = "Google API Rates of Sites per Grid Cell") +
  theme(strip.text.y = element_blank(),
        strip.background.y = element_blank(),
        panel.grid = element_blank(),
        plot.margin = margin(0,0,0,0, "cm"),
        strip.text.x = element_text(color = "white"))

g2 <- somedat %>%
  ggplot(mapping = aes(x = x, y = y)) +
  geom_point(alpha = 0.75, 
             mapping = aes(fill = group),
             shape = 21, color = "white", 
             size = 3) +
  scale_fill_manual(values = c("#0D0887", "#8405A7",
                               "#D35171","#d98d2b"),
                    guide = "none") +
  geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
                 color = "grey", size = 0.5, alpha = 0.5) +
  geom_ribbon(data = somebands, 
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group), color = NA, alpha = 0.5) +
  geom_line(data = somebands, 
            mapping = aes(x = x, y = fit), 
            color = "black", size = 1, linetype = "dashed") +
  geom_richtext(data = somecorr, 
                mapping = aes(x = 0,
                              y = y,
                              label = cor),
                hjust = 0, label.color = NA, fill = NA) +
  geom_richtext(data = somestats,
                mapping = aes(x = 0, y = y100,
                              label = r2),
                hjust = 0, label.color = NA, fill = NA) +
  geom_richtext(data = somestats,
                mapping = aes(x = 0, y = y75,
                              label = sigma),
                hjust = 0, label.color = NA, fill = NA) +
  facet_grid(rows = vars(reorder(type, as.numeric(type))),
             cols = vars(reorder(group, as.numeric(group))), 
             scales = "free") +
  theme_bw(base_size = 14) +
  labs(x = "",
       y = NULL) +
  theme(strip.text.y = ggtext::element_markdown(angle = 0, hjust = 0, size = 10),
        panel.grid = element_blank(),
        plot.margin = margin(0,0,0,0, "cm"),
        strip.text.x = element_text(color = "white"),
        panel.spacing.x = unit(0.5, "cm"))

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  ggpubr::as_ggplot(g) %>%
    return()
}


combo <- ggpubr::ggarrange(
  g1 %>% scale_panel_fill(side = "t", fill = "black"), 
  g2 %>% scale_panel_fill(side = "t", fill = c("#0D0887", "#8405A7",
                                               "#D35171","#d98d2b")), 
  widths = c(0.25, 1)) %>%
  ggpubr::annotate_figure(bottom = ggpubr::text_grob(
    label = "Rates of Human-Coded Sites per Grid Cell", 
    hjust = 0.5, size = 14, just = "bottom"))  %>%
  ggpubr::annotate_figure(top = ggpubr::text_grob(
    label = "Cells by Actual Rates of Social Infrastructure per 1,000 residents", 
    hjust = 0.5, size = 14, just = "top", vjust = 0.5)) +
    #bottom = ggpubr::text_grob(
    #label = "(Line indicates Bivariate OLS Line of Best Fit with 95% Confidence Intervals)", 
    #hjust = 0.5, size = 10, just = "bottom"))  +
  
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white") 

library(ggpubr)

ggsave(combo, filename = "viz/figure_3.png", 
       dpi = 500, width = 10, height = 5)

file.copy(from = "viz/figure_3.png", to = "viz/figure_B2.png")
```

Repeat this, but just as a very simple version for displaying in presentations.

```{r}
library(ggtext)
library(shadowtext)
gextra <- totaldat %>%
  filter(as.numeric(type) == 1, as.numeric(group) == 1) %>%
  ggplot(mapping = aes(x = x, y = y)) +
  scale_fill_manual(values = c("#0D0887"),
                    guide = "none") +
  geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
                 color = "#FAF9F6", size = 1, alpha = 0.5) +
  geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
                 color = "grey", size = 0.5, alpha = 0.5) +
  geom_point(mapping = aes(fill = group),
             alpha = 0.75, shape = 21, color = "#FAF9F6", 
             size = 3) +
  geom_ribbon(data = totalbands %>%
                  filter(as.numeric(type) == 1, as.numeric(group) == 1), 
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group), color = "#FAF9F6", alpha = 0.5) +
  geom_line(data = totalbands %>%
                filter(as.numeric(type) == 1, as.numeric(group) == 1), 
              mapping = aes(x = x, y = fit), 
            color = "black", size = 1, linetype = "dashed") +
#  geom_smooth(method = "lm", linetype = "dashed",
#              color = "black", size = 1,
#              mapping = aes(fill = group)) +
  geom_shadowtext(data = totalcorr %>%
                    filter(as.numeric(type) == 1, as.numeric(group) == 1), 
                mapping = aes(x = 0,
                              y = y,
                              label = cor),
                bg.r = 0.2, bg.color = "#FAF9F6", color = "black",
                hjust = 0, label.color = NA, fill = NA) +
  geom_shadowtext(data = totalstats %>%
                    filter(as.numeric(type) == 1, as.numeric(group) == 1) %>%
                  mutate(r2 = str_replace(r2, pattern = "R<sup>2</sup>", "R2")),
                mapping = aes(x = 0, y = y100,
                              label = r2),
                bg.r = 0.2, bg.color = "#FAF9F6", color = "black",
                hjust = 0, label.color = NA, fill = NA) +
  geom_shadowtext(data = totalstats %>%
                    filter(as.numeric(type) == 1, as.numeric(group) == 1),
                mapping = aes(x = 0, y = y75,
                              label = sigma),
                bg.r = 0.2, bg.color = "#FAF9F6", color = "black",
                hjust = 0, label.color = NA, fill = NA) +
  theme_bw(base_size = 14) +
  labs(x = "Rates of Validated Sites",
       y = "Rates of API Sites") +
  theme(strip.text = element_blank(),
        strip.background = element_blank(),
        panel.grid = element_blank(),
        axis.ticks = element_blank(),
        plot.margin = margin(0,0,0,0, "cm")) +
  coord_fixed(ratio = 1)

ggsave(gextra, filename = "processed_queries/bestfit_extra.png", dpi = 500, width = 2.5, height = 3)

magick::image_read("processed_queries/bestfit_extra.png") %>%
  magick::image_transparent(color = "white") %>%
  magick::image_write("processed_queries/bestfit_extra.png", quality = 100)
```

```{r, eval = FALSE}

#mydat %>% 
#  filter(type == "Ground\nTruthed\nMap\n(n = 10\ncells)",
#         group == "Parks") %>%
# It's just that outlier
#  filter(y < 3) %>%
#  lm(formula = y ~ x) %>%
# summary()

mydat %>% 
  filter(type == "Ground\nTruthed\nMap\n(n = 10\ncells)",
         group == "Parks") %>%
  mutate(x = ntile(x, 5),
         y = ntile(y, 5)) %>%
  lm(formula = y ~ x) %>%
  broom::glance()
```

### Ranks



#### Figure B3: Z-score

```{r, eval = FALSE}
tally <- read_rds("tally.rds") %>%
  mutate_at(vars(checked, googled, ground),
            funs(. / pop_density_int * 1000)) %>%
  filter(group  != "Other") %>%
  # For each group, rank cells from smallest to highest
  group_by(group) %>%
  mutate_at(vars(checked, googled, ground),
            # By quintiles
            funs(scale(.) %>% as.numeric()))


mydat <- bind_rows(
    tally %>%
    select(cell_id, x = checked, y = googled, group) %>%
    mutate(type = "<b>Online<br>Map</b><br>
           <br>
           <i>(Manual<br>Online<br>Searches)</i>
           <br>
           <br>73 cells"),
  tally %>%
    select(cell_id, x = ground, y = googled, group) %>%
    mutate(type = "<b>Ground<br>Truthed<br>Map</b><br>
           <br>
           <i>(In-Person<br>Visits)</i>
           <br>
           <br>10 cells")) %>%
 mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 
 
mybands <- mydat %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            predict(
              se = TRUE,
              newdata = data.frame(
                x = seq(from = -2.5, to = 2.5, length.out = 20))) %>%
            as.data.frame() %>%
            mutate(x = seq(from = -2.5, to = 2.5, length.out = 20)), 
          .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 


mycorr <- mydat %>%
  as.data.frame() %>%
  group_by(type, group) %>%
  summarize(cor = cor(x, y, use = "pairwise.complete.obs"),
            cor = paste("r = ", round(cor, 2), sep = "")) %>%
  mutate(x = -2.5, y = 0.75)

mystats <- mydat %>%
  as.data.frame() %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            broom::glance(), .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  select(type, group, r2 = r.squared, sigma) %>%
  mutate(r2 = paste(round(r2*100, 0), "% accuracy (R<sup>2</sup>)", sep = ""),
         sigma = paste("Avg. Error = ", round(sigma, 1), sep = "")) %>%
  mutate(x = -2.5, y_r2 = 2.25, y_sigma = 1.50) %>%
  mutate(group = factor(group, levels = levels(mydat$group)))

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}


g2 <- mydat %>%
  # Plot main aesthetics
  ggplot(mapping = aes(x = x, y = y)) +
  # Add nice lines from the predicted value to the observed
  #geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
  #               color = "grey", size = 0.5, alpha = 0.5) +
  # Give a point for every cells
  geom_jitter(alpha = 0.75, width = .5, height = .5,
              mapping = aes(fill = group),
              shape = 21, color = "white", 
              size = 3, alpha = 0.5) +
  # Colored based on this schema, by type of social infrastructure
  scale_fill_manual(values = c(
    "black","#0D0887", "#8405A7",
    "#D35171","#d98d2b"),
    guide = "none") +
  # Add a line of best fit and confidence interval,
  # whose fill matches the point color
  geom_ribbon(data = mybands,
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group),
              color = NA, alpha = 0.5) +
    geom_line(data = mybands, mapping = aes(x = x, y = fit,
                                            fill = group),
              color = "black", linetype = "dashed", size = 1) +
  #geom_smooth(method = "lm", linetype = "dashed",
  #            color = "black", size = 1,
  #            mapping = aes(fill = group)) +
  # Add a label for R2
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Outline"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"), 
                fill = "white", color = "white",  alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Value"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"),
                fill = NA, color = "black") +
  # Sigma
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Outline"),
                hjust = 0, label.color = NA,  
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "Black") +
    # Add a label for the correlation
  # First put a blank background
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Outline"),
                hjust = 0, label.color = NA,
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "black") +
  scale_size_manual(
    breaks = c("Outline", "Value"),
    values = c(3.5, 3.5),
    guide = "none") +
  facet_grid(rows = vars(reorder(type, as.numeric(type))),
             cols = vars(reorder(group, as.numeric(group)))) +
  theme_bw(base_size = 14) +
  labs(
    subtitle = "Cells Ranked by Z-Score (Standard Deviations from Sample Mean)",
    x = "Ranking of Human-Coded Sites per Grid Cell",
       y = "Google API Ranking of Sites per Grid Cell") +
#    caption = "(Line indicates Bivariate OLS Line of Best Fit with 95% Confidence Intervals. Points jittered by 0.5 for clarity.)") +
  theme(
    strip.text.y = ggtext::element_markdown(angle = 0, hjust = 0, size = 10),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    plot.caption = element_text(size = 10, hjust = 0.5),
    panel.grid = element_blank(),
    plot.margin = margin(0,0,0,0, "cm"),
    strip.text.x = element_text(color = "white"),
    panel.spacing.x = unit(0.5, "cm")) +
  scale_y_continuous(breaks = seq(from = -2, to = 2, length.out = 5),
                     labels = seq(from = -2, to = 2, length.out = 5) %>%
                       round(1)) +
  scale_x_continuous(breaks = seq(from = -2, to = 2, length.out = 5),
                     labels = seq(from = -2, to = 2, length.out = 5) %>%
                       round(1)) +
  coord_fixed(xlim = c(-2.5,2.5),
              ylim = c(-2.5,2.5)) 

combo <- g2 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_B3.png", 
       dpi = 500, width = 10, height = 5)
```


#### Figure B5: Quintile

```{r, eval = FALSE}
# Test
read_rds("tally.rds") %>%
  mutate_at(vars(checked, googled, ground),
            funs(. / pop_density_int * 1000)) %>%
  filter(group  != "Other") %>%
  # For each group, rank cells from smallest to highest
  group_by(group) %>%
  mutate_at(vars(checked, googled, ground),
            # By quintiles
            funs(ntile(., 5))) %>%
  filter(group == "Parks") %>%
  as_data_frame() %>%
  select(x = ground, y = googled)  %>%
  lm(formula = y ~ x) %>%
  summary()
```

```{r, eval = FALSE}
tally <- read_rds("tally.rds") %>%
  mutate_at(vars(checked, googled, ground),
            funs(. / pop_density_int * 1000)) %>%
  filter(group  != "Other") %>%
  # For each group, rank cells from smallest to highest
  group_by(group) %>%
  mutate_at(vars(checked, googled, ground),
            # By quintiles
            funs(ntile(., 5)))

mydat <- bind_rows(
  
    tally %>%
    select(cell_id, x = checked, y = googled, group) %>%
    mutate(type = "<b>Online<br>Map</b><br>
           <br>
           <i>(Manual<br>Online<br>Searches)</i>
           <br>
           <br>73 cells"),
  tally %>%
    select(cell_id, x = ground, y = googled, group) %>%
    mutate(type = "<b>Ground<br>Truthed<br>Map</b><br>
           <br>
           <i>(In-Person<br>Visits)</i>
           <br>
           <br>10 cells")) %>%
 mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 
 
mybands <- mydat %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            predict(
              se = TRUE,
              newdata = data.frame(
                x = seq(from = 0, to = 6, length.out = 20))) %>%
            as.data.frame() %>%
            mutate(x = seq(from = 0, to = 6, length.out = 20)), 
          .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev()))

mycorr <- mydat %>%
  as.data.frame() %>%
  group_by(type, group) %>%
  summarize(cor = cor(x, y, use = "pairwise.complete.obs"),
            cor = paste("r = ", round(cor, 2), sep = "")) %>%
  #left_join(by = c("type", "group"), y = myposition) %>%
  mutate(x = 0, y = 4) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 


mystats <- mydat %>%
  as.data.frame() %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            broom::glance(), .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  select(type, group, r2 = r.squared, sigma) %>%
  mutate(r2 = paste(round(r2*100, 0), "% accuracy (R<sup>2</sup>)", sep = ""),
         sigma = paste("Avg. Error = ", round(sigma, 1), sep = "")) %>%
  #left_join(by = c("type", "group"), y = myposition) %>%
  mutate(x = 0, y_r2 = 5.5, y_sigma = 4.75) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) %>%
  mutate(group = factor(group, levels = levels(mydat$group)))



# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}


g2 <- mydat %>%
  # Plot main aesthetics
  ggplot(mapping = aes(x = x, y = y)) +
  # Add nice lines from the predicted value to the observed
  #geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
  #               color = "grey", size = 0.5, alpha = 0.5) +
  # Give a point for every cells
  geom_jitter(alpha = 0.75, width = .5, height = .5,
              mapping = aes(fill = group),
              shape = 21, color = "white", 
              size = 3, alpha = 0.5) +
  # Colored based on this schema, by type of social infrastructure
  scale_fill_manual(values = c(
    "black","#0D0887", "#8405A7",
    "#D35171","#d98d2b"),
    guide = "none") +
  # Add a line of best fit and confidence interval,
  # whose fill matches the point color
  geom_ribbon(data = mybands,
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group),
              color = NA, alpha = 0.5) +
  geom_line(data = mybands, mapping = aes(x = x, y = fit,
                                            fill = group),
              color = "black", linetype = "dashed", size = 1) +
  # Add a label for R2
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Outline"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"), 
                fill = "white", color = "white",  alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Value"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"),
                fill = NA, color = "black") +
  # Sigma
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Outline"),
                hjust = 0, label.color = NA,  
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "Black") +
    # Add a label for the correlation
  # First put a blank background
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Outline"),
                hjust = 0, label.color = NA,
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "black") +
  scale_size_manual(
    breaks = c("Outline", "Value"),
    values = c(3.5, 3.5),
    guide = "none") +
  facet_grid(rows = vars(reorder(type, as.numeric(type))),
             cols = vars(reorder(group, as.numeric(group)))) +
  theme_bw(base_size = 14) +
  labs(
    subtitle = "Cells Ranked by Quintile (1 = Lowest, 5 = Highest)",
    x = "Ranking of Human-Coded Sites per Grid Cell",
       y = "Google API Ranking of Sites per Grid Cell") +
#    caption = "(Line indicates Bivariate OLS Line of Best Fit with 95% Confidence Intervals. Points jittered by 0.5 for clarity.)") +
  theme(
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    plot.caption = element_text(size = 10, hjust = 0.5),
    strip.text.y = ggtext::element_markdown(angle = 0, hjust = 0, size = 10),
    panel.grid = element_blank(),
    plot.margin = margin(0,0,0,0, "cm"),
    strip.text.x = element_text(color = "white"),
    panel.spacing.x = unit(0.5, "cm")) +
  scale_y_continuous(breaks = c(1:5),
                     labels = c(1:5)) +
  scale_x_continuous(breaks = c(1:5),
                     labels = c(1:5)) +
  coord_fixed(xlim = c(0,6),
              ylim = c(0,6))

combo <- g2 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_B5.png", 
       dpi = 500, width = 10, height = 5)

```

#### Figure B4: Decile

```{r, eval = FALSE}
tally <- read_rds("tally.rds") %>%
  mutate_at(vars(checked, googled, ground),
            funs(. / pop_density_int * 1000)) %>%
  filter(group  != "Other") %>%
  # For each group, rank cells from smallest to highest
  group_by(group) %>%
  mutate_at(vars(checked, googled, ground),
            # By quintiles
            funs(ntile(., 10)))

mydat <- bind_rows(
  
    tally %>%
    select(cell_id, x = checked, y = googled, group) %>%
    mutate(type = "<b>Online<br>Map</b><br>
           <br>
           <i>(Manual<br>Online<br>Searches)</i>
           <br>
           <br>73 cells"),
  tally %>%
    select(cell_id, x = ground, y = googled, group) %>%
    mutate(type = "<b>Ground<br>Truthed<br>Map</b><br>
           <br>
           <i>(In-Person<br>Visits)</i>
           <br>
           <br>10 cells")) %>%
 mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) 
 
mybands <- mydat %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            predict(
              se = TRUE,
              newdata = data.frame(
                x = seq(from = 0, to = 11, length.out = 20))) %>%
            as.data.frame() %>%
            mutate(x = seq(from = 0, to = 11, length.out = 20)), 
          .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total\n",
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places\nof Worship",
    "Social Businesses" = "Social\nBusinesses",
    "Parks" = "Parks")) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev()))


mycorr <- mydat %>%
  as.data.frame() %>%
  group_by(type, group) %>%
  summarize(cor = cor(x, y, use = "pairwise.complete.obs"),
            cor = paste("r = ", round(cor, 2), sep = "")) %>%
#  left_join(by = c("type", "group"), y = myposition) %>%
  mutate(x = 0, y = 7)

mystats <- mydat %>%
  as.data.frame() %>%
  mutate(mysplit = paste(type, group, sep = " - ")) %>%
  split(.$mysplit) %>%
  map_dfr(~lm(formula = y ~ x, data = .) %>%
            broom::glance(), .id = "mysplit")  %>%
  separate(col = mysplit, into = c("type", "group"), sep = " - ") %>%
  select(type, group, r2 = r.squared, sigma) %>%
  mutate(r2 = paste(round(r2*100, 0), "% accuracy (R<sup>2</sup>)", sep = ""),
         sigma = paste("Avg. Error = ", round(sigma, 1), sep = "")) %>%
#  left_join(by = c("type", "group"), y = myposition) %>%
  mutate(x = 0, y_r2 = 10, y_sigma = 8.5) %>%
  mutate(type = factor(type, levels = unique(type) %>% sort() %>% rev())) %>%
  mutate(group = factor(group, levels = levels(mydat$group)))


# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}


g2 <- mydat %>%
  # Plot main aesthetics
  ggplot(mapping = aes(x = x, y = y)) +
  # Add nice lines from the predicted value to the observed
  #geom_linerange(mapping = aes(x = x, ymin = y, ymax = y_hat),
  #               color = "grey", size = 0.5, alpha = 0.5) +
  # Give a point for every cells
  geom_jitter(alpha = 0.75, width = .5, height = .5,
              mapping = aes(fill = group),
              shape = 21, color = "white", 
              size = 3, alpha = 0.5) +
  # Colored based on this schema, by type of social infrastructure
  scale_fill_manual(values = c(
    "black","#0D0887", "#8405A7",
    "#D35171","#d98d2b"),
    guide = "none") +
  # Add a line of best fit and confidence interval,
  # whose fill matches the point color
  geom_ribbon(data = mybands,
              mapping = aes(x = x, y = fit,
                            ymin = fit - se.fit*1.96,
                            ymax = fit + se.fit*1.96,
                            fill = group),
              color = NA, alpha = 0.5) +
  geom_line(data = mybands, mapping = aes(x = x, y = fit,
                                            fill = group),
              color = "black", linetype = "dashed", size = 1) +
  # Add a label for R2
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Outline"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"), 
                fill = "white", color = "white",  alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_r2,
                              label = r2, size = "Value"),
                hjust = 0, label.color = NA, 
                label.padding = unit(0, "cm"),
                fill = NA, color = "black") +
  # Sigma
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Outline"),
                hjust = 0, label.color = NA,  
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mystats,
                mapping = aes(x = x, y = y_sigma,
                              label = sigma, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "Black") +
    # Add a label for the correlation
  # First put a blank background
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Outline"),
                hjust = 0, label.color = NA,
                fill = "white", color = "white", alpha = 0.5) +
  geom_richtext(data = mycorr, 
                mapping = aes(x = x, y = y,
                              label = cor, size = "Value"),
                hjust = 0, label.color = NA, 
                fill = NA, color = "black") +
  scale_size_manual(
    breaks = c("Outline", "Value"),
    values = c(3.5, 3.5),
    guide = "none") +
  facet_grid(rows = vars(reorder(type, as.numeric(type))),
             cols = vars(reorder(group, as.numeric(group)))) +
  theme_bw(base_size = 14) +
  labs(
    subtitle = "Cells Ranked by Decile (1 = Lowest, 10 = Highest)",
    x = "Ranking of Human-Coded Sites per Grid Cell",
       y = "Google API Ranking of Sites per Grid Cell") +
#    caption = "(Line indicates Bivariate OLS Line of Best Fit with 95% Confidence Intervals. Points jittered by 0.5 for clarity.)") +
  theme(
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    plot.caption = element_text(size = 10, hjust = 0.5),
    strip.text.y = ggtext::element_markdown(angle = 0, hjust = 0, size = 10),
    panel.grid = element_blank(),
    plot.margin = margin(0,0,0,0, "cm"),
    strip.text.x = element_text(color = "white"),
    panel.spacing.x = unit(0.5, "cm")) +
  scale_y_continuous(breaks = seq(from = 1, to = 10, length.out = 4),
                     labels = seq(from = 1, to = 10, length.out = 4) %>%
                       round(1)) +
  scale_x_continuous(breaks = seq(from = 1, to = 10, length.out = 4),
                     labels = seq(from = 1, to = 10, length.out = 4) %>%
                       round(1)) +
  coord_fixed(xlim = c(0,11),
              ylim = c(0,11)) 

combo <- g2 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_B4.png", 
       dpi = 500, width = 10, height = 5)
```



### Extra

Let's first measure the associations between overall tallies.

```{r}
tally <- read_rds("tally.rds") 

g1 <- tally %>%
  as.data.frame() %>%
  group_by(group) %>%
  summarize(first = cor(googled, checked, use = "pairwise.complete.obs"),
            second = cor(googled, ground, use = "pairwise.complete.obs"),
            third = cor(checked, ground, use = "pairwise.complete.obs")) %>%
  pivot_longer(cols = c(first, second, third),
               names_to = "type", values_to = "value") %>%
  mutate(type = type %>% recode_factor(
    "first" = "Google Sites <sup>1</sup><br>vs.<br>Checked Online <sup>2</sup>",
    "second" = "Google Sites <sup>1</sup><br>vs.<br>Ground Truthed <sup>3</sup>",
    "third" = "Checked Online <sup>2</sup><br>vs.<br>Ground Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.) %>% rev())) %>%
  ggplot(mapping = aes(x = type, y = group, fill = value, label = round(value, 2))) + 
  geom_tile(color = "white") +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 10, show.limits = TRUE)) +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_markdown(size = 11, hjust = 0),
        axis.text.x = element_markdown(size = 11, hjust = 0.5)) +
  labs(fill = "Correlation\n(Pearson's r)",
       x = "Measured of Social Infrastructure Compared",
       y = "Type of Social Infrastructure Examined",
       title = "How well does the Google Places API measure up?",
       subtitle = "Correlations between 3 Measures of Social Infrastructure\nRaw Count of Sites per Grid Cell",
       caption = "<sup>1</sup><b> Google sites</b> = sites captured by Google Maps Places API.<br><sup>2</sup>  <b> Checked Online</b> = sites found by human searches online.<br><sup>3</sup><b> Ground Truthed</b>= sites visually confirmed in person in 10 grid cells.<br><br><sup>Note: Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End")


ggsave(g1, filename = "viz/comparing_raw_count.png", dpi = 500, width = 8, height = 6)

datchecked %>% head
```


```{r}
# Next, let's measure the association between their rankings
# Do the rankings of grid cells usually line up?


library(ggtext)
tally <- read_rds("tally.rds") %>%
  group_by(group) %>%
  mutate(googled = ntile(googled, 100),
         checked = ntile(checked, 100),
         ground = ntile(ground, 100)) %>%
  ungroup()

g2 <- tally %>%
  as.data.frame() %>%
  group_by(group) %>%
  summarize(first = cor(googled, checked, use = "pairwise.complete.obs"),
            second = cor(googled, ground, use = "pairwise.complete.obs"),
            third = cor(checked, ground, use = "pairwise.complete.obs")) %>%
  pivot_longer(cols = c(first, second, third),
               names_to = "type", values_to = "value") %>%
  mutate(type = type %>% recode_factor(
    "first" = "Google Sites <sup>1</sup><br>vs.<br>Checked Online <sup>2</sup>",
    "second" = "Google Sites <sup>1</sup><br>vs.<br>Ground Truthed <sup>3</sup>",
    "third" = "Checked Online <sup>2</sup><br>vs.<br>Ground Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.) %>% rev())) %>%
  ggplot(mapping = aes(x = type, y = group, fill = value, label = round(value, 2))) + 
  geom_tile(color = "white") +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 10, show.limits = TRUE)) +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_markdown(size = 11, hjust = 0),
        axis.text.x = element_markdown(size = 11, hjust = 0.5)) +
  labs(fill = "Correlation\n(Pearson's r)",
       x = "Measured of Social Infrastructure Compared",
       y = "Type of Social Infrastructure Examined",
       title = "How well does the Google Places API measure up?",
       subtitle = "Correlations between 3 Measures of Social Infrastructure\nPercentile Ranking of Grid Cells (1-100)",
       caption = "<sup>1</sup><b> Google sites</b> = sites captured by Google Maps Places API.<br><sup>2</sup>  <b> Checked Online</b> = sites found by human searches online.<br><sup>3</sup><b> Ground Truthed</b>= sites visually confirmed in person in 10 grid cells.<br><br><sup>Note: Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End")


ggsave(g2, filename = "viz/comparing_rankings_1_100.png", dpi = 500, width = 8, height = 6)




library(ggtext)
tally <- read_rds("tally.rds") %>%
  group_by(group) %>%
  mutate(googled = ntile(googled, 10),
         checked = ntile(checked, 10),
         ground = ntile(ground, 10)) %>%
  ungroup()

g3 <- tally %>%
  as.data.frame() %>%
  group_by(group) %>%
  summarize(first = cor(googled, checked, use = "pairwise.complete.obs"),
            second = cor(googled, ground, use = "pairwise.complete.obs"),
            third = cor(checked, ground, use = "pairwise.complete.obs")) %>%
  pivot_longer(cols = c(first, second, third),
               names_to = "type", values_to = "value") %>%
  mutate(type = type %>% recode_factor(
    "first" = "Google Sites <sup>1</sup><br>vs.<br>Checked Online <sup>2</sup>",
    "second" = "Google Sites <sup>1</sup><br>vs.<br>Ground Truthed <sup>3</sup>",
    "third" = "Checked Online <sup>2</sup><br>vs.<br>Ground Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.) %>% rev())) %>%
  ggplot(mapping = aes(x = type, y = group, fill = value, label = round(value, 2))) + 
  geom_tile(color = "white") +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 10, show.limits = TRUE)) +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_markdown(size = 11, hjust = 0),
        axis.text.x = element_markdown(size = 11, hjust = 0.5)) +
  labs(fill = "Correlation\n(Pearson's r)",
       x = "Measured of Social Infrastructure Compared",
       y = "Type of Social Infrastructure Examined",
       title = "How well does the Google Places API measure up?",
       subtitle = "Correlations between 3 Measures of Social Infrastructure\nDecile Ranking of Grid Cells (1-10)",
       caption = "<sup>1</sup><b> Google sites</b> = sites captured by Google Maps Places API.<br><sup>2</sup>  <b> Checked Online</b> = sites found by human searches online.<br><sup>3</sup><b> Ground Truthed</b>= sites visually confirmed in person in 10 grid cells.<br><br><sup>Note: Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End")


ggsave(g3, filename = "viz/comparing_rankings_1_10.png", dpi = 500, width = 8, height = 6)





library(ggtext)
tally <- read_rds("tally.rds") %>%
  group_by(group) %>%
  mutate(googled = ntile(googled, 5),
         checked = ntile(checked, 5),
         ground = ntile(ground, 5)) %>%
  ungroup()

g4 <- tally %>%
  as.data.frame() %>%
  group_by(group) %>%
  summarize(first = cor(googled, checked, use = "pairwise.complete.obs"),
            second = cor(googled, ground, use = "pairwise.complete.obs"),
            third = cor(checked, ground, use = "pairwise.complete.obs")) %>%
  pivot_longer(cols = c(first, second, third),
               names_to = "type", values_to = "value") %>%
  mutate(type = type %>% recode_factor(
    "first" = "Google Sites <sup>1</sup><br>vs.<br>Checked Online <sup>2</sup>",
    "second" = "Google Sites <sup>1</sup><br>vs.<br>Ground Truthed <sup>3</sup>",
    "third" = "Checked Online <sup>2</sup><br>vs.<br>Ground Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.) %>% rev())) %>%
  ggplot(mapping = aes(x = type, y = group, fill = value, label = round(value, 2))) + 
  geom_tile(color = "white") +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 10, show.limits = TRUE)) +
  theme_classic(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_markdown(size = 11, hjust = 0),
        axis.text.x = element_markdown(size = 11, hjust = 0.5)) +
  labs(fill = "Correlation\n(Pearson's r)",
       x = "Measured of Social Infrastructure Compared",
       y = "Type of Social Infrastructure Examined",
       title = "How well does the Google Places API measure up?",
       subtitle = "Correlations between 3 Measures of Social Infrastructure\nQuintile Ranking of Grid Cells (1-5)",
       caption = "<sup>1</sup><b> Google sites</b> = sites captured by Google Maps Places API.<br><sup>2</sup>  <b> Checked Online</b> = sites found by human searches online.<br><sup>3</sup><b> Ground Truthed</b>= sites visually confirmed in person in 10 grid cells.<br><br><sup>Note: Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End")


ggsave(g4, filename = "viz/comparing_rankings_1_5.png", dpi = 500, width = 8, height = 6)
```




# 12. Covariates

Next, we're going to analyze how well these measures correlate with other known measures of social capital and key covariates.

## 12.1 Crime Rates

The Boston Police Department publishes all crime incident reports between 2015 and 2020, which we geolocate.

https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-new-system

```{r}
library(tidyverse)
library(sf)
library(rgdal)
# Equal Area projection
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"
# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
```

```{r, eval = FALSE}
# Import county boundaries
counties <- read_sf("county.kml") %>%
  st_transform(crs = aea)

# Import crime data
data.frame(file = dir("covariates/crime", full.names = TRUE)) %>%
  split(.$file) %>%
  map_dfr(~read_csv(.$file, col_types = cols(
    col_character(), col_character(), col_character(),
    col_character(),col_character(), col_character(),col_character(),
    col_datetime(), col_double(),col_double(),
    col_character(),col_double(),col_character(),
    col_character(),col_double(), col_double()))) %>%
  saveRDS("covariates/crime.rds")


read_rds("covariates/crime.rds") %>% 
  select(id = INCIDENT_NUMBER, date = OCCURRED_ON_DATE, 
         offense = OFFENSE_CODE,
         year = YEAR, lat = Lat, long = Long) %>%
  # eliminate extra unlocated reports
  filter(!is.na(lat), !is.na(long), lat != 0, long != 0) %>%
  filter(!is.na(year)) %>%
  st_as_sf(crs = wgs, coords = c("long", "lat")) %>%
  st_transform(crs = aea) %>%
  # Crop to just within suffolk county
  st_crop(y = st_bbox(counties)) %>%
  # Classify offenses as violent or non-violent
    mutate(offense = str_pad(offense, width = 5, side = "left", pad = "0")) %>%
  mutate(type = case_when(
    str_sub(offense, 1,3) %in% c("001", "002", "003", "004", "005", "008", "009") ~ "violent",
    offense %in% c("01704", "01730", "02511", "02611", "02647", 
                   "02648",  "02618", "02662", "02672", "03123") ~ "violent",
    TRUE ~ "nonviolent")) %>%
  # save!
  saveRDS("covariates/crime_points.rds")

```

I used this dataset from [Boston Open Data](https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-new-system/resource/3aeccf51-a231-4555-ba21-74572b4c33d6?inner_span=True) to sort crimes into these two basic types. Anything that involved direct bodily harm got classified as violent. This included murder or manslaughter of any kind, rape, robbery, assault, breaking and entering, burglary, stalking, arson, sexual assault, kidnapping/abduction, explosives, bomb threats, ballistics, biological threats, or threats to do bodily harm.

```{r, eval = FALSE}
cat <- readxl::read_excel("covariates/offense_codes.xlsx") %>%
  select(code = CODE, name = NAME) %>%
  mutate(code = str_pad(code, width = 5, side = "left", pad = "0")) %>%
  arrange(code) %>%
  mutate(type = case_when(
    str_sub(code, 1,3) %in% c("001", "002", "003", "004", "005", "008", "009") ~ "violent",
    code %in% c("01704", "01730", "02511", "02611", "02647", 
                "02648",  "02618", "02662", "02672", "03123") ~ "violent",
    TRUE ~ "nonviolent"))

cat %>%
  filter(type == "violent")

remove(cat)
```


```{r, eval = FALSE}
# Calculate the total number of crime incident reports which occurred within each fishnet gridcell.
library(tidyverse)
library(sf)
read_sf("grid_covariates_tracts.kml") %>% 
  select(-Name, -c(timestamp:icon)) %>%
  st_transform(crs = aea) %>%
  select(cell_id, pop_density = pop_density_int, geometry) %>%
  st_join(read_rds("covariates/crime_points.rds") %>%
            mutate(indicator = 1)) %>%
  group_by(cell_id, year) %>%
  summarize(crimes = sum(indicator == 1, na.rm = TRUE),
            violent = sum(indicator == 1 & type == "violent", na.rm = TRUE),
            nonviolent = sum(indicator == 1 & type == "nonviolent", na.rm = TRUE),
            pop_density = unique(pop_density, na.rm = TRUE),
            geometry = unique(geometry)) %>%
  ungroup()  %>%
  mutate(crime_rate = crimes / pop_density * 1000,
            violent_rate = violent / pop_density * 1000,
            nonviolent_rate = nonviolent / pop_density * 1000) %>%
    filter(!is.na(year)) %>%
  saveRDS("covariates/crime_grid.rds")
```

### Mapped

```{r}
crimes <- read_rds("covariates/crime_grid.rds") 

library(ggtext)

g1 <- ggplot() +
  geom_sf(data = crimes, 
          mapping = aes(fill = crime_rate),
          color = "white", size = 0.1) +
  facet_wrap(~year, ncol = 4) +
  scale_fill_viridis(option = "plasma") +
  theme_void(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        legend.position = c(0.85, 0.25),
        plot.background = element_rect(fill = "white", color = "white"),
        legend.title =  element_markdown(size = 12)) +
  labs(fill = "Crime Rate<br>per 1000<br>residents")

ggsave(g1, filename = "covariates/map_crime.png",
       dpi = 500, width = 6, height = 4.5)
```

### Correlation

```{r}
crimes <- read_rds("covariates/crime_grid.rds") %>%
  left_join(by = "cell_id",
            y = read_rds("tally.rds") %>%
              as.data.frame() %>%
              select(cell_id:ground)) %>%
  filter(!is.na(neighborhood)) %>%
  # Calculate sites per capita
  mutate_at(vars(googled:ground),
            # using interpolated population density
            funs(. / pop_density * 1000)) %>%
  as.data.frame() %>%
  select(-c(crimes:nonviolent, geometry)) %>%
  pivot_longer(cols = c(googled, checked, ground), 
               names_to = "site_type", values_to = "sites") %>%
  pivot_longer(cols = c(crime_rate, violent_rate, nonviolent_rate), 
               names_to = "crime_type", values_to = "crime") %>%
  filter(!is.na(sites)) %>%
  filter(group != "Other") %>%
  mutate(site_type = site_type %>% recode_factor(
    "googled" = "Google<br>Sites <sup>1</sup>",
    "checked" = "Checked<br>Online <sup>2</sup>",
    "ground" = "Ground<br>Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses") %>%
      factor(levels = c(
        "Total", "Community\nSpaces",
        "Places of\nWorship",
        "Parks",
        "Social\nBusinesses"))) %>%
  mutate(crime_type = crime_type %>% recode_factor(
    "crime_rate" = "Crime Overall",
    "nonviolent_rate" = "Nonviolent Crime",
    "violent_rate" = "Violent Crime")) 

library(ggtext)
g1 <- crimes %>% 
  group_by(year, group, site_type, crime_type) %>%
  summarize(mycor = cor(sites, crime, use = "pairwise.complete.obs") %>% round(3)) %>%
  ggplot(mapping = aes(x = site_type, y = year, 
                       label = round(mycor, 2), fill = mycor)) +
  geom_tile() +
  geom_text() +
  theme_bw(base_size = 14) +
  facet_grid(crime_type~group) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 1, barwidth = 20, show.limits = TRUE)) +
  theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0)) +
  labs(x = "Measurement of Social Infrastructure",
       subtitle = "Social Infrastructure by Type",
       y = "Correlation by Year",
       fill = "Correlation\n(Pearson's r)",
       title = "Correlations between Social Infrastructure and Crime over Time",
       caption = "<sup>1</sup>
       <b> Google Sites</b> = sites captured by Google Maps Places API.
       <sup>2</sup>
       <b> Checked Online</b> = sites found by human searches online.
       <br>
       <sup>3</sup>
       <b> Ground Truthed</b>= sites visually confirmed in person in 10 grid cells.
       <br>
       <b>Note:</b> Rates of Crime and Social Infrastructure measured per 1,000 residents. Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore, Jamaica Plain, Mission Hill, Roxbury, South Boston, South End.")


# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}

combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "covariates/cor_crime.png", dpi = 500, width = 12, height = 8)
```

### Controls

```{r}
crimes <- read_rds("covariates/crime_grid.rds") %>%
  left_join(by = "cell_id",
            y = read_rds("tally.rds") %>%
              as.data.frame() %>%
              select(cell_id:ground)) %>%
  filter(!is.na(neighborhood)) %>%
  # Calculate sites per capita
  mutate_at(vars(googled:ground),
            # using interpolated population density
            funs(. / pop_density * 1000)) %>%
  as.data.frame() %>%
  select(-c(crimes:nonviolent, geometry)) %>%
  pivot_longer(cols = c(googled, checked, ground), 
               names_to = "site_type", values_to = "sites") %>%
  pivot_longer(cols = c(crime_rate, violent_rate, nonviolent_rate), 
               names_to = "crime_type", values_to = "crime") %>%
  filter(!is.na(sites)) %>%
  filter(group != "Other") %>%
  mutate(site_type = site_type %>% recode_factor(
    "googled" = "Google<br>Sites <sup>1</sup>",
    "checked" = "Checked<br>Online <sup>2</sup>",
    "ground" = "Ground<br>Truthed <sup>3</sup>")) %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses") %>%
      factor(levels = c(
        "Total", "Community\nSpaces",
        "Places of\nWorship",
        "Parks",
        "Social\nBusinesses"))) %>%
  mutate(crime_type = crime_type %>% recode_factor(
    "crime_rate" = "Crime Overall",
    "nonviolent_rate" = "Nonviolent Crime",
    "violent_rate" = "Violent Crime")) %>%
  # Join in some additional census traits
  left_join(by = "cell_id",
            y = read_sf("grid_covariates_tracts.kml") %>%
              as.data.frame() %>%
              select(cell_id, pop_women:pop_age_65_plus)) %>%
  # Since crime rates and site rates are both right skewed rates,
  # we're going to add a tiny logical constant
  # so that they can be log transformed for analysis
  # as normalized variables
  group_by(group, site_type, crime_type) %>%
  mutate_at(vars(crime, sites),
            funs(if_else(
              condition = . == 0,
              true = sort(unique(.))[2] / 2,
              false = .)))


myvif <- crimes %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(type = paste(group, site_type, crime_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = log(crime) ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite +
                pop_density + 
                year, data = .) %>%
            car::vif() %>%
            .^2 %>%
            .[,3] %>%
            max() %>%
            data.frame(vif = .),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "site_type", "crime_type"))

# Everything is fine except for Ground-Truthed, which has barely any observations,
# so of course the VIF will be through the roof

mysum <- crimes %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(crime = log(crime)) %>%
  mutate(type = paste(group, site_type, crime_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = crime ~ sites + 
                income_inequality +
                median_income +
                pop_nonwhite +
                pop_density + 
                year, data = .) %>%
            moderndive::get_regression_summaries(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "site_type", "crime_type"))


mystats <- crimes %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(crime = log(crime)) %>%
  group_by(group) %>%
  mutate_at(vars(crime, sites, income_inequality, median_income, pop_nonwhite, pop_density),
            funs(scale(.) %>% as.numeric())) %>%
  mutate(type = paste(group, site_type, crime_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = crime ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite +
                pop_density + 
                year, data = .) %>%
            moderndive::get_regression_table(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "site_type", "crime_type"))  %>%
  mutate(group = factor(group, levels = levels(crimes$group)),
         site_type = factor(site_type, 
                            levels = levels(crimes$site_type)),
         crime_type = factor(crime_type, 
                             levels = levels(crimes$crime_type))) %>%
  mutate(label = paste(
    round(estimate, 4) %>% format(scientific = FALSE),
    gtools::stars.pval(p_value)), sep = "")

crimes %>%
  filter(crime == 0)

crimes %>%
  filter(sites == 0)
```


```{r}
mytheme <- theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),   axis.text.y = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0))
```

```{r}
g1 <- mystats %>% 
  filter(term == "sites") %>%
  mutate(site_type = factor(site_type, levels = levels(site_type) %>% rev())) %>%
  mutate(group = factor(group, levels = c(
    "Total", 
    "Community\nSpaces",
    "Places of\nWorship",
    "Social\nBusinesses",
    "Parks") %>% rev())) %>%
  ggplot(mapping = aes(x = site_type, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci,
                       fill = estimate, label = label)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_col(width = 0.5, color = "grey") +
  geom_linerange() +
  geom_point() +
  geom_text(nudge_x = 0.4) + 
  coord_flip() +
  facet_grid(rows = vars(crime_type), cols = vars(group),
             scales = "free") +
    scale_fill_gradient2(low = "#DC267F", mid = "white", 
                       high = "#648FFF", midpoint = 0,
                       limits = c(-2, 2),
                       breaks = seq(from = -2, to = 2, length.out = 9)) +
  guides(fill = guide_colorsteps(barheight = 1, barwidth = 20,
                                 show.limits = TRUE))  +
  theme_bw(base_size = 14) +
  mytheme +
  theme(plot.subtitle = ggtext::element_markdown(size = 15)) +
  labs(
    subtitle = "Association between Social Infrastructure and Crime Rates",
    y = "Standardized Effect of Site Rates (Z-score) on (logged) Crime Rates (Z-score) \n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Size of Standardized Effect (Beta)",
    x = "Type of Social Infrastructure Measure",
    caption = "<b>Models: </b>Generated from OLS Models, which predict logged Crime Rate using
    the Rate of Social Infrastructure (shown above), Median Income,
    <br>
    Income Inequality (Gini Coefficient), % Non-White Population, and Population Density,
    plus annual fixed effects. All models explained between 60-94% of variation in outcome.
    <br>
    <b>Colinearity</b>: VIF below 10 for Google Sites (n = 406, Max = 4.2) and Points Checked Online (n = 406, Max = 5.4).
    <br>Ground-Truth models (n = 70) were colinear (Max VIF = 31.4), but results matched other models.
    <br>
    <b>Standardized Effects</b>: All numeric variables were rescaled into Z-scores so that beta coefficients reflect the projected increase in logged Crime Rates in standard deviations<br>as the rate of Social Infrastructure increases by 1 standard deviation. <b>Statistical Significance</b>: *** p < 0.001, ** p < 0.01, * p < 0.05, . p < 0.10.")

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}

combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "covariates/beta_crime.png", dpi = 500, width = 12, height = 8)
```


## Facebook

```{r, message=FALSE, warning = FALSE, eval = FALSE}
# Get county polygon
counties <- read_sf("county.kml") %>%
  mutate(indicator = 1) %>%
  st_transform(crs = aea) %>%
  select(indicator, geometry)

# Get zipcodes that overlap with county polygons
tigris::zctas(cb = TRUE, starts_with = "02", state = "MA", year = 2019) %>% 
  st_as_sf() %>%
  select(geoid = GEOID10, area_land = ALAND10, geometry) %>%
  st_transform(crs = aea) %>%
  st_join(counties) %>%
  filter(indicator == 1) %>%
  select(-indicator) %>%
  write_sf("zipcodes.kml", driver = "kml", delete_layer = TRUE)

# Visualize
ggplot() +
  geom_sf(data = read_sf("zipcodes.kml")) +
  geom_sf(data = read_sf("county.kml"), fill = NA, color = "black", size = 2)
```

```{r, eval = FALSE}
#install.packages("RSQLite")
library(RSQLite)
library(DBI)
library(tidyverse)
# Create a 
#my_db <- src_sqlite("~/zcta_zcta_shard0.tsv", create = FALSE)
#my_db <- src_sqlite(my_db_file, create = TRUE)
#my_db %>% copy_to(read_delim("~/zcta_zcta_shard0.tsv", delim = " "))

# read csv file into sql database

path <- "/home/rstudio-user/fb.sqlite"

#con <- dbConnect(RSQLite::SQLite(), dbname = path)

#dbWriteTable(con, name = path, value = "~/zcta_zcta_shard0.tsv",
#             row.names = FALSE, header = TRUE, sep = "\t")

#remove(con)
# Get digits 2 through 5 of each zipcode 
# (since our database's zipcodes have had their first digit, 0, removed)
myzip <- read_sf("zipcodes.kml")$geoid %>% unique() %>% str_sub(2,5)

# Access our sql database.
myfb <- dbConnect(RSQLite::SQLite(), dbname = path)

# Drawing from our sql database
myfb %>% tbl(path) %>%
  # Filter to sites in Boston
  filter(user_loc %in% myzip) %>%
  # Filer to destinations in Boston
  filter(fr_loc %in% myzip) %>%
  # Collect the data
  collect() %>%
  # Rename the fields to more intuitive titles
  rename(from = user_loc,
         to = fr_loc) %>%
  # Add back in the zeros, for easy joining
  mutate(from = str_pad(from, width = 5, side = "left", pad = "0"),
         to = str_pad(to, width = 5, side = "left", pad = "0")) %>%
  # Save to file
  write_csv("covariates/boston_fb_sci.csv")
remove(myfb, path, myzip)
```


```{r}
library(tidygraph)
library(sfnetworks)
library(ggraph)

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Import zipcode polygons
zip <- read_sf("zipcodes.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = wgs)

# Grab a vector of zipcodes located within or overlapping our milestones
grid <- read_sf("grid_covariates_tracts.kml") %>%
  filter(str_detect(milestone, "M1|M2|M3")) %>%
  select(-c(Name:icon), cell_id, geometry) %>%
  st_transform(crs = aea) %>%
  st_join(zip %>% st_transform(crs = aea)) %>%
  filter(!is.na(geoid)) %>%
  as.data.frame() %>%
  select(geoid) %>% unlist() %>% unique()

# Import zipcode polygons
zip <- read_sf("zipcodes.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = wgs) %>%
  # Filter to milestones
  filter(geoid %in% grid)


# Get centroids
points <- zip %>% 
  st_transform(crs = aea) %>%
  group_by(geoid) %>%
  summarize(geometry = st_centroid(geometry)) %>%
  ungroup() %>%
  st_transform(crs = wgs)

# Extract coordinate points
geo <- data.frame(
  geoid = points$geoid,
  points %>% st_coordinates())

# Import Facebook zipcode pairs within Boston
fb <- read_csv("covariates/boston_fb_sci.csv") %>%
# Zoom into just zipcodes within our milestone 1-3 zone
  filter(from %in% grid, to %in% grid)



# Make a graph object
g <- tidygraph::tbl_graph(nodes = points, edges = fb, 
                          directed = FALSE, node_key = "geoid") %>%
  activate("edges") %>%
  # Drop extra edges - 
  # each edge (except the diagnoal)
  # reoccurs in the upper and lower matrix,
  # but we don't want to double count, so we'll drop half of them
  as.igraph() %>%
  igraph::simplify(remove.multiple = TRUE, 
                   remove.loops = FALSE, 
                   edge.attr.comb = list(scaled_sci = function(x) unique(x),
                                         geometry = "ignore",
                                         name = "ignore")) %>%
  # Convert back to tidygraph
  as_tbl_graph() %>%
  # Convert to network
  as_sfnetwork() %>%
  convert(to_spatial_explicit, .clean = TRUE)  %>%
  activate("nodes") %>%
  # Calculate total weighted degree
  mutate(wdeg = centrality_degree(weights = .E()$scaled_sci, loops = TRUE)) %>%
  # Let's also calculate the total weighted degree just within each community
  activate("edges") %>%
  mutate(same_sci = if_else(from == to, true = scaled_sci, false = 0)) %>%
  mutate(diff_sci = if_else(from != to, true = scaled_sci, false = 0)) %>%
  activate("nodes") %>%
  mutate(wdeg_same = centrality_degree(weights = .E()$same_sci, loops = TRUE)) %>%
  mutate(wdeg_diff = centrality_degree(weights = .E()$diff_sci, loops = TRUE)) %>%
  # Join in the geometry for polygons to our points, to access later
  left_join(by = "geoid", y = zip %>% as.data.frame() %>% select(geoid, shape = geometry))


g %>%
  saveRDS("connectivity.rds")

# Now take our graph object
# and convert it into a set of polygons containing weighted degree
poly <- g %>%
  st_as_sf("nodes") %>%
  as.data.frame() %>%
  select(geoid, wdeg, wdeg_same, wdeg_diff, geometry = shape) %>%
  st_as_sf(crs = wgs)

# Convert this into a grid-cell-by-grid-cell estimate
grid <- read_sf("grid_covariates_tracts.kml") %>%
  filter(str_detect(milestone, "M1|M2|M3")) %>%
  select(-c(Name:icon), cell_id, geometry) %>%
  st_transform(crs = aea) %>%
  st_join(poly %>% st_transform(crs = aea)) %>%
  # Take the mean connectivity of each zipcode
  group_by(cell_id) %>%
  summarize(
    wdeg = mean(wdeg, na.rm = TRUE),
    wdeg_same = mean(wdeg_same, na.rm = TRUE),
    wdeg_diff = mean(wdeg_diff, na.rm = TRUE),
    geometry = unique(geometry)) %>%
  # Then log-transform it, because it's a super skewed value
  mutate(wdeg = log(wdeg),
         wdeg_same = log(wdeg_same),
         wdeg_diff = log(wdeg_diff)) %>%
  # Then rescale it into a z-score, to show which neighborhoods are more connected than others
  mutate(ties = scale(wdeg),
         ties_same = scale(wdeg_same),
         ties_diff = scale(wdeg_diff))

ggplot() +
  geom_sf(data = zip, fill = "black", color = "white") +
  geom_sf(data = st_as_sf(g, "edges"), color = "grey", alpha = 0.1)  +
  geom_sf(data = poly,
          mapping = aes(fill = scale(wdeg)),
          color = "white") + 
  geom_sf(data = grid, fill = NA, color = "black") +
  scale_fill_viridis(option = "plasma", begin = 0, end = 0.9)

mygoogle <- read_sf("mygoogle.kml") %>%
  select(-c(Name:icon))



myties <- bind_rows(
  grid %>%
    select(cell_id, ties = ties) %>%
    mutate(type = "<b>Friends<br>in any Zipcode</b>"),
  grid %>%
    select(cell_id, ties = ties_same) %>%
    mutate(type = "<b>Friends<br>in Same Zipcode</b>"),
  grid %>%
    select(cell_id, ties = ties_diff) %>%
    mutate(type = "<b>Friends<br>in Different Zipcodes</b>"))

library(ggtext)

g1 <- ggplot() +
  geom_sf(data = myties, mapping = aes(fill = round(ties,2) ), 
          size = 0.5, color = "black") +
  scale_fill_viridis(option = "plasma", begin = 0, end = 1) +
  geom_sf(data = mygoogle, color = "white", fill = "black",
          shape = 21, size = 1, alpha = 0.5) +
  facet_wrap(~type) +
  theme_void(base_size = 14) +
  theme(legend.position = "bottom",
        plot.caption = element_markdown(size = 11, hjust = 0),
        plot.caption.position = "plot",
        strip.text.x = element_markdown(size = 10, hjust = 0.5),
        plot.title = element_markdown(size = 16, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        legend.title = element_markdown(size = 14, hjust = 0.5),
        plot.background = element_rect(fill = "white", color = "white")) +
  labs(fill = "Social Connectivity\nIndex <sup>2</sup> (Z-Score)",
       subtitle = "Social Connectivity in Boston City Blocks <sup>1</sup><br>",
       title = "Estimating Local Connectivity <sup>3</sup> through Facebook Friends",
       caption = "<sup>1</sup>
       <b> Grid</b>: Grid cells depict 73 grid cells (1 km<sup>2</sup>) in Core Boston neighborhoods.<br>Points depicts social infrastructure, geolocated from Google Maps Places API.
       <br><br>
       <sup>2</sup>
       <b> Social Connectivity Index (SCI)</b> ranks pairs of US zipcodes from 1 to 1 billion<br>based on how many Facebook friends they share.
       <br><br>
       <sup>3</sup>
       <b> Local Connectivity</b>: We zoomed into zipcode pairs within grid cells.<br>To calculate each zipcode's connectivity, we summed index scores among<br>connected zipcodes. For each grid cell, we averaged connectivity in overlapping<br>zipcodes, logged values to normalize them, and rescaled them as Z-scores.
       <br><br>
       <b>Interpretation</b>: Each cell's shading reflects how many standard deviations from<br>the study area mean is that cell's social connectivity.") +
  guides(fill = guide_colorsteps(ticks = TRUE, barwidth = 10, barheight = 1))

g1
ggsave(g1, filename = "covariates/map_fb_sci.png", dpi = 500, width = 6, height = 6.5)

```




## 12.2 Social Capital

### Correlation

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Get a simple estimate of population density within a fishnet grid cells using the mean of the rate of tracts
# Import tracts
tracts <- read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  left_join(by = "geoid", 
            y = read_csv("covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid)) %>%
  filter(str_sub(geoid, 1,5) == "25025") %>% 
  select(year, geoid, social_capital, bonding, bridging, linking)) %>%
  st_transform(crs = aea)

tracts_grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = aea) %>%
  st_join(tracts) %>%
  as.data.frame() %>%
  group_by(cell_id, year) %>%
  summarize_at(vars(social_capital:linking),
               list(~mean(., na.rm = TRUE)))


grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  select(-pop_density) %>% rename(pop_density = pop_density_int) %>%
  left_join(by = "cell_id",
            y = read_rds("tally.rds") %>%
              as.data.frame() %>%
              select(cell_id, group, googled:ground)) %>%
  # Zoom into just milestones 1-3
  filter(str_detect(milestone, "M1|M2|M3")) %>%
  # Zoom into just populated cells
  filter(pop_density > 0) %>%
  st_transform(crs = aea) %>%
  # Join in social capital grid cell estimates
  left_join(by = c("cell_id"), y = tracts_grid)


mydat <- grid %>%
  as.data.frame() %>%
  select(-geometry) %>%
   # Calculate sites per capita
  mutate_at(vars(googled:ground),
            funs(. / pop_density * 1000)) %>%
  #select(cell_id, group, year, social_capital:linking, 
  #       googled:ground, pop_density) %>%
  pivot_longer(cols = c(social_capital, bonding, bridging, linking), 
               names_to = "sc_type", values_to = "sc") %>%
  filter(!is.na(googled)) %>%
  filter(group != "Other") %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.))) %>%
  mutate(sc_type = sc_type %>% recode_factor(
    "social_capital" = "Social\nCapital",
    "bonding" = "Bonding\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "linking" = "Linking\nSocial Capital") %>%
      factor(levels = levels(.))) 

mydat %>% saveRDS("shield/sc_data.rds")

mystat <- mydat %>%
  group_by(year, group, sc_type) %>%
  # Replace instances of 0 sites with half the second smallest non-negative value
  # To deal with the probelm that you can't log-transform zero observations
  mutate(googled = if_else(googled == 0, 
                         true = sort(unique(googled))[2]/2,
                         false = googled)) %>% 
  summarize(mycor = cor(googled, sc, use = "pairwise.complete.obs") %>% round(3)) 

library(ggtext)
g1 <- mystat %>% 
  ggplot(mapping = aes(x = reorder(group, as.numeric(group)), y = year, 
                       label = round(mycor, 2), fill = mycor)) +
  geom_tile(color = "black", size = 0.1) +
  geom_text(size = 4) +
  facet_grid(rows = vars(reorder(sc_type, as.numeric(sc_type))) ) +
  scale_fill_gradient2(low = "#DC267F", mid = "white", 
                       high = "#648FFF", midpoint = 0, 
                       limits = c(-1, 1), 
                       breaks = c(-1.00,  -0.5, 
                                  0, 0.5, 1.00)) +
  guides(fill = guide_colorsteps(barheight = 1, barwidth = 10, 
                                 show.limits = TRUE)) +
  scale_x_discrete(position = "top") +
  theme_bw(base_size = 14) +
  theme(legend.position = "bottom",
        plot.subtitle = element_markdown(size = 12, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0)) +
  labs(x = "Social Infrastructure by Type",
       y = "Year",
       subtitle = "Social Capital by Type",
       title = "Social Infrastructure vs. Social Capital",
       fill = "Correlation (Pearson's r)",
       caption = "<sup>1</sup><b> Google Sites</b> = sites captured by Google Maps Places API.<br>
       <b>Note:</b> Searches focused on 73 grid cells (1 km<sup>2</sup>) in 10 core Boston neighborhoods:<br>Back Bay, Beacon Hill, Dorchester, Downtown, Fenway/Kenmore,<br> Jamaica Plain, Mission Hill, Roxbury, South Boston, South End")

library(ggtext)
ggsave(g1, filename = "covariates/cor_sci.png",
       dpi = 500, width = 6, height = 9)
```

### Model

```{r, eval = FALSE}
# Which are the two cells that get dropped?
mydat %>% 
  filter(sc_type == "Social\nCapital") %>% 
  filter(group == "Total") %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  select(cell_id, year, sc, googled, income_inequality, median_income, 
         pop_nonwhite, pop_some_college, pop_density) %>%
  filter(is.na(sc) | is.na(googled) | is.na(income_inequality) |
           is.na(median_income) | is.na(pop_nonwhite) | is.na(pop_some_college) |
           is.na(pop_density) | is.na(year))

```

```{r}
myvif <- mydat %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(type = paste(group, sc_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = sc ~ googled + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density + 
                year, data = .) %>%
            car::vif() %>%
            .^2 %>%
            .[,3] %>%
            max() %>%
            data.frame(vif = .),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "sc_type"))


mysum <- mydat %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(type = paste(group, sc_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = sc ~ googled + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density + 
                year, data = .) %>%
            moderndive::get_regression_summaries(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "sc_type"))


mystats <- mydat %>%
  mutate(year = factor(year)) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  # Rescale numeric predictors for comparability
  mutate_at(vars(sc, googled, income_inequality, median_income,
                 pop_nonwhite, pop_some_college, pop_density),
            funs(scale(.) %>% as.numeric())) %>%
  mutate(type = paste(group, sc_type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = sc ~ googled + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density + 
                year, data = .) %>%
            moderndive::get_regression_table(digits = 5),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "sc_type")) %>%
  filter(term == "googled") %>%
  # Add a label
  mutate(label = paste(round(estimate, 4) %>% format(scientific = FALSE), 
                       gtools::stars.pval(p_value), sep = ""))

```
```{r}
mytheme <- theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0))

mystats %>%
  filter(sc_type == "Bridging\nSocial Capital")


```

```{r}
g1 <- mystats %>% 
  mutate(sc_type = factor(sc_type, levels = c(
    "Social\nCapital",
    "Bonding\nSocial Capital",
    "Bridging\nSocial Capital",
    "Linking\nSocial Capital"))) %>%
  mutate(group = factor(group, levels = c(
    "Total", "Community\nSpaces", "Places of\nWorship",
    "Social\nBusinesses", "Parks") %>% rev())) %>%
  ggplot(mapping = aes(x = group, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci,
                       fill = if_else(estimate > 0, "Positive", "Negative"),
                       label = label)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_col(width = 0.5) +
  geom_linerange() +
  geom_point() +
  geom_text(nudge_x = 0.40) +
  facet_grid(cols = vars(reorder(sc_type, as.numeric(sc_type) )),
             scales = "free") +
  coord_flip() +
  scale_fill_manual(values = c("#DC267F", "#648FFF")) +
  theme_bw(base_size = 14) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  mytheme +
  theme(panel.spacing.x = unit(0.5, "cm"),
        panel.border = element_rect(color = "grey", fill = NA),
        plot.subtitle = ggtext::element_markdown(size = 15)) +
  labs(
    subtitle = "Association between Social Infrastructure and Social Capital",
    y = "Standardized Effect of Site Rates (Z-score) on Social Capital (Z-score) \n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Direction of Effect (Beta)",
    x = "Type of Social Infrastructure Measure")

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}

combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_C1.png", dpi = 500, width = 12, height = 5.5)

combo2 <- g1 +
  labs(caption = "<b>Models: </b>Generated from OLS Models, which predict Social Capital Index (0 to 1) using
    the Rate of Social Infrastructure (shown above), Median Income, % Some College,
    <br>
    Income Inequality (Gini Coefficient), % Non-White Population, and Population Density,
    plus annual fixed effects. All models explained between 42-85% of variation in outcome.
    <br>
    <b>Standardized Effects</b>: All numeric variables were rescaled into Z-scores so that beta coefficients reflect the projected increase in Social Capital in standard deviations<br>as the rate of Social Infrastructure increases by 1 standard deviation.
    <b>Colinearity</b>: VIF below 10 for Google Sites (n = 639, Max = 4.6).
    <br>
    <b>Statistical Significance</b>: *** p < 0.001; ** p < 0.01; * p < 0.05; . p < 0.10.") %>%
    scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "covariates/beta_sci.png", dpi = 500, width = 12, height = 6)
```

### Other


```{r, eval = FALSE}
mypoints <- read_rds("shapes/boston_points.rds") %>%
  # Exclude any points that aren't directly in a cell
  st_join(fish) %>%
  filter(!is.na(id)) %>%
  select(-id, -pop_density)




sci <- read_rds("shapes/tracts_boston.rds") %>%
  left_join(by = c( "geoid"), 
            y = read_csv("shapes/social_capital/sci_census_tracts_2022_004_10.csv") %>%
              mutate(geoid = as.character(geoid)) %>%
              filter(str_sub(geoid, 1,5) == "25025") %>% 
              select(year, geoid, social_capital, bonding, bridging, linking)) %>%
  st_join(mypoints) %>%
  group_by(geoid, year) %>%
  summarize(
#    sites = sum(!is.na(group)),
    # Since we're not going to count museums, ditch this category.
    sites = sum(!is.na(group) & file != "museum"),
    # Commmunity Space
    community = sum(group == "Community Space", na.rm = TRUE),
    library = sum(file == "library", na.rm = TRUE),
    community_center = sum(file == "community center", na.rm = TRUE),
    city_hall = sum(file == "city hall", na.rm = TRUE),
    
    place_of_worship = sum(file == "place of worship", na.rm = TRUE),
    # Parks
    parks = sum(group == "Parks", na.rm = TRUE),
    park = sum(file == "park", na.rm = TRUE),
    fountain = sum(file == "fountain", na.rm = TRUE),
    square = sum(file == "square", na.rm = TRUE),
    # Social Business
    social = sum(group == "Social Businesses", na.rm = TRUE),
    
    bookstore = sum(file == "bookstore", na.rm = TRUE),
    cafe = sum(file == "cafe", na.rm = TRUE),
    coffeeshop = sum(file == "coffeeshop", na.rm = TRUE),
    beauty_salon = sum(file == "beauty salon", na.rm = TRUE),
    barbershop = sum(file == "barbershop", na.rm = TRUE),
    # Recreation
    sports_field = sum(file == "sports field", na.rm = TRUE),
    recreation_center = sum(file == "recreation center", na.rm = TRUE),
    aquarium = sum(file == "aquarium", na.rm = TRUE),
    art_gallery = sum(file == "art gallery", na.rm = TRUE),
    zoo = sum(file == "zoo", na.rm = TRUE),
    museum = sum(file == "museum", na.rm = TRUE),
    garden = sum(file == "garden", na.rm = TRUE),
    
    rec = sum(group == "Recreation & Education", na.rm = TRUE),
    
    pop_density = unique(pop_density),
    pop = unique(pop),
    bonding = unique(bonding),
    bridging = unique(bridging),
    linking= unique(linking),
    social_capital = unique(social_capital),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  mutate_at(vars(sites:rec), funs(. / pop * 1000))


sci %>%
  as.data.frame() %>%
  pivot_longer(cols = c(social_capital, bonding, bridging, linking),
               names_to = "type",values_to = "value") %>%
  mutate(type = type %>% dplyr::recode_factor(
    "social_capital" = "Overall\nSocial Capital",
    "bonding" = "Bonding\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "linking" = "Linking\nSocial Capital")) %>%
  ggplot(mapping = aes(x = value, y = sites, color = type)) +
  facet_wrap(~type, scales = "free_x") +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = "lm", color = "black") +
  scale_y_log10() +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.border = element_rect(fill = NA, color = "black"),
        plot.caption = element_text(hjust = 0.5)) +
  labs(x = "Social Capital 2011-2018 (0 = min, 1 = max)",
       y = "Social Infrastructure Rate *",
       caption = "* Rates calculated per 1000 residents, tabulated per census tract.") +
  guides(color = FALSE) +
  viridis::scale_color_viridis(option = "plasma", discrete = TRUE, begin = 0.2, end = 0.8) +
  ggsave("fig_social_cap_cor.png", dpi = 500, width = 6, height = 6)
```

## 12.2B Simply Analytics Measures

```{r}
library(sf)
# Simply Analytics made probabilistic estimates of membership block-group-to-block-group based on demographics
bind_rows(
 
  read_sf("shield/bonding.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id, 
           fraternal = value0, religious = value1, 
           veterans = value2, country_clubs = value3) %>%
    as_tibble(),
  
  read_sf("shield/bridging.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           civic = value0, business = value1, collectors = value2, 
           union = value3, charitable = value4) %>%
    as_tibble(),
  
  read_sf("shield/linking.shp") %>%
    magrittr::set_colnames(value = names(.) %>% tolower()) %>%
    select(geoid = spatial_id,
           church_board = value2, school_board = value1, local_board = value0) %>%
    as_tibble()) %>%
  select(-geometry) %>%
  pivot_longer(cols = -c(geoid), names_to = "variable", 
               values_to = "x", values_drop_na = TRUE) %>%
  pivot_wider(id_cols = c(geoid), names_from = variable, values_from = x) %>%
  # Join back in geographies from one of them
  left_join(by = "geoid",
            y = read_sf("shield/linking.shp") %>%
              magrittr::set_colnames(value = names(.) %>% tolower()) %>%
              select(geoid = spatial_id, geometry)) %>%
  saveRDS("shield/simply_analytics_data.rds")

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


# Average to grid
grid <- read_sf("grid_covariates_tracts.kml") %>%
  select(cell_id, geometry) %>%
  st_transform(crs = aea) %>%
  st_join(read_rds("shield/simply_analytics_data.rds") %>%
            st_as_sf() %>%
            st_transform(crs = aea)) %>%
  as.data.frame() %>%
  group_by(cell_id) %>%
  summarize_at(vars(fraternal:local_board),
               list(~mean(., na.rm = TRUE)))

# Load in the original SC data at cell level
dat <- read_rds("shield/sc_data.rds") %>%
  filter(year == 2020) %>%
  pivot_wider(id_cols = -c(sc_type, sc), 
              names_from = sc_type, values_from = sc) %>%
  # Join in the grid averages
  left_join(by = "cell_id", y = grid)

# Load
dat %>% 
  select(cell_id, group, googled, sc = `Social\nCapital`, 
         bonding = `Bonding\nSocial Capital`, 
         bridging = `Bridging\nSocial Capital`, 
         linking = `Linking\nSocial Capital`, 
         fraternal:local_board) %>%
  pivot_longer(cols = c(sc:local_board), names_to = "var", values_to = "x") %>%
  # Rescale numeric predictors for comparability
  group_by(group, var) %>%
  mutate_at(vars(sc, googled, income_inequality, median_income,
                 pop_nonwhite, pop_some_college, pop_density),
            funs(scale(.) %>% as.numeric())) %>%
  mutate(googled = case_when(
    googled == 0 ~ googled %>% unique() %>% sort(FALSE) %>% .[2] / 2,
    TRUE ~ googled),
    x = case_when(
    x == 0 ~ x %>% unique() %>% sort(FALSE) %>% .[2] / 2,
    TRUE ~ x)) %>%
  ungroup() %>%
  # Both variables are rates, so when we log them both, 
  # they produce nice stats!
  group_by(group) %>%
  summarize(r = cor(log(googled), log(x) ))


mymodels <- dat %>% 
  mutate(pop_nonwhite = 1 - pop_white) %>%
  select(cell_id, group, googled, 
      # sc = `Social\nCapital`, 
      #   bonding = `Bonding\nSocial Capital`, 
      #   bridging = `Bridging\nSocial Capital`, 
      #   linking = `Linking\nSocial Capital`, 
         fraternal:local_board,
       income_inequality,
       median_income,
       pop_nonwhite,
       pop_some_college,
       pop_density) %>%
  pivot_longer(cols = c(fraternal:local_board), names_to = "sc_type", values_to = "sc") %>%
    # Rescale numeric predictors for comparability
  group_by(group, sc_type) %>%
  mutate_at(vars(sc, googled, income_inequality, median_income,
                 pop_nonwhite, pop_some_college, pop_density),
            funs(scale(.) %>% as.numeric())) %>%
  ungroup() %>%
  #mutate(googled = case_when(
  #  googled == 0 ~ googled %>% unique() %>% sort(FALSE) %>% .[2] / 2,
  #  TRUE ~ googled)) %>%
  #  x = case_when(
  #  x == 0 ~ x %>% unique() %>% sort(FALSE) %>% .[2] / 2,
  #  TRUE ~ x)) %>%
  mutate(type = paste(group, sc_type, sep = " split ")) %>%
  split(.$type) %>%
  map(~lm(formula = sc ~ googled + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .),
          .id = "type")

mycoef <- mymodels %>%
  map_dfr(~moderndive::get_regression_table(.), .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "sc_type"))


mygof <- mymodels %>%
  map_dfr(~moderndive::get_regression_summaries(.), .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "sc_type"))

#mycoef %>% 
#  filter(term == "googled") %>%
#  ggplot(mapping = aes(x = group, y = estimate)) +
#  geom_col() +
#  facet_wrap(~sc_type, scales = "free_x") +
#  coord_flip()

mytheme <- theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0))

mystat <- mycoef %>%
  mutate(overall_type = sc_type %>% recode_factor(
    "country_clubs" = "<i>(Quasi)</i><br>Bridging<br>Groups",
    "veterans" = "<i>(Quasi)</i><br>Bridging<br>Groups",
    "fraternal" = "<i>(Quasi)</i><br>Bridging<br>Groups",
    
    "religious" = "Bridging<br>Groups",
    "business" = "Bridging<br>Groups",
    "charitable" = "Bridging<br>Groups",
    "civic" = "Bridging<br>Groups",
    "collectors" = "Bridging<br>Groups",
    "union" = "Bridging<br>Groups",
    
    "church_board" = "Linking<br>Groups",
    "local_board" = "Linking<br>Groups",
    "school_board" = "Linking<br>Groups"),
    
    sc_type = sc_type %>% recode_factor(
    "country_clubs" = "Country Clubs",
    "veterans" = "Veterans Associations",
    "fraternal" = "Fraternal Orders",
    "religious" = "Religious Orgs",
    
    "business" = "Business Clubs",
    "charitable" = "Charitable Orgs",
    "civic" = "Civic Orgs",
    "collectors" = "Collectors Clubs",
    "union" = "Unions",
    
    "church_board" = "Church Board",
    "school_board" = "School Board",
    "local_board" = "Local Govt Body"),
    group = factor(group, levels = c(
    "Total", "Community\nSpaces", "Places of\nWorship",
    "Social\nBusinesses", "Parks") %>% rev())) %>%
  mutate(label = paste(round(estimate, 4), gtools::stars.pval(p_value), sep = "")) %>%
  filter(term == "googled")

library(ggh4x)

get_viz = function(data){
  data %>%
    ggplot(mapping = aes(x = group, y = estimate, 
                         ymin = lower_ci, ymax = upper_ci,
                         fill = if_else(estimate > 0, "Positive", "Negative"),
                         label = label)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
    geom_col(width = 0.5) +
    geom_linerange() +
    geom_point() +
    geom_text(nudge_x = 0.40) +
    facet_grid(cols = vars(reorder(sc_type, as.numeric(sc_type))),
               rows = vars(overall_type)
               #scales = "free"
               ) +
    coord_flip() +
    scale_fill_manual(values = c("#DC267F", "#648FFF"),
                      breaks = c("Negative", "Positive")) +
    theme_bw(base_size = 14) +
    scale_y_continuous(expand = expansion(c(0,0)), n.breaks = 4, minor_breaks = NULL) +
    mytheme +
    theme(panel.spacing.x = unit(0.5, "cm"),
          panel.border = element_rect(color = "grey", fill = NA),
          plot.subtitle = ggtext::element_markdown(size = 15),
          axis.title.x = element_text(hjust = 0.5),
          axis.title.y = element_text(hjust = 0),
          strip.text.y = ggtext::element_markdown(size = 12, hjust = 0, angle = 0),
          strip.background.y = element_blank()) +
    labs(subtitle = NULL, x = NULL, y = NULL, fill = NULL) %>%
    return()
}


# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}


# Make visuals, row by row!
g1 <- mystat %>%
  filter(overall_type == "<i>(Quasi)</i><br>Bridging<br>Groups")  %>%
  get_viz() +
  labs(subtitle = "Association between Social Infrastructure and Membership Rates in Community Groups", x = " ")   +
  guides(fill = "none")
g1 <- g1 %>%
  scale_panel_fill(side = "t", fill = rep("#0D0887", 3))

g2 <- mystat %>%
  filter(overall_type == "Bridging<br>Groups")  %>%
  get_viz() +
  labs(   x = "Social Infrastructure Measure")  +
  guides(fill = "none")
g2 <- g2 %>%
  scale_panel_fill(side = "t", fill = rep("#8405A7", 6))

g3 <- mystat %>%
  filter(overall_type == "Linking<br>Groups")  %>%
  get_viz() +
  labs(fill = "Direction of Effect (Beta)",
       y = "Standardized Effect of Site Rates (Z-score) on Membership Rates (Z-score) \n(Standardized Beta Coefficients with 95% Confidence Intervals)",
       x = " ") +
  guides(fill = "none")
g3 <- g3 %>%
  scale_panel_fill(side = "t", fill = rep("#D35171", 3))


combo <- ggpubr::ggarrange(g1,g2,g3, nrow = 3, legend = "bottom", common.legend = TRUE)

ggsave(combo, filename = "viz/figure_C3.png", width = 12, height = 8)


remove(g1,g2,g3,combo)


# Finally, let's do a quick correlation analysis to see how well these values correlate.
dat %>% 
  select(sc = `Social\nCapital`,
         bonding = `Bonding\nSocial Capital`,
         bridging = `Bridging\nSocial Capital`,
         linking = `Linking\nSocial Capital`, 
         fraternal:local_board) %>%
  cor() %>%
  round(2)
```

```{r}
  
  mutate(overall_type = sc_type %>% recode_factor(
    "country_clubs" = "<i>(Quasi)</i><br>Bonding/<br>Bridging<br>Groups",
    "veterans" = "<i>(Quasi)</i><br>Bonding/<br>Bridging<br>Groups",
    "fraternal" = "<i>(Quasi)</i><br>Bonding/<br>Bridging<br>Groups",
    
    "religious" = "Bridging<br>Groups",
    "business" = "Bridging<br>Groups",
    "charitable" = "Bridging<br>Groups",
    "civic" = "Bridging<br>Groups",
    "collectors" = "Bridging<br>Groups",
    "union" = "Bridging<br>Groups",
    
    "church_board" = "Linking<br>Groups",
    "local_board" = "Linking<br>Groups",
    "school_board" = "Linking<br>Groups"),
    
    sc_type = sc_type %>% recode_factor(
    "country_clubs" = "Country Clubs",
    "veterans" = "Veterans Associations",
    "fraternal" = "Fraternal Orders",
    "religious" = "Religious Orgs",
    
    "business" = "Business Clubs",
    "charitable" = "Charitable Orgs",
    "civic" = "Civic Orgs",
    "collectors" = "Collectors Clubs",
    "union" = "Unions",
    
    "church_board" = "Church Board",
    "school_board" = "School Board",
    "local_board" = "Local Govt Body"),
    group = factor(group, levels = c(
    "Total", "Community\nSpaces", "Places of\nWorship",
    "Social\nBusinesses", "Parks") %>% rev())) %>%
```

```{r}


combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_C1.png", dpi = 500, width = 12, height = 5.5)

combo2 <- g1 +
  labs(caption = "<b>Models: </b>Generated from OLS Models, which predict Social Capital Index (0 to 1) using
    the Rate of Social Infrastructure (shown above), Median Income, % Some College,
    <br>
    Income Inequality (Gini Coefficient), % Non-White Population, and Population Density,
    plus annual fixed effects. All models explained between 42-85% of variation in outcome.
    <br>
    <b>Standardized Effects</b>: All numeric variables were rescaled into Z-scores so that beta coefficients reflect the projected increase in Social Capital in standard deviations<br>as the rate of Social Infrastructure increases by 1 standard deviation.
    <b>Colinearity</b>: VIF below 10 for Google Sites (n = 639, Max = 4.6).
    <br>
    <b>Statistical Significance</b>: *** p < 0.001; ** p < 0.01; * p < 0.05; . p < 0.10.") %>%
    scale_panel_fill(side = "t", 
                   fill = c("black", "#0D0887", "#8405A7",
                            "#D35171","#d98d2b")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "covariates/beta_sci.png", dpi = 500, width = 12, height = 6)
```



## 12.3 Voting

### Data


#### Precincts


```{r}
# Download Boston precincts, as of 2017
# https://data.boston.gov/dataset/precincts/resource/95d37dea-9dec-4fda-a8a6-c7d9df7adc35

shapes <- read_sf("covariates/boston_precinct_polygons.kml") %>%
  select(id = OBJECTID, ward_precinct = WARD_PRECINCT, geometry) %>%
  left_join(by = "ward_precinct",
            y = read_csv("covariates/boston_votes_data.csv") ) %>%
  mutate(type = type %>% recode_factor(
    "BIDEN and HARRIS" = "(%) Voted Democrat",
    "TRUMP and PENCE" = "(%) Voted Republican",
    "BALLOTS CAST" = "(%) Voter Turnout")) 
wards <- shapes %>%
  select(ward_precinct, geometry) %>%
  mutate(ward = str_sub(ward_precinct, 1,2)) %>%
  group_by(ward) %>%
  summarize(geometry = st_union(geometry))

g1 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = shapes %>%
            filter(type == "(%) Voted Democrat"),
          size = 0.2, color = "white",
          mapping = aes(fill = percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voted\nDemocrat",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g1, filename = "covariates/map_dem.png", dpi = 500, height = 6, width = 9)


g2 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = shapes %>%
            filter(type == "(%) Voter Turnout"),
          size = 0.2, color = "white",
          mapping = aes(fill = percent)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voter Turnout",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g2, filename = "covariates/map_turnout.png", dpi = 500, height = 6, width = 9)
```

```{r}
# Transform to zipcode
shapes <- read_sf("covariates/boston_precinct_polygons.kml") %>%
  select(id = OBJECTID, ward_precinct = WARD_PRECINCT, geometry) %>%
  left_join(by = "ward_precinct",
            y = read_csv("covariates/boston_votes_data.csv") ) %>%
  mutate(type = type %>% recode_factor(
    "BIDEN and HARRIS" = "(%) Voted Democrat",
    "TRUMP and PENCE" = "(%) Voted Republican",
    "BALLOTS CAST" = "(%) Voter Turnout")) %>%
  st_transform(crs = aea)

read_sf("zipcodes.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = aea) %>%
  st_join(shapes) %>%
  group_by(geoid, type) %>%
  summarize(percent = mean(percent, na.rm = TRUE)) %>%
  ungroup() %>%
  saveRDS("zipcode_votes.rds")
```

#### Polling Places

```{r}
# Download Polling Places in Boston as Points
#https://data.boston.gov/dataset/polling-locations/resource/f5b67c80-98a9-4e51-bfa0-ffdb8961d547

read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/f7c6dc9eb6b14463a3dd87451beba13f_5.kml?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(-c(Name:icon)) %>%

  write_sf("covariates/polling.kml", driver = "kml", delete_layer = TRUE)

#ggplot() +
#  geom_sf(data = poll)
```

```{r}

shapes <- read_sf("covariates/boston_precinct_polygons.kml") %>%
  select(id = OBJECTID, ward_precinct = WARD_PRECINCT, geometry) %>%
  left_join(by = "ward_precinct",
            y = read_csv("covariates/boston_votes_data.csv") %>%
              mutate(type = type %>% recode_factor(
                "BIDEN and HARRIS" = "democrat",
                "TRUMP and PENCE" = "republican",
                "BALLOTS CAST" = "turnout"))  %>%
              pivot_wider(id_cols = c(ward, precinct, ward_precinct),
                          names_from = type, values_from = percent)) %>%
  st_transform(crs = aea) 

wards <- shapes %>%
  select(ward_precinct, geometry) %>%
  mutate(ward = str_sub(ward_precinct, 1,2)) %>%
  group_by(ward) %>%
  summarize(geometry = st_union(geometry))


poll <- read_sf("covariates/polling.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs= aea)

points <- shapes %>%
  group_by(ward_precinct) %>%
  summarize(democrat = unique(democrat),
            republican = unique(republican),
            turnout = unique(turnout),
            geometry = st_centroid(geometry))

buffers <- points %>%
  group_by(ward_precinct) %>%
  summarize(geometry = st_buffer(geometry, dist = 500, endCapStyle = "ROUND"))

g3 <- ggplot() +
  geom_sf(data = wards, fill = NA, color = "grey", size = 5) +
  geom_sf(data = shapes,
          size = 0.2, color = "white",
          mapping = aes(fill = turnout)) +
  geom_sf(data = wards, fill = NA, color = "black", size = 0.5) +
  geom_sf(data = buffers, shape = 21, color = NA, fill = "grey", alpha = 0.25, size = 1) +
  geom_sf(data = poll, shape = 21, color = "white", fill = "black", size = 0.1) +
  theme_bw(base_size = 20) +
  scale_fill_viridis(option = "plasma") +
  labs(fill = "% Voter Turnout",
       subtitle = "Boston Precincts",
       x = "2020 Presidential Election") +
  theme(legend.position = "right",
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(fill = guide_colorsteps(bar.width = 1, bar.height = 10, show.limits = TRUE))

ggsave(g3, filename = "covariates/map_poll.png", dpi = 500, height = 6, width = 9)

```

#### Grid

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"



shapes <- read_sf("covariates/boston_precinct_polygons.kml") %>%
  st_transform(crs = aea) %>%
  select(id = OBJECTID, ward_precinct = WARD_PRECINCT, geometry) %>%
  left_join(by = "ward_precinct",
            y = read_csv("covariates/boston_votes_data.csv") ) %>%
  mutate(type = type %>% recode_factor(
    "BIDEN and HARRIS" = "(%) Voted Democrat",
    "TRUMP and PENCE" = "(%) Voted Republican",
    "BALLOTS CAST" = "(%) Voter Turnout")) 

myvotes <- read_sf("grid_covariates_tracts.kml") %>%
  select(cell_id, geometry) %>%
  st_transform(crs = aea) %>%
  st_join(shapes) %>%
  as.data.frame() %>%
  group_by(cell_id, type) %>%
  summarize_at(vars(percent),
               list(~mean(., na.rm = TRUE)))


grid <- read_rds("tally.rds") %>%
  select(cell_id, group, sites = googled, pop_density = pop_density_int,
         pop_women:pop_age_65_plus) %>%
  mutate(sites = sites / pop_density * 1000) %>%
  left_join(by = "cell_id", y = myvotes) %>%
  filter(pop_density > 0)  


mydat <- grid %>%
  as.data.frame() %>%
  select(-geometry) %>%
   # Calculate sites per capita
  mutate_at(vars(sites),
            funs(. / pop_density * 1000)) %>%
  #select(cell_id, group, year, social_capital:linking, 
  #       googled:ground, pop_density) %>%
  filter(!is.na(sites)) %>%
  filter(group != "Other") %>%
  # Impute small but reasonable value for zero cases,
  # so we can log-transform them
   ungroup() %>%
  mutate(group = group %>% recode_factor(
    "Total" = "Total", 
    "Community Spaces" = "Community\nSpaces",
    "Places of Worship" = "Places of\nWorship",
    "Parks" = "Parks",
    "Social Businesses" = "Social\nBusinesses", 
    "Other" = "Other") %>%
      factor(levels = levels(.)))
```

### Correlation

```{r}
mydat %>% 
  filter(type != "(%) Voted Republican") %>%
  group_by(type, group) %>%
  summarize(cor = cor(percent, sites)) %>%
  ggplot(mapping = aes(x = group, y = cor)) +
  geom_col() +
  facet_grid(cols = vars(type))
```

### Model

```{r}
mytheme <- theme(
    panel.grid = element_blank(),
    panel.border = element_blank(),
    legend.position = "bottom",
    strip.text.x = element_text(color = "white"),
        plot.title = element_markdown(size = 14, hjust = 0.5),
        plot.subtitle = element_markdown(size = 14, hjust = 0.5),
        axis.text.x = element_markdown(size = 10, hjust = 0.5),
                plot.caption = element_markdown(size = 10, hjust = 0))
```
```{r}
myvif <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
            car::vif() %>%
            max() %>%
            data.frame(vif = .),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type"))
# myvif
# Pretty solid! ~4.5

mysum <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  mutate_at(vars(percent, sites, income_inequality,
                 median_income, pop_nonwhite, pop_some_college,
                 pop_density), funs(scale(.) %>% as.numeric())) %>%
  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
              moderndive::get_regression_summaries(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type"))



mystats <- mydat %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
    mutate_at(vars(percent, sites, income_inequality,
                 median_income, pop_nonwhite, pop_some_college,
                 pop_density), funs(scale(.) %>% as.numeric())) %>%

  mutate(type = paste(group, type, sep = " split ")) %>%
  split(.$type) %>%
  map_dfr(~lm(formula = percent ~ sites + 
                income_inequality +
                median_income + 
                pop_nonwhite + pop_some_college +
                pop_density, data = .) %>%
              moderndive::get_regression_table(),
          .id = "type") %>%
  separate(col = type, sep = " split ",
           into = c("group", "type")) %>%
    filter(term == "sites") %>%
  filter(type != "(%) Voted Republican") %>%
  mutate(label = paste(
    round(estimate, 4) %>% format(scientific = FALSE),
    gtools::stars.pval(p_value), sep = ""))

g1 <- mystats %>% 
  mutate(group = factor(group, levels = c(
    "Total", "Community\nSpaces", "Places of\nWorship", 
    "Social\nBusinesses", "Parks") %>% rev())) %>%
  ggplot(mapping = aes(x = group, y = estimate, 
                       ymin = lower_ci, ymax = upper_ci,
                       fill = if_else(estimate > 0, "Positive", "Negative"),
                       label = label)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_col(width = 0.5) +
  geom_linerange() +
  geom_point() +
  geom_text(nudge_x = 0.4) +
  facet_grid(cols = vars(type),
             scales = "free") +
  coord_flip() +
  scale_fill_manual(values = c("#DC267F", "#648FFF")) +
  theme_bw(base_size = 14) +
  #scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  mytheme +
  theme(panel.spacing.x = unit(0.5, "cm"),
        plot.subtitle = ggtext::element_markdown(size = 15),
        panel.border = element_rect(color = "grey", fill = NA)) +
  labs(
    subtitle = "Association between Social Infrastructure and Voting",
    y = "Standardized Effect of Site Rates (Z-score) on Voting Outcomes (Z-score)\n(Standardized Beta Coefficients with 95% Confidence Intervals)",
    fill = "Direction of Effect (Beta)",
    x = "Type of Social Infrastructure")

# Write a quick function to adjust headers
scale_panel_fill = function(myplot, side = "t", fill){
  g <- ggplot_gtable(ggplot_build(myplot))
  strip_r <- which(grepl(paste('strip-', side, sep = ""), g$layout$name))
  fills <- fill
  
  k <- 1
  for (i in strip_r) {
    j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
    g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
    k <- k+1
  }
  
  as_ggplot(g) %>%
    return()
}

combo <- g1 %>%
  scale_panel_fill(side = "t", 
                   fill = c("#0D0887", 
                            "#D35171")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo, filename = "viz/figure_C2.png", dpi = 500, width = 12, height = 5)


combo2 <- g1 + 
labs(caption = "<b>Models: </b>Generated from OLS Models, which predict Percentage Voted (or Voted Democrat) using
    the Rate of Social Infrastructure (shown above), Median Income,<br>% Some College, 
    Income Inequality (Gini Coefficient), % Non-White Population, and Population Density.<br>All models explained between 43-86% of variation in outcome. 
    <b>Colinearity</b>: VIF below 10 for Google Sites (n = 71, Max = 4.5)
    <br>
    <b>Standardized Effects</b>: All numeric variables were rescaled into Z-scores so that beta coefficients reflect the projected increase in Voting Outcomes in standard deviations<br>as the rate of Social Infrastructure increases by 1 standard deviation.
    <b>Statistical Significance</b>: *** p < 0.001; ** p < 0.01; * p < 0.05; . p < 0.10.") %>%
  scale_panel_fill(side = "t", 
                   fill = c("#0D0887", 
                            "#D35171")) +
  ggpubr::bgcolor(color = "white") +
  ggpubr::border(color = "white")

ggsave(combo2, filename = "covariates/beta_vote.png", dpi = 500, width = 12, height = 6)
```


# 13. Race & Ethnicity


```{r, eval = FALSE}
library(tidyverse)
library(sf)

myvars <- c("sites", "parks", "community", "social", "place_of_worship")

mydat <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(myvars), names_to = "variable", values_to = "obs") %>%
  left_join(by = "cell_id", y = read_sf("grid_covariates_tracts.kml") %>%
              as_tibble() %>%
              select(cell_id, pop_white, pop_black, pop_hisplat, pop_asian)) %>%
  as_tibble() %>%
  select(cell_id, variable, obs, pop_white:pop_asian, pop_density) %>%
  mutate(pop_nonwhite = 1 - pop_white)  %>%
  rename(y = obs) %>%
  #filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid.rds")) %>%
  # Impute zeros with very small value to make it log-transformable
  group_by(variable) %>%
  mutate(y = if_else(y == 0, sort(unique(y))[2] / 2, y)) %>%
  ungroup()


  # Rescale outcome variable
  #group_by(variable) %>%
  #mutate(obs = scale(obs) %>% as.numeric()) %>%
  #ungroup()

mytheme <-theme_set(
  theme_bw(base_size = 14) +
    theme(panel.grid = element_blank(),
          axis.text.x = ggtext::element_markdown(size = 14),
          axis.text.y = ggtext::element_markdown(size = 14),
          strip.text.y = ggtext::element_markdown(size = 14),
          strip.text.x = ggtext::element_markdown(size = 14))
  
)
```

```{r}

combo <- bind_rows(
  mydat %>%
    mutate(race = if_else(pop_black > 0.50 | pop_hisplat > 0.50,
                          true = "Black/Hispanic",
                          false = "Other"),
           type = "50"),
  mydat %>%
    mutate(race = if_else(pop_black > 0.60 | pop_hisplat > 0.60,
                          true = "Black/Hispanic",
                          false = "Other"),
           type = "60"),
  mydat %>%
    mutate(race = if_else(pop_black > 0.70 | pop_hisplat > 0.70,
                          true = "Black/Hispanic",
                          false = "Other"),
           type = "70"),
  mydat %>%
    mutate(race = if_else(pop_black > 0.40 | pop_hisplat > 0.40,
                          true = "Black/Hispanic",
                          false = "Other"),
           type = "40"),
  mydat %>%
    mutate(race = if_else(pop_black > 0.30 | pop_hisplat > 0.30,
                          true = "Black/Hispanic",
                          false = "Other"),
           type = "30")) 

mydiff <- combo %>%
  group_by(type, race, variable) %>%
  summarize(mean = mean(y),
            n = n()) %>%
  ungroup() %>%
  mutate(group = if_else(race == "Other", "Other", variable)) %>%
  mutate(type = type %>% recode_factor(
    `30` = ">30%\nBlack / Hispanic\n(n = 74 vs. 109)", 
    `40` = ">40%\nBlack / Hispanic\n(n = 54 vs. 129)", 
    `50` = ">50%\nBlack / Hispanic\n(n = 31 vs. 152)", 
    `60` = ">60%\nBlack / Hispanic\n(n = 19 vs. 164)", 
    `70` = ">70%\nBlack / Hispanic\n(n = 5 vs. 178)"),
   variable = variable %>% recode_factor(
      "sites" = "<b>Total Sites</b><br>(n = 1208)",
      "community" = "<span style='color:#0D0887'><b>Community Spaces</b></span><br>(n = 225)",
      "place_of_worship" = "<span style='color:#8405A7'><b>Places of Worship</b></span><br>(n = 224)",
      "social" = "<span style='color:#D35171'><b>Social Businesses</b></span><br>(n = 225)",
      "parks" = "<span style='color:#d98d2b'><b>Parks</b></span><br>(n = 534)")) %>%
  mutate(race = race %>% recode_factor(
    "Black/Hispanic" =  "Black/<br>Hispanic",
    "Other" = "Other"))  %>%
  group_by(variable) %>%
  mutate(ybase = mean(mean) / 4,
         ymid = mean(mean))

mytest <- combo %>% 
  mutate(mysplit = paste(type, " - ", variable, sep = "")) %>%
  split(.$mysplit) %>%
  map_dfr(~infer::t_test(x = ., formula = y ~ race, order = c("Black/Hispanic", "Other")), 
          .id = "mysplit") %>%
  separate(col = "mysplit", into = c("type", "variable"), sep = " - ") %>%
    mutate(type = type %>% recode_factor(
    `30` = ">30%\nBlack / Hispanic\n(n = 74 vs. 109)", 
    `40` = ">40%\nBlack / Hispanic\n(n = 54 vs. 129)", 
    `50` = ">50%\nBlack / Hispanic\n(n = 31 vs. 152)", 
    `60` = ">60%\nBlack / Hispanic\n(n = 19 vs. 164)", 
    `70` = ">70%\nBlack / Hispanic\n(n = 5 vs. 178)"),
    variable = variable %>% recode_factor(
      "sites" = "<b>Total Sites</b><br>(n = 1208)",
      "community" = "<span style='color:#0D0887'><b>Community Spaces</b></span><br>(n = 225)",
      "place_of_worship" = "<span style='color:#8405A7'><b>Places of Worship</b></span><br>(n = 224)",
      "social" = "<span style='color:#D35171'><b>Social Businesses</b></span><br>(n = 225)",
      "parks" = "<span style='color:#d98d2b'><b>Parks</b></span><br>(n = 534)")) %>%
  mutate(sig = if_else(p_value < 0.05, "p < 0.05", "insignificant")) %>%
  mutate(diff = paste(round(estimate, 2), gtools::stars.pval(p_value), sep = "")) %>%
  left_join(by = "variable", y = mydiff %>% select(variable, ybase, ymid) %>% distinct())

g1 <- ggplot() +
  geom_col(data = mydiff,
           mapping = aes(x = race, y = mean, fill = group, color = race),
           size = 0.5, width = 0.75) +
  geom_line(data = mydiff,
            mapping = aes(x = race, y = mean, group = 1)) +
  geom_point(data = mydiff, 
             mapping = aes(x = race, y = mean, fill = group, color = race),
             shape = 21, fill = "white", size = 2) +
  geom_text(data = mydiff %>%
              filter(race == "Black/<br>Hispanic"),
            mapping = aes(x = race, y = ybase, group = group,
                          label = round(mean, 2)), color = "white") +
  geom_text(data = mydiff %>%
              filter(race != "Black/<br>Hispanic"),
            mapping = aes(x = race, y = ybase, group = group,
                          label = round(mean, 2)), color = "black") +
  geom_label(data = mytest,
            mapping = aes(x = 1.5, y = ymid, 
                          label = diff), fill = "white", alpha = 0.75) +
  
  scale_color_manual(values = c("black", "darkgrey")) +
  scale_fill_manual(
    breaks = c("sites", "community", "place_of_worship", "social", "parks", "Other"),
    values = c("black", "#0D0887", "#8405A7", "#D35171", "#d98d2b", "grey")) +
  facet_grid(
    rows = vars(variable), 
    cols = vars(type), scales = "free_y") +
       theme_bw(base_size = 14) +
  theme(strip.text.y = ggtext::element_markdown(size = 14, angle = 0, hjust = 0),
        axis.text.x = ggtext::element_markdown(size = 14),
        legend.text = ggtext::element_markdown(size = 14),
        strip.background.y = element_blank(),
        plot.caption = ggtext::element_markdown(size = 10, hjust = 0),
        strip.background.x = element_rect(fill = "black", color = "white"),
        strip.text.x = element_text(size = 12, color = "white"),
        panel.grid = element_blank(),
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(color = "none", fill = "none") +
  labs(subtitle = "Lower Rates of Social Infrastructure in Black/Hispanic Neighborhoods", 
       y = "Average Rate of Social Infrastructure\nper 1,000 residents",
       x = "Boston Cells by Resident Race & Ethnicity")

ggsave(g1, filename = "viz/figure_C4.png", dpi = 500, width = 10, height = 6.8)

g1b <- g1 + 
  labs(caption = "<b>Statistical Significance</b>: *** p < 0.001, ** p < 0.01, * p < 0.05, . p < 0.10. Based on two-tailed t-tests.<br><b>Sample Size</b> for each group written atop panels as number of cells above vs. below threshold (eg. >30%).")


ggsave(g1b, filename = "covariates/bar_race.png", dpi = 500, width = 10, height = 7)
```

```{r}
onediff <- mydiff %>%
  filter(type == ">50%\nBlack / Hispanic\n(n = 31 vs. 152)")
onetest <- mytest %>%
  filter(type == ">50%\nBlack / Hispanic\n(n = 31 vs. 152)")
  
g2 <- ggplot() +
  geom_col(data = onediff,
           mapping = aes(x = race, y = mean, fill = group, color = race),
           size = 0.5, width = 0.75) +
  geom_line(data = onediff,
            mapping = aes(x = race, y = mean, group = 1)) +
  geom_point(data = onediff, 
             mapping = aes(x = race, y = mean, fill = group, color = race),
             shape = 21, fill = "white", size = 2) +
  geom_text(data = onediff %>%
              filter(race == "Black/<br>Hispanic"),
            mapping = aes(x = race, y = ybase, group = group,
                          label = round(mean, 2)), color = "white") +
  geom_text(data = onediff %>%
              filter(race != "Black/<br>Hispanic"),
            mapping = aes(x = race, y = ybase, group = group,
                          label = round(mean, 2)), color = "black") +
  geom_label(data = onetest,
            mapping = aes(x = 1.5, y = ymid, 
                          label = diff), fill = "white", alpha = 0.75) +
  
  scale_color_manual(values = c("black", "darkgrey")) +
  scale_fill_manual(
    breaks = c("sites", "community", "place_of_worship", "social", "parks", "Other"),
    values = c("black", "#0D0887", "#8405A7", "#D35171", "#d98d2b", "grey")) +
  facet_wrap(~variable, ncol = 5,
    scales = "free_y") +
       theme_bw(base_size = 14) +
  theme(strip.text.x = ggtext::element_markdown(size = 14, angle = 0, hjust = 0.5),
        axis.text.x = ggtext::element_markdown(size = 14),
        legend.text = ggtext::element_markdown(size = 14),
        strip.background.y = element_blank(),
        plot.caption = ggtext::element_markdown(size = 10, hjust = 0),
        strip.background.x = element_rect(fill = "white", color = "white"),
        panel.grid = element_blank(),
        plot.subtitle = element_text(hjust = 0.5)) +
  guides(color = "none", fill = "none") +
  labs(subtitle = "Lower Rates of Social Infrastructure in Black/Hispanic Neighborhoods", 
       y = "Average Rate of Social Infrastructure\nper 1,000 residents",
       x = "Boston Cells by Resident Race & Ethnicity")

ggsave(g2, filename = "viz/figure_5.png", dpi = 500, width = 12, height = 5)

g2b <- g2 +
       labs(caption = "<b>Statistical Significance</b>: *** p < 0.001, ** p < 0.01, * p < 0.05, . p < 0.10. Based on two-tailed t-tests.<br><b>Sample Size:</b> Examines 31 city blocks where over 50% of resident are Black or Hispanic, vs. 152 other neighborhoods.")


ggsave(g2b, filename = "covariates/bar_race_just_one.png", dpi = 500, width = 12, height = 5)
```

## Check Demographics

```{r}
myvars <- c("sites", "parks", "community", "social", "place_of_worship")

mydat <- read_rds("processed_queries/sites_crosstab.rds") %>%
  pivot_longer(cols = c(myvars), names_to = "variable", values_to = "obs") %>%
  left_join(by = "cell_id", y = read_sf("grid_covariates_tracts.kml") %>%
              as_tibble() %>%
              select(cell_id, neighborhood, median_income, 
                     pop_white, pop_black, pop_hisplat, pop_asian)) %>%
  as_tibble() %>%
  select(cell_id, variable, neighborhood, obs, median_income, pop_white:pop_asian, pop_density) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  rename(y = obs) 

mydat %>% 
  group_by(neighborhood) %>%
  summarize(min = min(median_income, na.rm = TRUE) %>% round(0),
            max = max(median_income, na.rm = TRUE) %>% round(0))
```


# 14. Map of Ground-Truthed Cells

## 14.1 Get Data

```{r}
dir.create("groundtruth_map")

library(tidyverse)
library(sf)
library(ggspatial)

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

```

```{r, include = FALSE, eval = FALSE}
# Get names of our 10 grid cells 
sample <- read_sf("grid_samples_for_ground_truthing.kml") %>%
  filter(type == "masters") %>%
  as_tibble() %>% 
  select(cell_id = Name)

read_rds("tally.rds") %>%
  filter(cell_id %in% sample$cell_id) %>%
  filter(group == "Total") %>%
  select(cell_id, group, neighborhood, pop_white, pop_density_int, ground) %>%
  mutate(rate = ground / pop_density_int * 1000) %>%
  mutate(neighborhood = case_when(
    cell_id == "Cell 183" ~ "North End",
    cell_id == "Cell 124" ~ "Fenway (Northeastern)",
    cell_id == "Cell 142" ~ "Fenway (Boston U.)",
    cell_id == "Cell 82" ~ "Jamaica Plains (Pondside)",
    TRUE ~ neighborhood)) %>%
  saveRDS("groundtruth_map/blocks.rds")
```

```{r, eval = FALSE, include = FALSE}
tigris::area_water(state = "MA", county = "025", year = 2019) %>%
  st_as_sf() %>%
  st_transform(crs = aea) %>%
  summarize(geometry = st_union(geometry)) %>%
  saveRDS("groundtruth_map/water.rds")


tigris::linear_water(state = "MA", county = "025", year = 2019) %>%
  st_as_sf() %>%
  st_transform(crs = aea) %>%
  saveRDS("groundtruth_map/rivers.rds")

```
```{r, eval = FALSE, include = FALSE}
dir.create("groundtruth_map/contour")
download.file(url = "https://s3.us-east-1.amazonaws.com/download.massgis.digital.mass.gov/shapefiles/state/contours250k.zip", destfile = "groundtruth_map/contour.zip")

unzip("groundtruth_map/contour.zip", exdir = "groundtruth_map/contour")

read_sf("groundtruth_map/contour/CONTOURS250K_ARC.shp") %>%
  st_transform(crs = aea) %>%
  st_join(read_rds("groundtruth_map/blocks.rds") %>% st_transform(crs = aea)) %>%
  filter(!is.na(cell_id)) %>%
  select(feet = CONTOUR_FT, cell_id, geometry) %>%
  write_rds("groundtruth_map/elevation.rds")
```

```{r, eval = FALSE, include = FALSE}
## Streets
download.file(url = "https://bostonopendata-boston.opendata.arcgis.com/datasets/cfd1740c2e4b49389f47a9ce2dd236cc_8.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D",
              destfile = "groundtruth_map/streets.geojson")

read_sf("groundtruth_map/streets.geojson") %>%
  select(street = ST_NAME, geometry) %>%
  group_by(street) %>%
  summarize(geometry = st_union(geometry)) %>%
  st_write("groundtruth_map/streets.geojson", delete_dsn = TRUE)

# Get list of shapes
mystreets <- read_rds("groundtruth_map/blocks.rds") %>%
  st_join(read_sf("groundtruth_map/streets.geojson") %>% st_transform(crs = aea)) %>%
  as_tibble() %>%
  select(street, cell_id) %>%
  distinct()

read_sf("groundtruth_map/streets.geojson") %>%
  filter(street %in% mystreets$street) %>%
  mutate(street = str_remove_all(street, pattern = " ") %>% na_if("")) %>%
  filter(!is.na(street)) %>%
  # Join in cell by cell data for streets that remain
  left_join(by = c("street"), y = mystreets) %>%
  # Convert to AEA
  st_transform(crs = aea) %>%
  write_rds("groundtruth_map/streets_block.rds")


remove(mystreets)
```


```{r, eval = FALSE, include = FALSE}
# Buildings
download.file(url = "https://bostonopendata-boston.opendata.arcgis.com/datasets/8bf3e3b0bde0432c82f76ee6a0608e7d_0.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D",
              destfile = "groundtruth_map/buildings.geojson")

read_sf("groundtruth_map/buildings.geojson") %>%
  select(building = BLDG_ID, geometry) %>%
  st_write("groundtruth_map/buildings.geojson", delete_dsn = TRUE)

#Get list of buildings
mybuildings <- read_rds("groundtruth_map/blocks.rds") %>%
  st_join(read_sf("groundtruth_map/buildings.geojson") %>% st_transform(crs = aea)) %>%
  as_tibble() %>%
  select(building, cell_id) %>%
  distinct()

read_sf("groundtruth_map/buildings.geojson") %>%
  filter(building %in% mybuildings$building) %>%
  mutate(building = str_remove_all(building, pattern = " ") %>% na_if("")) %>%
  filter(!is.na(building)) %>%
  # Join in cell by cell data for streets that remain
  left_join(by = c("building"), y = mybuildings) %>%
  # Convert to AEA
  st_transform(crs = aea) %>%
  write_rds("groundtruth_map/buildings_block.rds")

remove(mybuildings)
```

```{r, eval = FALSE, include = FALSE}
# Create folder for transit data
dir.create("groundtruth_map/transit")
# Download it from source (MASS GIS)
download.file(url = "https://s3.us-east-1.amazonaws.com/download.massgis.digital.mass.gov/shapefiles/state/mbta_rapid_transit.zip", destfile = "groundtruth_map/transit/transit.zip")
# Unzip the files we need
unzip(zipfile = "groundtruth_map/transit/transit.zip", 
      exdir = "groundtruth_map/transit")
read_sf("groundtruth_map/transit/MBTA_ARC.shp") %>%
  st_transform(crs = aea) %>%
  group_by(line = LINE) %>%
  summarize(geometry = st_union(geometry)) %>%
  saveRDS("groundtruth_map/train_lines.rds")

read_sf("groundtruth_map/transit/MBTA_NODE.shp") %>%
  st_transform(crs = aea) %>%
  select(stop = STATION, line = LINE) %>%
  saveRDS("groundtruth_map/train_stops.rds")
```

```{r, eval = FALSE, include = FALSE}
# Colleges
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/cbf14bb032ef4bd38e20429f71acb61a_2.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(id = SchoolId, name = Name, area = City, year_built = YearBuilt, 
         students = NumStudent, cost = Cost, url = URL, geometry) %>%
  mutate_at(vars(id, area, year_built),
            funs(na_if(., ""))) %>%
  mutate(url = if_else(url == "N/A", NA_character_, url)) %>%
  st_transform(crs = aea) %>%
  saveRDS("groundtruth_map/universities.rds")
```

## 14.2 Interpolate Race

```{r}
library(gstat)
library(sf)
library(rgdal)
library(tidyverse)

aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

shapes <- read_sf("block_groups.kml") %>%
  select(-c(Name:icon)) %>%
  left_join(by = "geoid", 
            y = read_csv("block_groups_census.csv") %>%
              mutate(geoid = as.character(geoid))) %>%
  st_transform(crs = aea) %>%
  # Drop Z-axis, which doesn't play well with gstat
  st_zm(drop = TRUE) %>%
  mutate(pop_nonwhite = 1 - pop_white) %>%
  select(geoid, pop_nonwhite, geometry) 

# Import blocks
blocks <- read_rds("groundtruth_map/blocks.rds") %>%
  # Let's try 1000 meters (1 city block is 100 meters long; 10 blocks are 1000 meters long)
  st_make_grid(cellsize = 100, square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  st_join(read_rds("groundtruth_map/blocks.rds")) %>%
  filter(!is.na(cell_id)) %>%
  select(cell_id) %>%
  mutate(id = 1:n())

#ggplot() +
  geom_sf(data = blocks)

m <- gstat::gstat(
    id = "cell_id", 
    formula = pop_nonwhite ~ 1, 
    set = list(idp = 2),
    data = shapes %>%
      na.omit() %>%
      as(Class = "Spatial")) %>%
  # Build shapes
  predict(blocks) %>%
  mutate(id = blocks$id,
         cell_id = blocks$cell_id) %>%
  as.data.frame() %>%
  select(id, cell_id, pop_nonwhite_int = cell_id.pred)

blocks %>%
  left_join(by = c("cell_id", "id"), y = m) %>%
  saveRDS("groundtruth_map/race_interpolated.rds")
```

## 14.3 Icons

```{r}
htmltools::tagList(fontawesome::fa_html_dependency())

library(fontawesome)
library(rsvg)
library(magick)
library(ggimage)
library(ggtext)

fa_png(name = "hands-helping",
       fill = "grey", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/hands-helping-grey.png")
fa_png(name = "store-alt",
       fill = "#0D0887", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/store-alt-blue.png")
fa_png(name = "place-of-worship",
       fill = "#8405A7", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/place-of-worship-purple.png")
fa_png(name = "coffee",
       fill = "#D35171", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/coffee-red.png")
fa_png(name = "tree",
       fill = "#d98d2b", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/tree-yellow.png")
fa_png(name = "times-circle",
       fill = "#333333", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/times-circle.png")
fa_png(name = "question-circle",
       fill = "#787276", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/question-circle.png")
fa_png(name = "dot-circle",
       fill = "#787276", fill_opacity = 1, 
       stroke = "white", stroke_width = 50, 
       file = "groundtruth_map/dot-circle.png")

myimages <- data.frame(
  myname = c("hands-helping-grey",
             "store-alt-blue",
             "place-of-worship-purple",
             "coffee-red",
             "tree-yellow",
             "dot-circle",
             "times-circle",
             "question-circle")) %>%
  mutate(icon = paste("groundtruth_map/", myname, ".png", sep = "")) %>%
  mutate(type = c("Total", "Community Spaces", "Places of Worship", 
                  "Social Businesses", "Parks", "Other",
                  "Not social infrastructure", "API Sites Not Found"))


# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds") 

mygoogle <- read_sf("mygoogle.kml") %>%
  select(-c(timestamp:icon))  %>%
  select(Name, group, status, geometry) %>%
  # Filter out any places Here that actually were found
  filter(!Name %in% myground$title) %>%
  filter(status != "Double", status != "New Sites") %>%
  mutate(status = status %>% na_if("") %>% tolower() %>% str_trim()) %>%
  mutate(status = case_when(
    status == "not social infrastructure" ~ "Not social infrastructure",
    TRUE ~ "API Sites Not Found")) 


# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


bind_rows(
  myground %>%
    mutate(status = "Confirmed in Field"),
  mygoogle) %>%
  mutate(type = case_when(
    status == "Confirmed in Field" ~ group,
    status != "Confirmed in Field" ~ status)) %>%
  select(cell_id, title, type, geometry) %>%
  st_transform(crs = aea) %>%
  as_tibble() %>% 
  left_join(by = "type", y = myimages %>% select(type, icon)) %>%
  saveRDS("groundtruth_map/points.rds")

remove(myground, mygoogle, myimages)


```

## 14.4 Map Data

```{r}
library(tidyverse)
library(sf)
library(ggspatial)
library(ggtext)
library(magick)
library(ggimage)

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"


myblock <- read_rds("groundtruth_map/blocks.rds") %>%
  filter(cell_id == "Cell 124")
mystreets <- read_rds("groundtruth_map/streets_block.rds") %>%
  filter(cell_id == "Cell 124")
mybuildings <- read_rds("groundtruth_map/buildings_block.rds") %>%
  filter(cell_id == "Cell 124")
mylines <- read_rds("groundtruth_map/train_lines.rds")
mystops <- read_rds("groundtruth_map/train_stops.rds") %>%
  mutate(line = if_else(str_detect(line, "[/]"), "MULTIPLE", line))  %>%
    # Filter to just those within the cell
  st_join(myblock %>% select(cell_id)) %>%
  filter(!is.na(cell_id))
myuni <- read_rds("groundtruth_map/universities.rds") %>%
  filter(students > 2000) %>%
  # Filter to just those within the cell
  st_join(myblock %>% select(cell_id)) %>%
  filter(!is.na(cell_id))
mybox <- myblock %>% st_bbox()
tinyblocks <- read_rds("groundtruth_map/race_interpolated.rds") %>%
  filter(cell_id == "Cell 124")
mypoints <- read_rds("groundtruth_map/points.rds") %>%
  filter(cell_id == "Cell 124") %>%
  st_as_sf() %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) 

g1 <- ggplot() +
  geom_sf(data = myblock, fill = "black", color = "black", size = 5) +
  geom_sf(data = tinyblocks, 
          mapping = aes(fill = pop_nonwhite_int*100), color = NA) +
  scale_fill_gradient(low = "tan", high = "darkorchid",
                      limits = c(0, 100)) +
  geom_sf(data = mybuildings, fill = "#3e3d53", color = "black") +
  geom_sf(data = mystreets, color = "#7f7d9c", size = 1.5) +
  # Add T-lines
  geom_sf(data = mylines, color = "white", size = 4) +
  geom_sf(data = mylines, mapping = aes(color = line), size = 3) +
  # Add T-stops
  geom_sf(data = mystops, mapping = aes(color = line), size = 5) +
  # Add T-stop labels
  geom_sf_label(data = mystops, mapping = aes(label = stop, color = line)) +
  # Give our T-lines appropriate colors
  scale_color_manual(
    # Sort the legend in this order
    breaks = c("ORANGE", "RED", "GREEN", "BLUE", "SILVER", "MULTIPLE"),
    # Give them the following colors
    values = c("darkorange", "firebrick", "seagreen", "deepskyblue", "grey", "black")) +
  # Add University labels
  geom_sf_label(data = myuni, mapping = aes(label = name), size = 5) +
  # Add points
  geom_sf(data = mypoints, 
            mapping = aes(
              x = x, y = y), 
          shape = 21, color = "black", fill = "white", size = 3) +
  geom_image(data = mypoints, 
            mapping = aes(
              x = x, y = y, image = icon)) +
  coord_sf(xlim = c(mybox["xmin"] + 45, mybox["xmax"] - 45),
           ylim = c(mybox["ymin"] + 45, mybox["ymax"] - 45), crs = aea) +
  theme_void(base_size = 14) +
  theme(panel.border = element_rect(fill = NA, color = "black", size = 2)) +
  theme(plot.caption = element_text(hjust = 0)) +
  labs(fill = "% Non-White*") +
  guides(color = "none",
         fill = guide_colorbar())


g2 <- g1 + 
  labs(caption = "* Interpolated to 100 square meter level from American Community Survey\nusing 2014-2019 5-year averages for census block groups.")

ggsave(g2, filename = "groundtruth_map/cell.png", 
       dpi = 500, width = 5, height =5)

remove(myblock, mybuildings, mylines, mypoints, mystops, mystreets, myuni, tinyblocks, mybox)

```

### Loop

```{r}
library(tidyverse)
library(sf)
library(ggspatial)
library(ggtext)
library(magick)
library(ggimage)

# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

get_viz = function(mycell){
  myblock <- read_rds("groundtruth_map/blocks.rds") %>%
    filter(cell_id == mycell)
  mystreets <- read_rds("groundtruth_map/streets_block.rds") %>%
    filter(cell_id == mycell)
  mybuildings <- read_rds("groundtruth_map/buildings_block.rds") %>%
    filter(cell_id == mycell)
  myelevation <- read_rds("groundtruth_map/elevation.rds") %>%
    filter(cell_id == mycell)
  mylines <- read_rds("groundtruth_map/train_lines.rds")
  mystops <- read_rds("groundtruth_map/train_stops.rds") %>%
    mutate(line = if_else(str_detect(line, "[/]"), "MULTIPLE", line)) 
    # Filter to just those within the cell
  #  st_join(myblock %>% select(cell_id)) %>%
  #  filter(!is.na(cell_id))
  #myuni <- read_rds("groundtruth_map/universities.rds") %>%
  #  filter(students > 2000) %>%
    # Filter to just those within the cell
  #  st_join(myblock %>% select(cell_id)) %>%
  #  filter(!is.na(cell_id))
  mybox <- myblock %>% st_bbox()
  tinyblocks <- read_rds("groundtruth_map/race_interpolated.rds") %>%
    filter(cell_id == mycell)
  mywater <- read_rds("groundtruth_map/water.rds")
  myriver <- read_rds("groundtruth_map/rivers.rds")
  
  mypoints <- read_rds("groundtruth_map/points.rds") %>%
    filter(cell_id == mycell) %>%
    st_as_sf() %>%
    mutate(x = st_coordinates(geometry)[,1],
           y = st_coordinates(geometry)[,2]) 
  
  g1 <- ggplot() +
    geom_sf(data = myblock, fill = "black", color = "black", size = 5) +
    geom_sf(data = tinyblocks, 
            mapping = aes(fill = pop_nonwhite_int*100), color = NA) +
    scale_fill_gradient(low = "white", high = "darkorchid",
                        limits = c(0, 100)) +
    geom_sf(data = myriver, color = "steelblue") +
    geom_sf(data = mywater, fill = "lightblue", color = "steelblue") +
    geom_sf(data = myelevation, color = "white", alpha = 0.75) +
    geom_sf(data = mybuildings, fill = "#3e3d53", color = "black") +
    geom_sf(data = mystreets, color = "#7f7d9c", size = 1.25) +
    # Add T-lines
    geom_sf(data = mylines, color = "white", size = 3) +
    geom_sf(data = mylines, mapping = aes(color = line), size = 2) +
    # Add T-stops
    geom_sf(data = mystops, color = "white", size = 4) +
    geom_sf(data = mystops, mapping = aes(color = line), size = 3) +
    # Add T-stop labels
    #geom_sf_label(data = mystops, mapping = aes(label = stop, color = line)) +
    # Give our T-lines appropriate colors
    scale_color_manual(
      # Sort the legend in this order
      breaks = c("ORANGE", "RED", "GREEN", "BLUE", "SILVER", "MULTIPLE"),
      # Give them the following colors
      values = c("darkorange", "firebrick", "seagreen", "deepskyblue", "grey", "black")) +
    # Add University labels
    #geom_sf_label(data = myuni, mapping = aes(label = name), size = 5) +
    # Add points, to provide a simple white background
    # behind all image
    #geom_sf(data = mypoints, 
    #        mapping = aes(
    #          x = x, y = y), 
    #        shape = 21, color = "black", stroke = 0.5,
    #        fill = "white", size = 5, alpha = 0.75) +
    # Layer images over
    geom_image(data = mypoints, 
               mapping = aes(
                 x = x, y = y, image = icon), size = 0.075) +
    coord_sf(xlim = c(mybox["xmin"] + 45, mybox["xmax"] - 45),
             ylim = c(mybox["ymin"] + 45, mybox["ymax"] - 45), crs = aea) +
    labs(fill = "% Non-White Residents",
         subtitle = paste(myblock$cell_id, "\nin ", myblock$neighborhood, sep = ""),
         caption = paste("Rate: ", round(myblock$rate, 1), sep = "")) +
         #
#         " in ", myblock$neighborhood, sep = "")
    guides(color = "none",
           fill = "none")
  remove(myblock, mybuildings, mylines, mypoints, 
         mystops, mystreets, myuni, tinyblocks, mybox)
  
  return(g1)
}

allviz <- data.frame(cell_id = read_rds("groundtruth_map/blocks.rds")$cell_id) %>%
  split(.$cell_id) %>%
  map(~get_viz(.$cell_id) +
        theme_void(base_size = 20) + 
        theme(plot.margin = margin(0,0,0,0, "cm"),
              plot.subtitle = element_text(
                size = 50, hjust = 0.5, lineheight = 0.2, vjust = 0.1,
                margin = margin(0,0,0,0,"cm")),
             plot.caption = element_text(
                size = 50, hjust = 0.5, lineheight = 0.2, vjust = 0.9,
                margin = margin(0,0,0,0,"cm")),
              panel.border = element_rect(fill = NA, color = "black", size = 2),
              legend.margin = margin(0.1,0.1,0.1,0.1,"cm"),
              legend.text = element_text(size = 40, hjust = 0,vjust = 0, lineheight = 0),
              legend.title = element_text(size = 50, hjust = 0, lineheight = 0)), 
      .id = "cell_id")

combo <- ggpubr::ggarrange(
  plotlist = allviz,
  ncol = 5, nrow = 2,
  legend = "none")

ggsave(combo, filename = "groundtruth_map/cell_all.png", 
       dpi = 500, width = 10, height = 5.5)

ggsave(combo, filename = "viz/figure_A8.png", 
       dpi = 500, width = 10, height = 5.5)
#       caption = "* Interpolated to 100 square meter level from American Community Survey\nusing 2014-2019 5-year averages for census block groups."


```

```{r}
mybox

ggplot() +
  geom_sf(data = mylines) +
  geom_image(data = mypoints %>% sample_n(size = 5),
          mapping = aes(x = x*-1, y = y*-1, image = icon))

myblock
#  xmin: 1,906,829 ymin: 527,765.5 xmax: 1907829 ymax: 528765.6
#  xmin: 16,365,120 ymin: 5,786,835 xmax: 16365470 ymax: 5788196
mypoints %>%
  st_as_sf()


ggplot() +
  geom_sf(data = myblock) +
  geom_point(data = mypoints, mapping = aes(x = x / 10, y = y / 10))
```




```{r, eval = FALSE, include = FALSE}
## Trees
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/edd17607c92348d2ab06c30bbe0aa9b7_0.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  filter(Inspected == "Yes") %>% 
  select(id = FID, species = Species, geometry) %>%
  st_write("groundtruth_map/trees.geojson", delete_dsn = TRUE)
```




```{r, eval = FALSE, include = FALSE}
# Buildings
download.file(url = "https://bostonopendata-boston.opendata.arcgis.com/datasets/8bf3e3b0bde0432c82f76ee6a0608e7d_0.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D",
              destfile = "raw_data/buildings.geojson")

read_sf("raw_data/buildings.geojson") %>%
  select(building = BLDG_ID, geometry) %>%
  st_write("raw_data/buildings.geojson", delete_dsn = TRUE)
```

```{r, eval = FALSE, include = FALSE}
neigh <- read_sf("neighborhoods.geojson") %>%
  filter(name == "Fenway") %>%
  st_join(read_sf("raw_data/buildings.geojson")) %>%
  as_tibble() %>%
  select(name, building)

read_sf("raw_data/buildings.geojson") %>%
  filter(building %in% neigh$building) %>%
  mutate(building = str_remove_all(building, pattern = " ") %>% na_if("")) %>%
  filter(!is.na(building)) %>%
  mutate(neighborhood = "Fenway") %>%
  st_write("buildings_fenway.geojson", delete_dsn = TRUE)

remove(neigh)
```

```{r, eval = FALSE, include = FALSE}
# Colleges
read_sf("https://bostonopendata-boston.opendata.arcgis.com/datasets/cbf14bb032ef4bd38e20429f71acb61a_2.geojson?outSR=%7B%22latestWkid%22%3A2249%2C%22wkid%22%3A102686%7D") %>%
  select(id = SchoolId, name = Name, area = City, year_built = YearBuilt, 
         students = NumStudent, cost = Cost, url = URL, geometry) %>%
  mutate_at(vars(id, area, year_built),
            funs(na_if(., ""))) %>%
  mutate(url = if_else(url == "N/A", NA_character_, url)) %>%
  st_write("universities.geojson")
```






# 15. Export

```{r}
dir.create("export")
```

## Masters

```{r}

# Import grid!
grid <- read_sf("milestones.kmz") %>%
  # Exclude extraneous files
  select(cell_id = Name, neighborhood, milestone) %>%
    # convert to equal area conic projection
  st_transform(crs = aea)

read_sf("validated/sites.kml") %>%
  # Get rid of fluff kml fields
  select(-c(timestamp:icon)) %>%
  # Zoom into just valid sites,
  # excluding any sites we identified were not social infrastrucutre,
  # which were not found, duplicated, or not yet verified ("")
  mutate(status = status %>% tolower() %>% str_trim() %>% na_if("")) %>%
  # Clean initials of data coders
  mutate(description = tolower(description) %>% 
           na_if("") %>% str_trim() %>% recode(
             "checked" = NA_character_,
             "j" = "jz", "nc " = "nc")) %>%
  mutate(group = group %>% tolower() %>% str_trim() %>% na_if("") %>%
           recode(
             "community space" = "community space", 
             "community spaces" ="community space",
             "social businesses" = "social business", 
             "social business" = "social business",
             "social busiesses" = "social business",
             "parks" = "park",
             "places of worship" = "place of worship",
             "places of worhsip" = "place of worship",
             "places of worsship" = "place of worship",
             "places of worhip" = "place of worship")) %>%
  # add unique id for each point
  mutate(id = 1:n()) %>%
  # condense number of columsn
  select(Name, description, id, group, status, 
         google_id = place_id, geometry) %>%
  # convert to equal area conic projection
  st_transform(crs = aea) %>%
  # Join in grid cells
  st_join(grid) %>%
  # Filter points to just those located within milestones 1, 2, and 3.
  filter(str_detect(milestone, pattern = "M1|M2|M3")) %>%
  select(Name, description, id, group, status, google_id, 
         cell_id, neighborhood, milestone, geometry) %>%
  mutate(group = group %>% recode(
    "community space" = "Community Spaces",
    "place of worship" = "Places of Worship",
    "park" = "Parks",
    "social business" = "Social Businesses",
    "other"= "Other")) %>%
  st_transform(crs = wgs) %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  as.data.frame() %>%
  write_csv("export/allsites_core_boston.csv")
```

```{r}
read_sf("block_groups.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs= wgs) %>%
  left_join(by = "geoid", 
            y = read_csv("block_groups_census.csv", 
                         col_types = list(geoid = col_character() ))) %>%
  st_write("export/block_groups.shp")

read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs= wgs) %>%
  left_join(by = "geoid", 
            y = read_csv("tracts_census.csv", 
                         col_types = list(geoid = col_character() ))) %>%
  st_write("export/tracts.shp")
```

## Workshop

```{r}
# Join together the block groups
mydat <- read_csv("block_groups_census.csv", 
         col_types = cols(geoid = col_character())) %>%
  select(geoid, pop, pop_women, pop_age_65_plus, 
         pop_black, pop_white, pop_asian,
         median_income, pop_unemployed)

read_sf("block_groups.kml") %>%
  select(geoid:geometry) %>%
  filter(area_land > 0) %>%
  left_join(by = "geoid", y = mydat) %>%
  st_cast(to = "POLYGON") %>%
  st_write("export/boston_block_groups.geojson", delete_dsn = TRUE)

remove(mydat)

# Get the points
read_sf("mysites_core_boston.kml") %>%
  # Filter out parking lots
  filter(str_detect(Name, "Parking|parking", negate = TRUE)) %>%
  select(Name, id, neighborhood, group) %>%
  st_write("export/boston_social_infra.geojson", delete_dsn = TRUE)

read_rds("dashdata/train_lines_boston.rds") %>%
  st_write("export/train_lines_boston.geojson", delete_dsn = TRUE)
```


```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

shapes <- read_sf("export/boston_block_groups.geojson") 

points <- read_sf("export/boston_social_infra.geojson")

myjoined <- shapes %>%
  # Join together. Ordinarily, we'd want to use the 
  # same coordinate reference system, 
  # but we're looking at such a small area 
  # that it doesn't make a difference.
  st_join(points) %>%
  # In how many cases did you get 
  group_by(geoid) %>%
  summarize(
    # Tally
    total = sum(group != "NA", na.rm = TRUE))


mydat <- read_csv("block_groups_census.csv") %>% 
  select(geoid, pop, pop_women, pop_age_65_plus, 
         pop_black, pop_white, pop_asian,
         median_income, pop_unemployed) %>%
  mutate(geoid = as.character(geoid))

mydat %>% head()
# pop, pop_women, pop_age_65_plus, 
#         pop_black, pop_white, pop_asian,
#         median_income, pop_unemployed
 
myjoined %>% head()
```


```{r}
  as.data.frame() %>%
  group_by(geoid) %>%
  summarize(
    total = n(),
    community_spaces = sum(group == "Community Spaces", na.rm =TRUE),
    worship = sum(group == "Places of Worship", na.rm = TRUE),
    parks = sum(group == "Parks", na.rm = TRUE),
    social = sum(group == "Social Businesses", na.rm = TRUE))

ggplot() +
  geom_sf(data = tally, mapping = aes(fill = total))

```

## For Final Usage Elsewhere

Finally, we need to consolidate our data into a usable dataset in the future.

Let's get a dataset that meets the following criteria:

- Contains cell ID, latitude and longitude, milestone.
- Distinguishes between points that were ground-truthed versus non-ground-truthed points.
- Distinguishes points that were found but are not social infrastructure.

```{r}
library(tidyverse)
library(sf)
# Let's get the 'final record' of google points, classified based on whether they were found, but including ALL OF THEM.
# Import original points
# using ALL POINTS in Boston, not just core Boston
mygoogle <- read_sf("mygoogle_boston.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  mutate(source = "googleapi") %>%
  # Assign uniform set of names
  select(name = Name, type = group,
         status = status, cell_id, 
         search = file, google_id = place_id,source, geometry) %>%
  # Fix status
  mutate(
    status = status %>% na_if("") %>% tolower() %>% str_trim(),
    # Fix an incorrectly coded site.
    status = if_else(name == "Chapel of Mary" & status == "new sites", "checked", status),
    status = case_when(
      status %in% c("been there", "checked") ~ "checked",
      status == "new sites" ~ "new",
      status == "double" ~ "duplicated",
      # Classify any sites that were left blank
      name %in% c(
        "Honeycomb Cafe",
        "Roxbury (formerly Dudley) Branch of the Boston Public Library",
        "Caff Nero") ~ "checked",
      # Classify any sites left blank that were actually not social infrastructure
      name %in% c(
        "Codman Square District", "Concord Fountain",
        "Death Certificates Registry",
        "Boston Human Resources Department",
        "Law Department-Investigating",
        "Commn-Persons With Dsabilities",
        "Collecting-Delinquent Tax Unit",
        "Horn Book Guide",
        "Boston Purchasing Division",
        "City of Boston Labor Relations",
        "JPizle Kitchen", # restaurant
        "BeCause Water",
        "Mitchell, Fuchsia, M.D.",
        "Dr. Philip Severin III, MD",
        "Laz parking","Laz Parking Ltd") ~  "not social infrastructure",
      TRUE ~ status),
    # Fix any missing types
    type = case_when(
      name == "Honeycomb Cafe" ~ "Social Businesses",
      name == "Roxbury (formerly Dudley) Branch of the Boston Public Library" ~ "Community Spaces",
      name == "Caff Nero" ~ "Social Businesses",
      TRUE ~ type),
    type = na_if(type, ""))

#mygoogle$status %>% unique()
#mygoogle$type %>% unique()
#mygoogle$name %>% unique()
```

Next, let's get the final list of points that were checked.

```{r}
# Import online-checked points
mychecked <- read_sf("mysites_all_boston.kml") %>%
  select(-c(description, timestamp:icon)) %>%
  # Filter out parking lots
  mutate(source = "googlemymaps") %>%
  select(name = Name, type = group, status, cell_id, 
         google_id, source, geometry)  %>%
  filter(str_detect(name, "Parking|parking", negate = TRUE)) %>%
  mutate(google_id = google_id %>% na_if("") %>% str_trim()) %>%
  mutate(status = case_when(
    is.na(google_id) & status == "checked" ~ "new", 
    is.na(google_id) & status == "been there" ~ "new",
    
    !is.na(google_id) & status == "checked" ~ "checked",
    !is.na(google_id) & status == "been there" ~ "checked",
  
    status == "new sites" ~ "new",
    TRUE ~status)) 
  # Zoom into just the sites we added...?
  #filter(status == "new")

#789 = have an id
mychecked %>%
  filter(!is.na(google_id)) %>%
  count()

# Import ground-truthed points
myground <- read_rds("mygroundtruthed.rds")  %>%
  mutate(source = "groundtruth_masters") %>%
  select(name = title, type = group, status = groundtruth, cell_id, source, geometry)  %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])  %>%
  as_tibble() %>%
  select(-geometry)

bind_rows(mygoogle, mychecked) %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  as_tibble() %>%
  select(-geometry) %>%
  # Join in ground-truthed points
  bind_rows(myground) %>%
  # Round to nearest 30 feet
  mutate(xish = round(x, 4), yish = round(y, 4)) %>%
  # Group by name and general 30-feet area; there's not going to be 
  # 2 caffe neros within the same 30 feet.
  group_by(name, xish, yish) %>%
  summarize(type = unique(type) %>% paste(collapse = ", "),
            cell_id = unique(cell_id) %>% paste(collapse = ", "),
            google_id = unique(google_id) %>% paste(collapse = ", "),
            status = unique(status) %>% paste(collapse = ", "),
            search = unique(search) %>% paste(collapse = ", "),
            source = unique(source) %>% sort() %>% paste(collapse = ", "),
            x = unique(x) %>% paste(collapse = ", "),
            y  = unique(y) %>% paste(collapse = ", ")) %>%
  ungroup() %>%
  mutate(
    google_id = str_remove(google_id, ", NA") %>% na_if("NA") %>% na_if(""),
    search = str_remove(search, ", NA") %>% na_if("NA") %>% na_if(""),
    type = case_when(
      name == "Allan Crite Community Garden" ~ "Parks",
      name == "Fenway Park" ~ "Parks",
      name == "Garden of Peace" ~ "Parks",
      name == "Iglesia Comunitaria Casa De Gloria" ~ "Places of Worship",
      name == "Islamic Society of Boston Cultural Center" ~ "Places of Worship",
      name == "St Leonard-Port Maurice Parish" ~ "Places of Worship",
      TRUE ~ type),
    status = case_when(
      # Was in original google results
      !is.na(google_id) & status == "checked" ~ "checked",
      !is.na(google_id) & status == "checked, new" ~ "checked",
      !is.na(google_id) & status == "checked, new, visited" ~ "checked, visited",
      !is.na(google_id) & status == "checked, visited" ~ "checked, visited",
      !is.na(google_id) & status == "visited" ~ "checked, visited",
      
      # Was not in original google results
      is.na(google_id) & status == "checked" ~ "new",
      is.na(google_id) & status == "checked, new" ~ "new",
      is.na(google_id) & status == "checked, new, visited" ~ "new, visited",
      is.na(google_id) & status == "new, visited" ~ "new, visited",
      is.na(google_id) & status == "visited" ~ "new, visited",
      
      TRUE ~ status),
    source = case_when(
      str_detect(source, "googleapi") ~ "google_api",
      TRUE ~ "humancoding_fall")) %>%
  ungroup() %>%
  select(-xish, -yish) %>%
  # Finally, filters just to points within cells
  filter(cell_id != "NA") %>%
  # A couple of places accidentally got a bunch of coordinates of different lengths, even though they show the same place. Just pick the first.  
    mutate_at(vars(x, y), list(~word(., 1) %>% str_remove(pattern = ",") %>% str_trim() %>% as.numeric())) %>%
  # if it has a google_id, it's not new - that means it was retuned in the original query
  mutate(status = if_else(!is.na(google_id) & status == "NA, new", 
                          true = "checked", false = status)) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)  %>%
  # We're going to cut any places located outside of Boston
  st_join(read_rds("neighborhoods.rds") %>% 
            st_transform(crs = 4326) %>% 
            select(neighborhood = name)) %>%
  filter(!is.na(neighborhood)) %>%
  mutate(status = if_else(
    condition = name %in% c(
      "African Studies Library",
      "Baker Square Condominiums",
      "Boston City",
      "Lofts At Lower Mills and Baker Square Condominums"), 
    true = "not social infrastructure", false = status),
    
    status = if_else(
      condition = name %in% c(
        "Archdale Community Center", 
        "Dorchester Park",
        "Lower Mills Branch of the Boston Public Library",
        "Flat Black Coffee",
        "Wesley United Methodist Church",
        "Ventura Park",
        "Sweet Life",
        "William G. Walsh Playground",
        "Top of the Line Barbershop"),
      true = "checked", false = status),
    
    status = if_else(
      google_id %in% c(
        "ChIJ3Wc0GUp744kRPWwNW7JpKSU",	
        "ChIJqb2b43N844kR8prF3geo2os"),
      true = "checked", false = status)) %>%
  mutate(x = st_coordinates(.$geometry)[,1],
         y = st_coordinates(.$geometry)[,2]) %>%
  as_tibble() %>%
  mutate(id = 1:n()) %>%
  write_csv("export/boston_sites_2022_03_03.csv")
```

```{r}
# Use boston neighborhood boundaries to exclude places on the other side of the River Charles

read_csv("export/boston_sites_2022_03_03.csv") %>%
  filter(!status %in% c("duplicated", "not found", "not social infrastructure")) %>%
  write_csv("export/boston_social_infra_2022_03_03.csv")
```

```{r}
read_rds("neighborhoods.rds") %>%
  st_write("export/neighborhoods.kml", delete_dsn = TRUE)


aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Get train lines within that zone
read_sf("transit/MBTA_ARC.shp") %>%
  st_transform(crs= aea) %>%
  select(line = LINE, geometry) %>%
  group_by(line) %>%
  summarize(geometry = st_union(geometry)) %>%
  ungroup() %>%
  st_write("export/trains.kml", delete_dsn = TRUE)
```




# Z. Robustness Tests

## Z1. Compare Grid Size effect on Rates

### Identify 2km-2 grid cells within Boston.

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Load in census tracts in suffolk county
tracts <-read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  # Remove Chelsea, Revere, and Winthrop,
  # Which are cities that border Boston,
  # also in Suffolk county
  filter(area_land > 0) %>%
  filter(!str_sub(geoid, 6,7) %in% c(15:18))  %>%
  st_transform(crs= aea)

# Create one big polygon outline of tracts within Boston
boston <- tracts %>% 
  summarize(geoid = "Boston", geometry = st_union(geometry))

# Now get the 1 km2 grid itself, and make a big outline
# Since we manually removed a few non-populated islands from it
boston_grid <- read_sf("grid_covariates_tracts.kml") %>%
  st_transform(crs = aea) %>%
  # Grab just cells that overlap with Boston
  st_join(boston, left = FALSE) %>%
  summarize(geoid = "Boston", geometry = st_union(geometry))

# Save the ids of cells within Boston for later use
read_sf("grid_covariates_tracts_2.kml") %>%
  st_transform(crs = aea) %>%
  # Use the boston polygon to identify ONLY cells that are located within/overlapping Boston border 
  st_join(boston_grid, left = FALSE) %>%
  as.data.frame() %>%
  select(cell_id) %>%
  unlist() %>%
  unname() %>%
  saveRDS("boston_grid_2.rds")

rm(list = ls())

# Good!
ggplot() +
  geom_sf(data = boston) +
  geom_sf(data = boston_grid, alpha = 0.5, fill = "blue") +
  geom_sf(data = read_sf("grid_covariates_tracts_2.kml") %>%
            st_transform(crs = aea) %>%
            filter(cell_id %in% read_rds("boston_grid_2.rds")), fill = NA)
```

### Tally Rates

We're going to do a cool descriptive sensitivity test.

```{r}
# Get North American Albers Equal Area Conic Projection
# https://spatialreference.org/ref/esri/north-america-albers-equal-area-conic/
aea <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Get EPSG:4326 (WGS 84) projection
#https://spatialreference.org/ref/epsg/wgs-84/
wgs <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

# Get queries for Places of Worship
myquery <- read_rds("prior_queries/places_of_worship.rds") %>%
  mutate(lat = geometry$location$lat,
         lng = geometry$location$lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = wgs) %>%
  st_transform(crs = aea)  %>%
  # Grab just the distinct results
  select(-grid) %>%
  group_by(place_id) %>%
  summarize_at(vars(name, business_status, 
                    #plus_code.compound_code, 
                    rating, types, user_ratings_total, vicinity, 
                    permanently_closed, geometry),
               funs(unique(.))) %>%
  ungroup() %>%
  # Classify the type
  mutate(types = .$types %>%
           map(~paste(., collapse = "; ")) %>%
           unlist())

# Rates tallied from a 1 km collection grid and tallied per 1 km

# Grab the 1-square kilometer fishnet grid
tally_1_1 <- read_sf("grid_covariates_tracts.kml") %>%
  select(-c(Name:icon)) %>%
  st_transform(crs = aea) %>%
  # Filter to cells within boston
  filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid.rds"))  %>%
  # Tally!
  st_join(myquery) %>%
  group_by(cell_id) %>%
  summarize(
    place_of_worship = sum(!is.na(place_id)),
     # Use interpolated population density
    pop_density = unique(pop_density_int),
    geometry = unique(geometry)) %>%
  ungroup() %>%
  # Calculate rate of sites per 1000 persons in the grid cell
  mutate_at(vars(place_of_worship), funs(. / pop_density * 1000) ) %>%
  mutate_at(vars(place_of_worship), funs(if_else(is.infinite(.) | is.nan(.),
                                          NA_real_, .)))

# Rates tallied from a 1 km collection grid and tallied per 2 km
#tally_1_2 <- read_sf("grid_covariates_tracts_2.kml") %>%
#  select(-c(Name:icon)) %>%
#  st_transform(crs = aea) %>%
#  # Filter to cells within boston
#  filter(str_remove(cell_id, "Cell ") %in% read_rds("boston_grid_2.rds"))  %>%
#  # Tally!
#  st_join(myquery) %>%
#  group_by(cell_id) %>%
#  summarize(
#    place_of_worship = sum(!is.na(place_id)),
#    # Keep interpolated population density
#    pop_density = unique(pop_density_int),
#    geometry = unique(geometry)) %>%
#  ungroup() %>%
#  # Calculate rate of sites per 1000 persons in the grid cell
#  mutate_at(vars(place_of_worship), funs(. / pop_density * 1000) ) %>%
#  mutate_at(vars(place_of_worship), funs(if_else(is.infinite(.) | is.nan(.),
#                                                 NA_real_, .)))

# Rates tallies from a 2-km collection grid and 1-km tally grid
tally_2_1 <- read_rds("processed_queries/sites_crosstab.rds") %>%
  # Grab pop-density (which is in fact the interpolated pop-density)
  select(cell_id, place_of_worship, pop_density, geometry)


# Let's download the newest versions of the SCI
library(dataverse)
library(tidyverse)

#dataverse::get_dataframe_by_name(
#  dataset = "10.7910/DVN/OSVCRC", 
#  filename = "index_tracts_V3_04_10_2022.tab",
#  server = "dataverse.harvard.edu") %>%
  # Filter to just Boston
#  filter(str_sub(geoid, 1,5) == "25025") %>%
  # save to file!
#  write_csv("covariates/sci_census_tracts_2022_04_10.csv")

# Calculate social capital indices in 2020
sci <- read_sf("tracts.kml") %>%
  select(-c(Name:icon)) %>%
  left_join(by = "geoid", 
#            y = read_csv("covariates/sci_census_tracts_2021_01_28.csv") %>%
            y = read_csv("covariates/sci_census_tracts_2022_04_10.csv") %>%
              mutate(geoid = as.character(geoid)) %>%
              filter(year == 2020) %>%
              select(year, geoid, social_capital, bonding, bridging, linking)) %>%
  st_transform(crs = aea) %>%
  # Join these into the tracts
  st_join(y = ., x = read_sf("grid_covariates_tracts.kml") %>%
            select(-c(Name:icon)) %>%
            st_transform(crs = aea)) %>%
  group_by(cell_id) %>%
  summarize_at(vars(social_capital:linking),
               list(~mean(., na.rm = TRUE)))

```


```{r}
# Bind them together
bind_rows(
  tally_1_1 %>% mutate(type = "1_1") %>%
    mutate(cell_id = as.character(cell_id)),
#  tally_1_2 %>% mutate(type = "1 km<sup>2</sup> Collection Grid<br>1 km<sup>2</sup> Tally Grid") %>%
#    mutate(cell_id = as.character(cell_id)),
  tally_2_1 %>% mutate(type = "2_1") %>%
    mutate(cell_id = as.character(cell_id))
) %>%
    # Log, but deal with zeros
  mutate(y = case_when(
    # Replace zeros with half the value of the smallest non-zero value
    place_of_worship == 0 ~ place_of_worship %>% unique() %>% sort(FALSE) %>% .[2] / 2,
    # otherwise, keep the same
    TRUE ~ place_of_worship),
    # Log transform
    ylogged = log(y)) %>%
  # Join in covariates 
  left_join(by = "cell_id",
            y = read_sf("grid_covariates_tracts.kml") %>%
              as_tibble() %>%
              select(cell_id, neighborhood, pop_women:pop_age_65_plus)) %>%
  left_join(by = "cell_id",
            y = sci %>% as_tibble() %>% select(cell_id, social_capital:linking)) %>%
  saveRDS("grid/compare_1_vs_2.rds")

rm(list = ls())
```

### Compare Rates

```{r}
tally <- read_rds("grid/compare_1_vs_2.rds")

mylabels <- as_labeller(c("1_1" = "<b>1 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid</b><br><i>Mean Rate: 0.258</i>",
                          "2_1" = "<b>2 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid</b><br><i>Mean Rate: 0.192</i>"))


# How much do results change when using a 1 km collection grid versus a 2 km tally grid?
g1 <- ggplot() +
   geom_sf(data = tally, mapping = aes(fill = place_of_worship, group = type),
          fill = NA, color = "#373737", size = 1) +
  geom_sf(data = tally, mapping = aes(fill = place_of_worship, group = type),
          color = "white") +
  facet_wrap(~type, labeller = mylabels) +
  theme_void(base_size = 14) +
  theme(plot.subtitle = ggtext::element_markdown(size = 14, hjust = 0.5),
        plot.caption = ggtext::element_markdown(size = 12, hjust = 0),
        
        strip.text.x = ggtext::element_markdown(size = 12, hjust = 0.5),
        plot.margin = margin(0,0,0,0, "cm"),
        legend.title = ggtext::element_markdown(size = 14, hjust = 0)) +
  viridis::scale_fill_viridis(option = "plasma", direction = -1, begin = 0.2, end = 0.95) +
  labs(fill = "<b>Rates of<br>Places<br>of Worship</b><br><br><i>per 1000<br>residents<br>per km<sup>2</sup></i>")

# Analyze the difference of means
ggsave(g1, filename = "viz/figure_D1_robustness.png", dpi = 300, width = 5, height = 3.5)
```

```{r}
# Looks like there is a very minor, slightly statistically significant difference between the two.
tally %>%
  as_tibble() %>%
  with(t.test(ylogged ~ type)) %>%
  broom::tidy() %>% 
  mutate_at(vars(estimate1, estimate2), list(~exp(.)))


tally %>%
  as_tibble() %>%
  group_by(type) %>%
  summarize(mean = mean(place_of_worship, na.rm = TRUE),
            median = median(place_of_worship, na.rm = TRUE))

# Calculate their relative ranking
tally %>%
  as_tibble() %>%
  group_by(type) %>%
  arrange(place_of_worship) %>%
  mutate(rank = 1:n()) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(cell_id), names_from = type, values_from = rank) %>%
  summarize(cor = cor(`1_1`, `2_1`))

tally %>%
  as_tibble() %>%
  pivot_wider(id_cols = c(cell_id), names_from = type, values_from = place_of_worship) %>%
  summarize(cor = cor(`1_1`, `2_1`))
```

### Correlations

Are there any major demographic differences? Do they correlate as expected with covariates?

```{r}
mycor <- read_rds("grid/compare_1_vs_2.rds") %>% 
  as_tibble() %>%
  pivot_longer(cols = c(
    pop_density, pop_women, pop_age_65_plus, 
    pop_black, pop_white, pop_asian, pop_hisplat,
    pop_some_college, median_income, 
    pop_unemployed, median_monthly_housing_cost, 
    social_capital:linking), names_to = "covariate", values_to = "x") %>%
  group_by(type, covariate) %>%
  summarize(cor = cor(y, x, use = "pairwise.complete.obs")) %>%
  ungroup() %>%
  # Relabel terms
  mutate(covariate = covariate %>% recode_factor(
    "pop_density" = "Pop\nDensity", 
    "pop_women" = "% Women",
    "pop_age_65_plus" = "% Over\nAge 65",
    "pop_black" = "% Black", 
    "pop_white" = "% White",
    "pop_asian" = "% Asian", 
    "pop_hisplat" = "% Hispanic\n/Latino",
    "pop_some_college" = "% Some\nCollege",
    "median_income" = "Median\nIncome",
    "pop_unemployed" = "Unemployment\nRate",
    "median_monthly_housing_cost" = "Median Monthly\nHousing Cost",
    "linking" = "Linking\nSocial Capital",
    "bridging" = "Bridging\nSocial Capital",
    "bonding" = "Bonding\nSocial Capital",
    "social_capital" = "Overall\nSocial Capital"),
    type = type %>% recode_factor(
      "1_1" = "1 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid",
      "2_1" = "2 km<sup>2</sup> Collection Grid<br>1 km<sup>1</sup> Tally Grid"))


g1 <- mycor %>%
  ggplot(mapping = aes(x = type, y = covariate, fill = cor, label = round(cor, 2))) +
  geom_tile(color = "#373737", size = 0.1) +
  geom_text(size = 5) +
  scale_fill_gradient2(low = "#DC267F", high = "#648FFF", mid = "white", 
                       midpoint = 0, limits = c(-1, 1),
                       breaks = c(-1, -0.75, -0.5, -0.25, 0, .25, .5, .75, 1))  +
  theme_classic() +
  theme(
    plot.title = element_markdown(hjust = 0.5, size = 14),
    plot.subtitle = element_markdown(hjust = 0.5, size = 12),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_text(size = 10),
        axis.text.x = ggtext::element_markdown(size = 10, hjust = 0.5),
    legend.title = ggtext::element_markdown(size = 10, hjust= 0),
    legend.text = ggtext::element_markdown(size = 12, hjust= 0),
        panel.border = element_blank(),
        legend.position = "right") +
  guides(fill = guide_colorsteps(barheight = 15, barwidth = 2,
                                 tick.colour = "#373737", frame.colour = "#373737", 
                                 show.limits = TRUE)) +
  labs(x = NULL, y = NULL, fill = "<b>Correlation</b><br>(<i>Pearson's R</i>)",
       title = "Demographic Correlations Unchanged<br>by Grid Collection Size",
       subtitle = "Correlations between Logged Rates<br>of Places of Worship vs. Demographics",
       caption = "Note: Demographics represent averages of tract level covariates.")

ggsave(g1, filename = "viz/figure_D2_robustness.png", width = 6, height = 6.5, dpi = 300)
# Get a simple estimate of population density within a fishnet grid cells using the mean of the rate of tracts
# Import tracts

```
















